// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"fmt"
	"strconv"
	"time"

	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
)

// MetricSettings provides common settings for a particular metric.
type MetricSettings struct {
	Enabled bool `mapstructure:"enabled"`
}

// MetricsSettings provides settings for saphanareceiver metrics.
type MetricsSettings struct {
	SaphanaAlertCount                       MetricSettings `mapstructure:"saphana.alert.count"`
	SaphanaBackupLatest                     MetricSettings `mapstructure:"saphana.backup.latest"`
	SaphanaColumnMemoryUsed                 MetricSettings `mapstructure:"saphana.column.memory.used"`
	SaphanaComponentMemoryUsed              MetricSettings `mapstructure:"saphana.component.memory.used"`
	SaphanaConnectionCount                  MetricSettings `mapstructure:"saphana.connection.count"`
	SaphanaCPUUsed                          MetricSettings `mapstructure:"saphana.cpu.used"`
	SaphanaDiskSizeCurrent                  MetricSettings `mapstructure:"saphana.disk.size.current"`
	SaphanaHostMemoryCurrent                MetricSettings `mapstructure:"saphana.host.memory.current"`
	SaphanaHostSwapCurrent                  MetricSettings `mapstructure:"saphana.host.swap.current"`
	SaphanaInstanceCodeSize                 MetricSettings `mapstructure:"saphana.instance.code_size"`
	SaphanaInstanceMemoryCurrent            MetricSettings `mapstructure:"saphana.instance.memory.current"`
	SaphanaInstanceMemorySharedAllocated    MetricSettings `mapstructure:"saphana.instance.memory.shared.allocated"`
	SaphanaInstanceMemoryUsedPeak           MetricSettings `mapstructure:"saphana.instance.memory.used.peak"`
	SaphanaLicenseExpirationTime            MetricSettings `mapstructure:"saphana.license.expiration.time"`
	SaphanaLicenseLimit                     MetricSettings `mapstructure:"saphana.license.limit"`
	SaphanaLicensePeak                      MetricSettings `mapstructure:"saphana.license.peak"`
	SaphanaNetworkRequestAverageTime        MetricSettings `mapstructure:"saphana.network.request.average_time"`
	SaphanaNetworkRequestCount              MetricSettings `mapstructure:"saphana.network.request.count"`
	SaphanaNetworkRequestFinishedCount      MetricSettings `mapstructure:"saphana.network.request.finished.count"`
	SaphanaReplicationAverageTime           MetricSettings `mapstructure:"saphana.replication.average_time"`
	SaphanaReplicationBacklogSize           MetricSettings `mapstructure:"saphana.replication.backlog.size"`
	SaphanaReplicationBacklogTime           MetricSettings `mapstructure:"saphana.replication.backlog.time"`
	SaphanaRowStoreMemoryUsed               MetricSettings `mapstructure:"saphana.row_store.memory.used"`
	SaphanaSchemaMemoryUsedCurrent          MetricSettings `mapstructure:"saphana.schema.memory.used.current"`
	SaphanaSchemaMemoryUsedMax              MetricSettings `mapstructure:"saphana.schema.memory.used.max"`
	SaphanaSchemaOperationCount             MetricSettings `mapstructure:"saphana.schema.operation.count"`
	SaphanaSchemaRecordCompressedCount      MetricSettings `mapstructure:"saphana.schema.record.compressed.count"`
	SaphanaSchemaRecordCount                MetricSettings `mapstructure:"saphana.schema.record.count"`
	SaphanaServiceCodeSize                  MetricSettings `mapstructure:"saphana.service.code_size"`
	SaphanaServiceCount                     MetricSettings `mapstructure:"saphana.service.count"`
	SaphanaServiceMemoryCompactorsAllocated MetricSettings `mapstructure:"saphana.service.memory.compactors.allocated"`
	SaphanaServiceMemoryCompactorsFreeable  MetricSettings `mapstructure:"saphana.service.memory.compactors.freeable"`
	SaphanaServiceMemoryEffectiveLimit      MetricSettings `mapstructure:"saphana.service.memory.effective_limit"`
	SaphanaServiceMemoryHeapCurrent         MetricSettings `mapstructure:"saphana.service.memory.heap.current"`
	SaphanaServiceMemoryLimit               MetricSettings `mapstructure:"saphana.service.memory.limit"`
	SaphanaServiceMemorySharedCurrent       MetricSettings `mapstructure:"saphana.service.memory.shared.current"`
	SaphanaServiceMemoryUsed                MetricSettings `mapstructure:"saphana.service.memory.used"`
	SaphanaServiceStackSize                 MetricSettings `mapstructure:"saphana.service.stack_size"`
	SaphanaServiceThreadCount               MetricSettings `mapstructure:"saphana.service.thread.count"`
	SaphanaTransactionBlocked               MetricSettings `mapstructure:"saphana.transaction.blocked"`
	SaphanaTransactionCount                 MetricSettings `mapstructure:"saphana.transaction.count"`
	SaphanaUptime                           MetricSettings `mapstructure:"saphana.uptime"`
	SaphanaVolumeOperationCount             MetricSettings `mapstructure:"saphana.volume.operation.count"`
	SaphanaVolumeOperationSize              MetricSettings `mapstructure:"saphana.volume.operation.size"`
	SaphanaVolumeOperationTime              MetricSettings `mapstructure:"saphana.volume.operation.time"`
}

func DefaultMetricsSettings() MetricsSettings {
	return MetricsSettings{
		SaphanaAlertCount: MetricSettings{
			Enabled: true,
		},
		SaphanaBackupLatest: MetricSettings{
			Enabled: true,
		},
		SaphanaColumnMemoryUsed: MetricSettings{
			Enabled: true,
		},
		SaphanaComponentMemoryUsed: MetricSettings{
			Enabled: true,
		},
		SaphanaConnectionCount: MetricSettings{
			Enabled: true,
		},
		SaphanaCPUUsed: MetricSettings{
			Enabled: true,
		},
		SaphanaDiskSizeCurrent: MetricSettings{
			Enabled: true,
		},
		SaphanaHostMemoryCurrent: MetricSettings{
			Enabled: true,
		},
		SaphanaHostSwapCurrent: MetricSettings{
			Enabled: true,
		},
		SaphanaInstanceCodeSize: MetricSettings{
			Enabled: true,
		},
		SaphanaInstanceMemoryCurrent: MetricSettings{
			Enabled: true,
		},
		SaphanaInstanceMemorySharedAllocated: MetricSettings{
			Enabled: true,
		},
		SaphanaInstanceMemoryUsedPeak: MetricSettings{
			Enabled: true,
		},
		SaphanaLicenseExpirationTime: MetricSettings{
			Enabled: true,
		},
		SaphanaLicenseLimit: MetricSettings{
			Enabled: true,
		},
		SaphanaLicensePeak: MetricSettings{
			Enabled: true,
		},
		SaphanaNetworkRequestAverageTime: MetricSettings{
			Enabled: true,
		},
		SaphanaNetworkRequestCount: MetricSettings{
			Enabled: true,
		},
		SaphanaNetworkRequestFinishedCount: MetricSettings{
			Enabled: true,
		},
		SaphanaReplicationAverageTime: MetricSettings{
			Enabled: true,
		},
		SaphanaReplicationBacklogSize: MetricSettings{
			Enabled: true,
		},
		SaphanaReplicationBacklogTime: MetricSettings{
			Enabled: true,
		},
		SaphanaRowStoreMemoryUsed: MetricSettings{
			Enabled: true,
		},
		SaphanaSchemaMemoryUsedCurrent: MetricSettings{
			Enabled: true,
		},
		SaphanaSchemaMemoryUsedMax: MetricSettings{
			Enabled: true,
		},
		SaphanaSchemaOperationCount: MetricSettings{
			Enabled: true,
		},
		SaphanaSchemaRecordCompressedCount: MetricSettings{
			Enabled: true,
		},
		SaphanaSchemaRecordCount: MetricSettings{
			Enabled: true,
		},
		SaphanaServiceCodeSize: MetricSettings{
			Enabled: true,
		},
		SaphanaServiceCount: MetricSettings{
			Enabled: true,
		},
		SaphanaServiceMemoryCompactorsAllocated: MetricSettings{
			Enabled: true,
		},
		SaphanaServiceMemoryCompactorsFreeable: MetricSettings{
			Enabled: true,
		},
		SaphanaServiceMemoryEffectiveLimit: MetricSettings{
			Enabled: true,
		},
		SaphanaServiceMemoryHeapCurrent: MetricSettings{
			Enabled: true,
		},
		SaphanaServiceMemoryLimit: MetricSettings{
			Enabled: true,
		},
		SaphanaServiceMemorySharedCurrent: MetricSettings{
			Enabled: true,
		},
		SaphanaServiceMemoryUsed: MetricSettings{
			Enabled: true,
		},
		SaphanaServiceStackSize: MetricSettings{
			Enabled: true,
		},
		SaphanaServiceThreadCount: MetricSettings{
			Enabled: true,
		},
		SaphanaTransactionBlocked: MetricSettings{
			Enabled: true,
		},
		SaphanaTransactionCount: MetricSettings{
			Enabled: true,
		},
		SaphanaUptime: MetricSettings{
			Enabled: true,
		},
		SaphanaVolumeOperationCount: MetricSettings{
			Enabled: true,
		},
		SaphanaVolumeOperationSize: MetricSettings{
			Enabled: true,
		},
		SaphanaVolumeOperationTime: MetricSettings{
			Enabled: true,
		},
	}
}

type metricSaphanaAlertCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.alert.count metric with initial data.
func (m *metricSaphanaAlertCount) init() {
	m.data.SetName("saphana.alert.count")
	m.data.SetDescription("Number of current alerts.")
	m.data.SetUnit("{alerts}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaAlertCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, alertRatingAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.AlertRating, pcommon.NewValueString(alertRatingAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaAlertCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaAlertCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaAlertCount(settings MetricSettings) metricSaphanaAlertCount {
	m := metricSaphanaAlertCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaBackupLatest struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.backup.latest metric with initial data.
func (m *metricSaphanaBackupLatest) init() {
	m.data.SetName("saphana.backup.latest")
	m.data.SetDescription("The age of the latest backup by start time.")
	m.data.SetUnit("s")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricSaphanaBackupLatest) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaBackupLatest) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaBackupLatest) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaBackupLatest(settings MetricSettings) metricSaphanaBackupLatest {
	m := metricSaphanaBackupLatest{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaColumnMemoryUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.column.memory.used metric with initial data.
func (m *metricSaphanaColumnMemoryUsed) init() {
	m.data.SetName("saphana.column.memory.used")
	m.data.SetDescription("The memory used in all columns.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaColumnMemoryUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, columnMemoryTypeAttributeValue string, columnMemorySubtypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.ColumnMemoryType, pcommon.NewValueString(columnMemoryTypeAttributeValue))
	dp.Attributes().Insert(A.ColumnMemorySubtype, pcommon.NewValueString(columnMemorySubtypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaColumnMemoryUsed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaColumnMemoryUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaColumnMemoryUsed(settings MetricSettings) metricSaphanaColumnMemoryUsed {
	m := metricSaphanaColumnMemoryUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaComponentMemoryUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.component.memory.used metric with initial data.
func (m *metricSaphanaComponentMemoryUsed) init() {
	m.data.SetName("saphana.component.memory.used")
	m.data.SetDescription("The memory used in components.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaComponentMemoryUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, componentAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Component, pcommon.NewValueString(componentAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaComponentMemoryUsed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaComponentMemoryUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaComponentMemoryUsed(settings MetricSettings) metricSaphanaComponentMemoryUsed {
	m := metricSaphanaComponentMemoryUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaConnectionCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.connection.count metric with initial data.
func (m *metricSaphanaConnectionCount) init() {
	m.data.SetName("saphana.connection.count")
	m.data.SetDescription("The number of current connections.")
	m.data.SetUnit("{connections}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaConnectionCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, connectionStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.ConnectionStatus, pcommon.NewValueString(connectionStatusAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaConnectionCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaConnectionCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaConnectionCount(settings MetricSettings) metricSaphanaConnectionCount {
	m := metricSaphanaConnectionCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaCPUUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.cpu.used metric with initial data.
func (m *metricSaphanaCPUUsed) init() {
	m.data.SetName("saphana.cpu.used")
	m.data.SetDescription("Total CPU time spent.")
	m.data.SetUnit("ms")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaCPUUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, cpuTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.CPUType, pcommon.NewValueString(cpuTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaCPUUsed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaCPUUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaCPUUsed(settings MetricSettings) metricSaphanaCPUUsed {
	m := metricSaphanaCPUUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaDiskSizeCurrent struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.disk.size.current metric with initial data.
func (m *metricSaphanaDiskSizeCurrent) init() {
	m.data.SetName("saphana.disk.size.current")
	m.data.SetDescription("The disk size.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaDiskSizeCurrent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, pathAttributeValue string, diskUsageTypeAttributeValue string, diskStateUsedFreeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Path, pcommon.NewValueString(pathAttributeValue))
	dp.Attributes().Insert(A.DiskUsageType, pcommon.NewValueString(diskUsageTypeAttributeValue))
	dp.Attributes().Insert(A.DiskStateUsedFree, pcommon.NewValueString(diskStateUsedFreeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaDiskSizeCurrent) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaDiskSizeCurrent) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaDiskSizeCurrent(settings MetricSettings) metricSaphanaDiskSizeCurrent {
	m := metricSaphanaDiskSizeCurrent{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaHostMemoryCurrent struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.host.memory.current metric with initial data.
func (m *metricSaphanaHostMemoryCurrent) init() {
	m.data.SetName("saphana.host.memory.current")
	m.data.SetDescription("The amount of physical memory on the host.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaHostMemoryCurrent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, memoryStateUsedFreeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.MemoryStateUsedFree, pcommon.NewValueString(memoryStateUsedFreeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaHostMemoryCurrent) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaHostMemoryCurrent) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaHostMemoryCurrent(settings MetricSettings) metricSaphanaHostMemoryCurrent {
	m := metricSaphanaHostMemoryCurrent{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaHostSwapCurrent struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.host.swap.current metric with initial data.
func (m *metricSaphanaHostSwapCurrent) init() {
	m.data.SetName("saphana.host.swap.current")
	m.data.SetDescription("The amount of swap space on the host.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaHostSwapCurrent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, hostSwapStateAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.HostSwapState, pcommon.NewValueString(hostSwapStateAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaHostSwapCurrent) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaHostSwapCurrent) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaHostSwapCurrent(settings MetricSettings) metricSaphanaHostSwapCurrent {
	m := metricSaphanaHostSwapCurrent{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaInstanceCodeSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.instance.code_size metric with initial data.
func (m *metricSaphanaInstanceCodeSize) init() {
	m.data.SetName("saphana.instance.code_size")
	m.data.SetDescription("The instance code size, including shared libraries of SAP HANA processes.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricSaphanaInstanceCodeSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaInstanceCodeSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaInstanceCodeSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaInstanceCodeSize(settings MetricSettings) metricSaphanaInstanceCodeSize {
	m := metricSaphanaInstanceCodeSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaInstanceMemoryCurrent struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.instance.memory.current metric with initial data.
func (m *metricSaphanaInstanceMemoryCurrent) init() {
	m.data.SetName("saphana.instance.memory.current")
	m.data.SetDescription("The size of the memory pool for all SAP HANA processes.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaInstanceMemoryCurrent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, memoryStateUsedFreeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.MemoryStateUsedFree, pcommon.NewValueString(memoryStateUsedFreeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaInstanceMemoryCurrent) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaInstanceMemoryCurrent) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaInstanceMemoryCurrent(settings MetricSettings) metricSaphanaInstanceMemoryCurrent {
	m := metricSaphanaInstanceMemoryCurrent{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaInstanceMemorySharedAllocated struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.instance.memory.shared.allocated metric with initial data.
func (m *metricSaphanaInstanceMemorySharedAllocated) init() {
	m.data.SetName("saphana.instance.memory.shared.allocated")
	m.data.SetDescription("The shared memory size of SAP HANA processes.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricSaphanaInstanceMemorySharedAllocated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaInstanceMemorySharedAllocated) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaInstanceMemorySharedAllocated) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaInstanceMemorySharedAllocated(settings MetricSettings) metricSaphanaInstanceMemorySharedAllocated {
	m := metricSaphanaInstanceMemorySharedAllocated{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaInstanceMemoryUsedPeak struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.instance.memory.used.peak metric with initial data.
func (m *metricSaphanaInstanceMemoryUsedPeak) init() {
	m.data.SetName("saphana.instance.memory.used.peak")
	m.data.SetDescription("The peak memory from the memory pool used by SAP HANA processes since the instance started (this is a sample-based value).")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricSaphanaInstanceMemoryUsedPeak) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaInstanceMemoryUsedPeak) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaInstanceMemoryUsedPeak) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaInstanceMemoryUsedPeak(settings MetricSettings) metricSaphanaInstanceMemoryUsedPeak {
	m := metricSaphanaInstanceMemoryUsedPeak{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaLicenseExpirationTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.license.expiration.time metric with initial data.
func (m *metricSaphanaLicenseExpirationTime) init() {
	m.data.SetName("saphana.license.expiration.time")
	m.data.SetDescription("The amount of time remaining before license expiration.")
	m.data.SetUnit("s")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaLicenseExpirationTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, systemAttributeValue string, productAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.System, pcommon.NewValueString(systemAttributeValue))
	dp.Attributes().Insert(A.Product, pcommon.NewValueString(productAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaLicenseExpirationTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaLicenseExpirationTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaLicenseExpirationTime(settings MetricSettings) metricSaphanaLicenseExpirationTime {
	m := metricSaphanaLicenseExpirationTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaLicenseLimit struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.license.limit metric with initial data.
func (m *metricSaphanaLicenseLimit) init() {
	m.data.SetName("saphana.license.limit")
	m.data.SetDescription("The allowed product usage as specified by the license (for example, main memory).")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaLicenseLimit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, systemAttributeValue string, productAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.System, pcommon.NewValueString(systemAttributeValue))
	dp.Attributes().Insert(A.Product, pcommon.NewValueString(productAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaLicenseLimit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaLicenseLimit) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaLicenseLimit(settings MetricSettings) metricSaphanaLicenseLimit {
	m := metricSaphanaLicenseLimit{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaLicensePeak struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.license.peak metric with initial data.
func (m *metricSaphanaLicensePeak) init() {
	m.data.SetName("saphana.license.peak")
	m.data.SetDescription("The peak product usage value during last 13 months, measured periodically.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaLicensePeak) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, systemAttributeValue string, productAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.System, pcommon.NewValueString(systemAttributeValue))
	dp.Attributes().Insert(A.Product, pcommon.NewValueString(productAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaLicensePeak) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaLicensePeak) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaLicensePeak(settings MetricSettings) metricSaphanaLicensePeak {
	m := metricSaphanaLicensePeak{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaNetworkRequestAverageTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.network.request.average_time metric with initial data.
func (m *metricSaphanaNetworkRequestAverageTime) init() {
	m.data.SetName("saphana.network.request.average_time")
	m.data.SetDescription("The average response time calculated over recent requests")
	m.data.SetUnit("ms")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricSaphanaNetworkRequestAverageTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaNetworkRequestAverageTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaNetworkRequestAverageTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaNetworkRequestAverageTime(settings MetricSettings) metricSaphanaNetworkRequestAverageTime {
	m := metricSaphanaNetworkRequestAverageTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaNetworkRequestCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.network.request.count metric with initial data.
func (m *metricSaphanaNetworkRequestCount) init() {
	m.data.SetName("saphana.network.request.count")
	m.data.SetDescription("The number of active and pending service requests.")
	m.data.SetUnit("{requests}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaNetworkRequestCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, activePendingRequestStateAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.ActivePendingRequestState, pcommon.NewValueString(activePendingRequestStateAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaNetworkRequestCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaNetworkRequestCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaNetworkRequestCount(settings MetricSettings) metricSaphanaNetworkRequestCount {
	m := metricSaphanaNetworkRequestCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaNetworkRequestFinishedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.network.request.finished.count metric with initial data.
func (m *metricSaphanaNetworkRequestFinishedCount) init() {
	m.data.SetName("saphana.network.request.finished.count")
	m.data.SetDescription("The number of service requests that have completed.")
	m.data.SetUnit("{requests}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaNetworkRequestFinishedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, internalExternalRequestTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.InternalExternalRequestType, pcommon.NewValueString(internalExternalRequestTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaNetworkRequestFinishedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaNetworkRequestFinishedCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaNetworkRequestFinishedCount(settings MetricSettings) metricSaphanaNetworkRequestFinishedCount {
	m := metricSaphanaNetworkRequestFinishedCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaReplicationAverageTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.replication.average_time metric with initial data.
func (m *metricSaphanaReplicationAverageTime) init() {
	m.data.SetName("saphana.replication.average_time")
	m.data.SetDescription("The average amount of time consumed replicating a log.")
	m.data.SetUnit("us")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaReplicationAverageTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, primaryHostAttributeValue string, secondaryHostAttributeValue string, portAttributeValue string, replicationModeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
	dp.Attributes().Insert(A.PrimaryHost, pcommon.NewValueString(primaryHostAttributeValue))
	dp.Attributes().Insert(A.SecondaryHost, pcommon.NewValueString(secondaryHostAttributeValue))
	dp.Attributes().Insert(A.Port, pcommon.NewValueString(portAttributeValue))
	dp.Attributes().Insert(A.ReplicationMode, pcommon.NewValueString(replicationModeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaReplicationAverageTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaReplicationAverageTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaReplicationAverageTime(settings MetricSettings) metricSaphanaReplicationAverageTime {
	m := metricSaphanaReplicationAverageTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaReplicationBacklogSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.replication.backlog.size metric with initial data.
func (m *metricSaphanaReplicationBacklogSize) init() {
	m.data.SetName("saphana.replication.backlog.size")
	m.data.SetDescription("The current replication backlog size.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaReplicationBacklogSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, primaryHostAttributeValue string, secondaryHostAttributeValue string, portAttributeValue string, replicationModeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.PrimaryHost, pcommon.NewValueString(primaryHostAttributeValue))
	dp.Attributes().Insert(A.SecondaryHost, pcommon.NewValueString(secondaryHostAttributeValue))
	dp.Attributes().Insert(A.Port, pcommon.NewValueString(portAttributeValue))
	dp.Attributes().Insert(A.ReplicationMode, pcommon.NewValueString(replicationModeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaReplicationBacklogSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaReplicationBacklogSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaReplicationBacklogSize(settings MetricSettings) metricSaphanaReplicationBacklogSize {
	m := metricSaphanaReplicationBacklogSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaReplicationBacklogTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.replication.backlog.time metric with initial data.
func (m *metricSaphanaReplicationBacklogTime) init() {
	m.data.SetName("saphana.replication.backlog.time")
	m.data.SetDescription("The current replication backlog.")
	m.data.SetUnit("us")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaReplicationBacklogTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, primaryHostAttributeValue string, secondaryHostAttributeValue string, portAttributeValue string, replicationModeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.PrimaryHost, pcommon.NewValueString(primaryHostAttributeValue))
	dp.Attributes().Insert(A.SecondaryHost, pcommon.NewValueString(secondaryHostAttributeValue))
	dp.Attributes().Insert(A.Port, pcommon.NewValueString(portAttributeValue))
	dp.Attributes().Insert(A.ReplicationMode, pcommon.NewValueString(replicationModeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaReplicationBacklogTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaReplicationBacklogTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaReplicationBacklogTime(settings MetricSettings) metricSaphanaReplicationBacklogTime {
	m := metricSaphanaReplicationBacklogTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaRowStoreMemoryUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.row_store.memory.used metric with initial data.
func (m *metricSaphanaRowStoreMemoryUsed) init() {
	m.data.SetName("saphana.row_store.memory.used")
	m.data.SetDescription("The used memory for all row tables.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaRowStoreMemoryUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, rowMemoryTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.RowMemoryType, pcommon.NewValueString(rowMemoryTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaRowStoreMemoryUsed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaRowStoreMemoryUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaRowStoreMemoryUsed(settings MetricSettings) metricSaphanaRowStoreMemoryUsed {
	m := metricSaphanaRowStoreMemoryUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaSchemaMemoryUsedCurrent struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.schema.memory.used.current metric with initial data.
func (m *metricSaphanaSchemaMemoryUsedCurrent) init() {
	m.data.SetName("saphana.schema.memory.used.current")
	m.data.SetDescription("The memory size for all tables in schema.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaSchemaMemoryUsedCurrent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaAttributeValue string, schemaMemoryTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Schema, pcommon.NewValueString(schemaAttributeValue))
	dp.Attributes().Insert(A.SchemaMemoryType, pcommon.NewValueString(schemaMemoryTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaSchemaMemoryUsedCurrent) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaSchemaMemoryUsedCurrent) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaSchemaMemoryUsedCurrent(settings MetricSettings) metricSaphanaSchemaMemoryUsedCurrent {
	m := metricSaphanaSchemaMemoryUsedCurrent{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaSchemaMemoryUsedMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.schema.memory.used.max metric with initial data.
func (m *metricSaphanaSchemaMemoryUsedMax) init() {
	m.data.SetName("saphana.schema.memory.used.max")
	m.data.SetDescription("The estimated maximum memory consumption for all fully loaded tables in schema (data for open transactions is not included).")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaSchemaMemoryUsedMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Schema, pcommon.NewValueString(schemaAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaSchemaMemoryUsedMax) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaSchemaMemoryUsedMax) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaSchemaMemoryUsedMax(settings MetricSettings) metricSaphanaSchemaMemoryUsedMax {
	m := metricSaphanaSchemaMemoryUsedMax{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaSchemaOperationCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.schema.operation.count metric with initial data.
func (m *metricSaphanaSchemaOperationCount) init() {
	m.data.SetName("saphana.schema.operation.count")
	m.data.SetDescription("The number of operations done on all tables in schema.")
	m.data.SetUnit("{operations}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaSchemaOperationCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaAttributeValue string, schemaOperationTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Schema, pcommon.NewValueString(schemaAttributeValue))
	dp.Attributes().Insert(A.SchemaOperationType, pcommon.NewValueString(schemaOperationTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaSchemaOperationCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaSchemaOperationCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaSchemaOperationCount(settings MetricSettings) metricSaphanaSchemaOperationCount {
	m := metricSaphanaSchemaOperationCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaSchemaRecordCompressedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.schema.record.compressed.count metric with initial data.
func (m *metricSaphanaSchemaRecordCompressedCount) init() {
	m.data.SetName("saphana.schema.record.compressed.count")
	m.data.SetDescription("The number of entries in main during the last optimize compression run for all tables in schema.")
	m.data.SetUnit("{records}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaSchemaRecordCompressedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Schema, pcommon.NewValueString(schemaAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaSchemaRecordCompressedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaSchemaRecordCompressedCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaSchemaRecordCompressedCount(settings MetricSettings) metricSaphanaSchemaRecordCompressedCount {
	m := metricSaphanaSchemaRecordCompressedCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaSchemaRecordCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.schema.record.count metric with initial data.
func (m *metricSaphanaSchemaRecordCount) init() {
	m.data.SetName("saphana.schema.record.count")
	m.data.SetDescription("The number of records for all tables in schema.")
	m.data.SetUnit("{records}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaSchemaRecordCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaAttributeValue string, schemaRecordTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Schema, pcommon.NewValueString(schemaAttributeValue))
	dp.Attributes().Insert(A.SchemaRecordType, pcommon.NewValueString(schemaRecordTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaSchemaRecordCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaSchemaRecordCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaSchemaRecordCount(settings MetricSettings) metricSaphanaSchemaRecordCount {
	m := metricSaphanaSchemaRecordCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaServiceCodeSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.service.code_size metric with initial data.
func (m *metricSaphanaServiceCodeSize) init() {
	m.data.SetName("saphana.service.code_size")
	m.data.SetDescription("The service code size, including shared libraries.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaServiceCodeSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serviceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Service, pcommon.NewValueString(serviceAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaServiceCodeSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaServiceCodeSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaServiceCodeSize(settings MetricSettings) metricSaphanaServiceCodeSize {
	m := metricSaphanaServiceCodeSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaServiceCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.service.count metric with initial data.
func (m *metricSaphanaServiceCount) init() {
	m.data.SetName("saphana.service.count")
	m.data.SetDescription("The number of services in a given status.")
	m.data.SetUnit("{services}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaServiceCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serviceStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.ServiceStatus, pcommon.NewValueString(serviceStatusAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaServiceCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaServiceCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaServiceCount(settings MetricSettings) metricSaphanaServiceCount {
	m := metricSaphanaServiceCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaServiceMemoryCompactorsAllocated struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.service.memory.compactors.allocated metric with initial data.
func (m *metricSaphanaServiceMemoryCompactorsAllocated) init() {
	m.data.SetName("saphana.service.memory.compactors.allocated")
	m.data.SetDescription("The part of the memory pool that can potentially (if unpinned) be freed during a memory shortage.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaServiceMemoryCompactorsAllocated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serviceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Service, pcommon.NewValueString(serviceAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaServiceMemoryCompactorsAllocated) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaServiceMemoryCompactorsAllocated) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaServiceMemoryCompactorsAllocated(settings MetricSettings) metricSaphanaServiceMemoryCompactorsAllocated {
	m := metricSaphanaServiceMemoryCompactorsAllocated{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaServiceMemoryCompactorsFreeable struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.service.memory.compactors.freeable metric with initial data.
func (m *metricSaphanaServiceMemoryCompactorsFreeable) init() {
	m.data.SetName("saphana.service.memory.compactors.freeable")
	m.data.SetDescription("The memory that can be freed during a memory shortage.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaServiceMemoryCompactorsFreeable) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serviceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Service, pcommon.NewValueString(serviceAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaServiceMemoryCompactorsFreeable) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaServiceMemoryCompactorsFreeable) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaServiceMemoryCompactorsFreeable(settings MetricSettings) metricSaphanaServiceMemoryCompactorsFreeable {
	m := metricSaphanaServiceMemoryCompactorsFreeable{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaServiceMemoryEffectiveLimit struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.service.memory.effective_limit metric with initial data.
func (m *metricSaphanaServiceMemoryEffectiveLimit) init() {
	m.data.SetName("saphana.service.memory.effective_limit")
	m.data.SetDescription("The effective maximum memory pool size, calculated considering the pool sizes of other processes.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaServiceMemoryEffectiveLimit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serviceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Service, pcommon.NewValueString(serviceAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaServiceMemoryEffectiveLimit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaServiceMemoryEffectiveLimit) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaServiceMemoryEffectiveLimit(settings MetricSettings) metricSaphanaServiceMemoryEffectiveLimit {
	m := metricSaphanaServiceMemoryEffectiveLimit{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaServiceMemoryHeapCurrent struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.service.memory.heap.current metric with initial data.
func (m *metricSaphanaServiceMemoryHeapCurrent) init() {
	m.data.SetName("saphana.service.memory.heap.current")
	m.data.SetDescription("The size of the heap portion of the memory pool.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaServiceMemoryHeapCurrent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serviceAttributeValue string, memoryStateUsedFreeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Service, pcommon.NewValueString(serviceAttributeValue))
	dp.Attributes().Insert(A.MemoryStateUsedFree, pcommon.NewValueString(memoryStateUsedFreeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaServiceMemoryHeapCurrent) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaServiceMemoryHeapCurrent) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaServiceMemoryHeapCurrent(settings MetricSettings) metricSaphanaServiceMemoryHeapCurrent {
	m := metricSaphanaServiceMemoryHeapCurrent{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaServiceMemoryLimit struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.service.memory.limit metric with initial data.
func (m *metricSaphanaServiceMemoryLimit) init() {
	m.data.SetName("saphana.service.memory.limit")
	m.data.SetDescription("The configured maximum memory pool size.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaServiceMemoryLimit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serviceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Service, pcommon.NewValueString(serviceAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaServiceMemoryLimit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaServiceMemoryLimit) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaServiceMemoryLimit(settings MetricSettings) metricSaphanaServiceMemoryLimit {
	m := metricSaphanaServiceMemoryLimit{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaServiceMemorySharedCurrent struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.service.memory.shared.current metric with initial data.
func (m *metricSaphanaServiceMemorySharedCurrent) init() {
	m.data.SetName("saphana.service.memory.shared.current")
	m.data.SetDescription("The size of the shared portion of the memory pool.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaServiceMemorySharedCurrent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serviceAttributeValue string, memoryStateUsedFreeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Service, pcommon.NewValueString(serviceAttributeValue))
	dp.Attributes().Insert(A.MemoryStateUsedFree, pcommon.NewValueString(memoryStateUsedFreeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaServiceMemorySharedCurrent) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaServiceMemorySharedCurrent) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaServiceMemorySharedCurrent(settings MetricSettings) metricSaphanaServiceMemorySharedCurrent {
	m := metricSaphanaServiceMemorySharedCurrent{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaServiceMemoryUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.service.memory.used metric with initial data.
func (m *metricSaphanaServiceMemoryUsed) init() {
	m.data.SetName("saphana.service.memory.used")
	m.data.SetDescription("The used memory from the operating system perspective.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaServiceMemoryUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serviceAttributeValue string, serviceMemoryUsedTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Service, pcommon.NewValueString(serviceAttributeValue))
	dp.Attributes().Insert(A.ServiceMemoryUsedType, pcommon.NewValueString(serviceMemoryUsedTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaServiceMemoryUsed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaServiceMemoryUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaServiceMemoryUsed(settings MetricSettings) metricSaphanaServiceMemoryUsed {
	m := metricSaphanaServiceMemoryUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaServiceStackSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.service.stack_size metric with initial data.
func (m *metricSaphanaServiceStackSize) init() {
	m.data.SetName("saphana.service.stack_size")
	m.data.SetDescription("The service stack size.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaServiceStackSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serviceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Service, pcommon.NewValueString(serviceAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaServiceStackSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaServiceStackSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaServiceStackSize(settings MetricSettings) metricSaphanaServiceStackSize {
	m := metricSaphanaServiceStackSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaServiceThreadCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.service.thread.count metric with initial data.
func (m *metricSaphanaServiceThreadCount) init() {
	m.data.SetName("saphana.service.thread.count")
	m.data.SetDescription("The number of service threads in a given status.")
	m.data.SetUnit("{threads}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaServiceThreadCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, threadStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.ThreadStatus, pcommon.NewValueString(threadStatusAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaServiceThreadCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaServiceThreadCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaServiceThreadCount(settings MetricSettings) metricSaphanaServiceThreadCount {
	m := metricSaphanaServiceThreadCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaTransactionBlocked struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.transaction.blocked metric with initial data.
func (m *metricSaphanaTransactionBlocked) init() {
	m.data.SetName("saphana.transaction.blocked")
	m.data.SetDescription("The number of transactions waiting for a lock.")
	m.data.SetUnit("{transactions}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricSaphanaTransactionBlocked) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaTransactionBlocked) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaTransactionBlocked) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaTransactionBlocked(settings MetricSettings) metricSaphanaTransactionBlocked {
	m := metricSaphanaTransactionBlocked{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaTransactionCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.transaction.count metric with initial data.
func (m *metricSaphanaTransactionCount) init() {
	m.data.SetName("saphana.transaction.count")
	m.data.SetDescription("The number of transactions.")
	m.data.SetUnit("{transactions}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaTransactionCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, transactionTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.TransactionType, pcommon.NewValueString(transactionTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaTransactionCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaTransactionCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaTransactionCount(settings MetricSettings) metricSaphanaTransactionCount {
	m := metricSaphanaTransactionCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaUptime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.uptime metric with initial data.
func (m *metricSaphanaUptime) init() {
	m.data.SetName("saphana.uptime")
	m.data.SetDescription("The uptime of the database.")
	m.data.SetUnit("s")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaUptime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, systemAttributeValue string, databaseAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.System, pcommon.NewValueString(systemAttributeValue))
	dp.Attributes().Insert(A.Database, pcommon.NewValueString(databaseAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaUptime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaUptime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaUptime(settings MetricSettings) metricSaphanaUptime {
	m := metricSaphanaUptime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaVolumeOperationCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.volume.operation.count metric with initial data.
func (m *metricSaphanaVolumeOperationCount) init() {
	m.data.SetName("saphana.volume.operation.count")
	m.data.SetDescription("The number of operations executed.")
	m.data.SetUnit("{operations}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaVolumeOperationCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, pathAttributeValue string, diskUsageTypeAttributeValue string, volumeOperationTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Path, pcommon.NewValueString(pathAttributeValue))
	dp.Attributes().Insert(A.DiskUsageType, pcommon.NewValueString(diskUsageTypeAttributeValue))
	dp.Attributes().Insert(A.VolumeOperationType, pcommon.NewValueString(volumeOperationTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaVolumeOperationCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaVolumeOperationCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaVolumeOperationCount(settings MetricSettings) metricSaphanaVolumeOperationCount {
	m := metricSaphanaVolumeOperationCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaVolumeOperationSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.volume.operation.size metric with initial data.
func (m *metricSaphanaVolumeOperationSize) init() {
	m.data.SetName("saphana.volume.operation.size")
	m.data.SetDescription("The size of operations executed.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaVolumeOperationSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, pathAttributeValue string, diskUsageTypeAttributeValue string, volumeOperationTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Path, pcommon.NewValueString(pathAttributeValue))
	dp.Attributes().Insert(A.DiskUsageType, pcommon.NewValueString(diskUsageTypeAttributeValue))
	dp.Attributes().Insert(A.VolumeOperationType, pcommon.NewValueString(volumeOperationTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaVolumeOperationSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaVolumeOperationSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaVolumeOperationSize(settings MetricSettings) metricSaphanaVolumeOperationSize {
	m := metricSaphanaVolumeOperationSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSaphanaVolumeOperationTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills saphana.volume.operation.time metric with initial data.
func (m *metricSaphanaVolumeOperationTime) init() {
	m.data.SetName("saphana.volume.operation.time")
	m.data.SetDescription("The time spent executing operations.")
	m.data.SetUnit("ms")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSaphanaVolumeOperationTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, pathAttributeValue string, diskUsageTypeAttributeValue string, volumeOperationTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Path, pcommon.NewValueString(pathAttributeValue))
	dp.Attributes().Insert(A.DiskUsageType, pcommon.NewValueString(diskUsageTypeAttributeValue))
	dp.Attributes().Insert(A.VolumeOperationType, pcommon.NewValueString(volumeOperationTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSaphanaVolumeOperationTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSaphanaVolumeOperationTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSaphanaVolumeOperationTime(settings MetricSettings) metricSaphanaVolumeOperationTime {
	m := metricSaphanaVolumeOperationTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user settings.
type MetricsBuilder struct {
	startTime                                     pcommon.Timestamp // start time that will be applied to all recorded data points.
	metricsCapacity                               int               // maximum observed number of metrics per resource.
	resourceCapacity                              int               // maximum observed number of resource attributes.
	metricsBuffer                                 pmetric.Metrics   // accumulates metrics data before emitting.
	metricSaphanaAlertCount                       metricSaphanaAlertCount
	metricSaphanaBackupLatest                     metricSaphanaBackupLatest
	metricSaphanaColumnMemoryUsed                 metricSaphanaColumnMemoryUsed
	metricSaphanaComponentMemoryUsed              metricSaphanaComponentMemoryUsed
	metricSaphanaConnectionCount                  metricSaphanaConnectionCount
	metricSaphanaCPUUsed                          metricSaphanaCPUUsed
	metricSaphanaDiskSizeCurrent                  metricSaphanaDiskSizeCurrent
	metricSaphanaHostMemoryCurrent                metricSaphanaHostMemoryCurrent
	metricSaphanaHostSwapCurrent                  metricSaphanaHostSwapCurrent
	metricSaphanaInstanceCodeSize                 metricSaphanaInstanceCodeSize
	metricSaphanaInstanceMemoryCurrent            metricSaphanaInstanceMemoryCurrent
	metricSaphanaInstanceMemorySharedAllocated    metricSaphanaInstanceMemorySharedAllocated
	metricSaphanaInstanceMemoryUsedPeak           metricSaphanaInstanceMemoryUsedPeak
	metricSaphanaLicenseExpirationTime            metricSaphanaLicenseExpirationTime
	metricSaphanaLicenseLimit                     metricSaphanaLicenseLimit
	metricSaphanaLicensePeak                      metricSaphanaLicensePeak
	metricSaphanaNetworkRequestAverageTime        metricSaphanaNetworkRequestAverageTime
	metricSaphanaNetworkRequestCount              metricSaphanaNetworkRequestCount
	metricSaphanaNetworkRequestFinishedCount      metricSaphanaNetworkRequestFinishedCount
	metricSaphanaReplicationAverageTime           metricSaphanaReplicationAverageTime
	metricSaphanaReplicationBacklogSize           metricSaphanaReplicationBacklogSize
	metricSaphanaReplicationBacklogTime           metricSaphanaReplicationBacklogTime
	metricSaphanaRowStoreMemoryUsed               metricSaphanaRowStoreMemoryUsed
	metricSaphanaSchemaMemoryUsedCurrent          metricSaphanaSchemaMemoryUsedCurrent
	metricSaphanaSchemaMemoryUsedMax              metricSaphanaSchemaMemoryUsedMax
	metricSaphanaSchemaOperationCount             metricSaphanaSchemaOperationCount
	metricSaphanaSchemaRecordCompressedCount      metricSaphanaSchemaRecordCompressedCount
	metricSaphanaSchemaRecordCount                metricSaphanaSchemaRecordCount
	metricSaphanaServiceCodeSize                  metricSaphanaServiceCodeSize
	metricSaphanaServiceCount                     metricSaphanaServiceCount
	metricSaphanaServiceMemoryCompactorsAllocated metricSaphanaServiceMemoryCompactorsAllocated
	metricSaphanaServiceMemoryCompactorsFreeable  metricSaphanaServiceMemoryCompactorsFreeable
	metricSaphanaServiceMemoryEffectiveLimit      metricSaphanaServiceMemoryEffectiveLimit
	metricSaphanaServiceMemoryHeapCurrent         metricSaphanaServiceMemoryHeapCurrent
	metricSaphanaServiceMemoryLimit               metricSaphanaServiceMemoryLimit
	metricSaphanaServiceMemorySharedCurrent       metricSaphanaServiceMemorySharedCurrent
	metricSaphanaServiceMemoryUsed                metricSaphanaServiceMemoryUsed
	metricSaphanaServiceStackSize                 metricSaphanaServiceStackSize
	metricSaphanaServiceThreadCount               metricSaphanaServiceThreadCount
	metricSaphanaTransactionBlocked               metricSaphanaTransactionBlocked
	metricSaphanaTransactionCount                 metricSaphanaTransactionCount
	metricSaphanaUptime                           metricSaphanaUptime
	metricSaphanaVolumeOperationCount             metricSaphanaVolumeOperationCount
	metricSaphanaVolumeOperationSize              metricSaphanaVolumeOperationSize
	metricSaphanaVolumeOperationTime              metricSaphanaVolumeOperationTime
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func NewMetricsBuilder(settings MetricsSettings, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		startTime:                                     pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                                 pmetric.NewMetrics(),
		metricSaphanaAlertCount:                       newMetricSaphanaAlertCount(settings.SaphanaAlertCount),
		metricSaphanaBackupLatest:                     newMetricSaphanaBackupLatest(settings.SaphanaBackupLatest),
		metricSaphanaColumnMemoryUsed:                 newMetricSaphanaColumnMemoryUsed(settings.SaphanaColumnMemoryUsed),
		metricSaphanaComponentMemoryUsed:              newMetricSaphanaComponentMemoryUsed(settings.SaphanaComponentMemoryUsed),
		metricSaphanaConnectionCount:                  newMetricSaphanaConnectionCount(settings.SaphanaConnectionCount),
		metricSaphanaCPUUsed:                          newMetricSaphanaCPUUsed(settings.SaphanaCPUUsed),
		metricSaphanaDiskSizeCurrent:                  newMetricSaphanaDiskSizeCurrent(settings.SaphanaDiskSizeCurrent),
		metricSaphanaHostMemoryCurrent:                newMetricSaphanaHostMemoryCurrent(settings.SaphanaHostMemoryCurrent),
		metricSaphanaHostSwapCurrent:                  newMetricSaphanaHostSwapCurrent(settings.SaphanaHostSwapCurrent),
		metricSaphanaInstanceCodeSize:                 newMetricSaphanaInstanceCodeSize(settings.SaphanaInstanceCodeSize),
		metricSaphanaInstanceMemoryCurrent:            newMetricSaphanaInstanceMemoryCurrent(settings.SaphanaInstanceMemoryCurrent),
		metricSaphanaInstanceMemorySharedAllocated:    newMetricSaphanaInstanceMemorySharedAllocated(settings.SaphanaInstanceMemorySharedAllocated),
		metricSaphanaInstanceMemoryUsedPeak:           newMetricSaphanaInstanceMemoryUsedPeak(settings.SaphanaInstanceMemoryUsedPeak),
		metricSaphanaLicenseExpirationTime:            newMetricSaphanaLicenseExpirationTime(settings.SaphanaLicenseExpirationTime),
		metricSaphanaLicenseLimit:                     newMetricSaphanaLicenseLimit(settings.SaphanaLicenseLimit),
		metricSaphanaLicensePeak:                      newMetricSaphanaLicensePeak(settings.SaphanaLicensePeak),
		metricSaphanaNetworkRequestAverageTime:        newMetricSaphanaNetworkRequestAverageTime(settings.SaphanaNetworkRequestAverageTime),
		metricSaphanaNetworkRequestCount:              newMetricSaphanaNetworkRequestCount(settings.SaphanaNetworkRequestCount),
		metricSaphanaNetworkRequestFinishedCount:      newMetricSaphanaNetworkRequestFinishedCount(settings.SaphanaNetworkRequestFinishedCount),
		metricSaphanaReplicationAverageTime:           newMetricSaphanaReplicationAverageTime(settings.SaphanaReplicationAverageTime),
		metricSaphanaReplicationBacklogSize:           newMetricSaphanaReplicationBacklogSize(settings.SaphanaReplicationBacklogSize),
		metricSaphanaReplicationBacklogTime:           newMetricSaphanaReplicationBacklogTime(settings.SaphanaReplicationBacklogTime),
		metricSaphanaRowStoreMemoryUsed:               newMetricSaphanaRowStoreMemoryUsed(settings.SaphanaRowStoreMemoryUsed),
		metricSaphanaSchemaMemoryUsedCurrent:          newMetricSaphanaSchemaMemoryUsedCurrent(settings.SaphanaSchemaMemoryUsedCurrent),
		metricSaphanaSchemaMemoryUsedMax:              newMetricSaphanaSchemaMemoryUsedMax(settings.SaphanaSchemaMemoryUsedMax),
		metricSaphanaSchemaOperationCount:             newMetricSaphanaSchemaOperationCount(settings.SaphanaSchemaOperationCount),
		metricSaphanaSchemaRecordCompressedCount:      newMetricSaphanaSchemaRecordCompressedCount(settings.SaphanaSchemaRecordCompressedCount),
		metricSaphanaSchemaRecordCount:                newMetricSaphanaSchemaRecordCount(settings.SaphanaSchemaRecordCount),
		metricSaphanaServiceCodeSize:                  newMetricSaphanaServiceCodeSize(settings.SaphanaServiceCodeSize),
		metricSaphanaServiceCount:                     newMetricSaphanaServiceCount(settings.SaphanaServiceCount),
		metricSaphanaServiceMemoryCompactorsAllocated: newMetricSaphanaServiceMemoryCompactorsAllocated(settings.SaphanaServiceMemoryCompactorsAllocated),
		metricSaphanaServiceMemoryCompactorsFreeable:  newMetricSaphanaServiceMemoryCompactorsFreeable(settings.SaphanaServiceMemoryCompactorsFreeable),
		metricSaphanaServiceMemoryEffectiveLimit:      newMetricSaphanaServiceMemoryEffectiveLimit(settings.SaphanaServiceMemoryEffectiveLimit),
		metricSaphanaServiceMemoryHeapCurrent:         newMetricSaphanaServiceMemoryHeapCurrent(settings.SaphanaServiceMemoryHeapCurrent),
		metricSaphanaServiceMemoryLimit:               newMetricSaphanaServiceMemoryLimit(settings.SaphanaServiceMemoryLimit),
		metricSaphanaServiceMemorySharedCurrent:       newMetricSaphanaServiceMemorySharedCurrent(settings.SaphanaServiceMemorySharedCurrent),
		metricSaphanaServiceMemoryUsed:                newMetricSaphanaServiceMemoryUsed(settings.SaphanaServiceMemoryUsed),
		metricSaphanaServiceStackSize:                 newMetricSaphanaServiceStackSize(settings.SaphanaServiceStackSize),
		metricSaphanaServiceThreadCount:               newMetricSaphanaServiceThreadCount(settings.SaphanaServiceThreadCount),
		metricSaphanaTransactionBlocked:               newMetricSaphanaTransactionBlocked(settings.SaphanaTransactionBlocked),
		metricSaphanaTransactionCount:                 newMetricSaphanaTransactionCount(settings.SaphanaTransactionCount),
		metricSaphanaUptime:                           newMetricSaphanaUptime(settings.SaphanaUptime),
		metricSaphanaVolumeOperationCount:             newMetricSaphanaVolumeOperationCount(settings.SaphanaVolumeOperationCount),
		metricSaphanaVolumeOperationSize:              newMetricSaphanaVolumeOperationSize(settings.SaphanaVolumeOperationSize),
		metricSaphanaVolumeOperationTime:              newMetricSaphanaVolumeOperationTime(settings.SaphanaVolumeOperationTime),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
	if mb.resourceCapacity < rm.Resource().Attributes().Len() {
		mb.resourceCapacity = rm.Resource().Attributes().Len()
	}
}

// ResourceOption applies changes to provided resource.
type ResourceOption func(pcommon.Resource)

// WithDbSystem sets provided value as "db.system" attribute for current resource.
func WithDbSystem(val string) ResourceOption {
	return func(r pcommon.Resource) {
		r.Attributes().UpsertString("db.system", val)
	}
}

// WithSaphanaHost sets provided value as "saphana.host" attribute for current resource.
func WithSaphanaHost(val string) ResourceOption {
	return func(r pcommon.Resource) {
		r.Attributes().UpsertString("saphana.host", val)
	}
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead. Resource attributes should be provided as ResourceOption arguments.
func (mb *MetricsBuilder) EmitForResource(ro ...ResourceOption) {
	rm := pmetric.NewResourceMetrics()
	rm.Resource().Attributes().EnsureCapacity(mb.resourceCapacity)
	for _, op := range ro {
		op(rm.Resource())
	}
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName("otelcol/saphanareceiver")
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricSaphanaAlertCount.emit(ils.Metrics())
	mb.metricSaphanaBackupLatest.emit(ils.Metrics())
	mb.metricSaphanaColumnMemoryUsed.emit(ils.Metrics())
	mb.metricSaphanaComponentMemoryUsed.emit(ils.Metrics())
	mb.metricSaphanaConnectionCount.emit(ils.Metrics())
	mb.metricSaphanaCPUUsed.emit(ils.Metrics())
	mb.metricSaphanaDiskSizeCurrent.emit(ils.Metrics())
	mb.metricSaphanaHostMemoryCurrent.emit(ils.Metrics())
	mb.metricSaphanaHostSwapCurrent.emit(ils.Metrics())
	mb.metricSaphanaInstanceCodeSize.emit(ils.Metrics())
	mb.metricSaphanaInstanceMemoryCurrent.emit(ils.Metrics())
	mb.metricSaphanaInstanceMemorySharedAllocated.emit(ils.Metrics())
	mb.metricSaphanaInstanceMemoryUsedPeak.emit(ils.Metrics())
	mb.metricSaphanaLicenseExpirationTime.emit(ils.Metrics())
	mb.metricSaphanaLicenseLimit.emit(ils.Metrics())
	mb.metricSaphanaLicensePeak.emit(ils.Metrics())
	mb.metricSaphanaNetworkRequestAverageTime.emit(ils.Metrics())
	mb.metricSaphanaNetworkRequestCount.emit(ils.Metrics())
	mb.metricSaphanaNetworkRequestFinishedCount.emit(ils.Metrics())
	mb.metricSaphanaReplicationAverageTime.emit(ils.Metrics())
	mb.metricSaphanaReplicationBacklogSize.emit(ils.Metrics())
	mb.metricSaphanaReplicationBacklogTime.emit(ils.Metrics())
	mb.metricSaphanaRowStoreMemoryUsed.emit(ils.Metrics())
	mb.metricSaphanaSchemaMemoryUsedCurrent.emit(ils.Metrics())
	mb.metricSaphanaSchemaMemoryUsedMax.emit(ils.Metrics())
	mb.metricSaphanaSchemaOperationCount.emit(ils.Metrics())
	mb.metricSaphanaSchemaRecordCompressedCount.emit(ils.Metrics())
	mb.metricSaphanaSchemaRecordCount.emit(ils.Metrics())
	mb.metricSaphanaServiceCodeSize.emit(ils.Metrics())
	mb.metricSaphanaServiceCount.emit(ils.Metrics())
	mb.metricSaphanaServiceMemoryCompactorsAllocated.emit(ils.Metrics())
	mb.metricSaphanaServiceMemoryCompactorsFreeable.emit(ils.Metrics())
	mb.metricSaphanaServiceMemoryEffectiveLimit.emit(ils.Metrics())
	mb.metricSaphanaServiceMemoryHeapCurrent.emit(ils.Metrics())
	mb.metricSaphanaServiceMemoryLimit.emit(ils.Metrics())
	mb.metricSaphanaServiceMemorySharedCurrent.emit(ils.Metrics())
	mb.metricSaphanaServiceMemoryUsed.emit(ils.Metrics())
	mb.metricSaphanaServiceStackSize.emit(ils.Metrics())
	mb.metricSaphanaServiceThreadCount.emit(ils.Metrics())
	mb.metricSaphanaTransactionBlocked.emit(ils.Metrics())
	mb.metricSaphanaTransactionCount.emit(ils.Metrics())
	mb.metricSaphanaUptime.emit(ils.Metrics())
	mb.metricSaphanaVolumeOperationCount.emit(ils.Metrics())
	mb.metricSaphanaVolumeOperationSize.emit(ils.Metrics())
	mb.metricSaphanaVolumeOperationTime.emit(ils.Metrics())
	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user settings, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(ro ...ResourceOption) pmetric.Metrics {
	mb.EmitForResource(ro...)
	metrics := pmetric.NewMetrics()
	mb.metricsBuffer.MoveTo(metrics)
	return metrics
}

// RecordSaphanaAlertCountDataPoint adds a data point to saphana.alert.count metric.
func (mb *MetricsBuilder) RecordSaphanaAlertCountDataPoint(ts pcommon.Timestamp, val string, alertRatingAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaAlertCount, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaAlertCount.recordDataPoint(mb.startTime, ts, i, alertRatingAttributeValue)
	}
	return nil
}

// RecordSaphanaBackupLatestDataPoint adds a data point to saphana.backup.latest metric.
func (mb *MetricsBuilder) RecordSaphanaBackupLatestDataPoint(ts pcommon.Timestamp, val string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaBackupLatest, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaBackupLatest.recordDataPoint(mb.startTime, ts, i)
	}
	return nil
}

// RecordSaphanaColumnMemoryUsedDataPoint adds a data point to saphana.column.memory.used metric.
func (mb *MetricsBuilder) RecordSaphanaColumnMemoryUsedDataPoint(ts pcommon.Timestamp, val string, columnMemoryTypeAttributeValue string, columnMemorySubtypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaColumnMemoryUsed, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaColumnMemoryUsed.recordDataPoint(mb.startTime, ts, i, columnMemoryTypeAttributeValue, columnMemorySubtypeAttributeValue)
	}
	return nil
}

// RecordSaphanaComponentMemoryUsedDataPoint adds a data point to saphana.component.memory.used metric.
func (mb *MetricsBuilder) RecordSaphanaComponentMemoryUsedDataPoint(ts pcommon.Timestamp, val string, componentAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaComponentMemoryUsed, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaComponentMemoryUsed.recordDataPoint(mb.startTime, ts, i, componentAttributeValue)
	}
	return nil
}

// RecordSaphanaConnectionCountDataPoint adds a data point to saphana.connection.count metric.
func (mb *MetricsBuilder) RecordSaphanaConnectionCountDataPoint(ts pcommon.Timestamp, val string, connectionStatusAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaConnectionCount, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaConnectionCount.recordDataPoint(mb.startTime, ts, i, connectionStatusAttributeValue)
	}
	return nil
}

// RecordSaphanaCPUUsedDataPoint adds a data point to saphana.cpu.used metric.
func (mb *MetricsBuilder) RecordSaphanaCPUUsedDataPoint(ts pcommon.Timestamp, val string, cpuTypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaCPUUsed, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaCPUUsed.recordDataPoint(mb.startTime, ts, i, cpuTypeAttributeValue)
	}
	return nil
}

// RecordSaphanaDiskSizeCurrentDataPoint adds a data point to saphana.disk.size.current metric.
func (mb *MetricsBuilder) RecordSaphanaDiskSizeCurrentDataPoint(ts pcommon.Timestamp, val string, pathAttributeValue string, diskUsageTypeAttributeValue string, diskStateUsedFreeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaDiskSizeCurrent, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaDiskSizeCurrent.recordDataPoint(mb.startTime, ts, i, pathAttributeValue, diskUsageTypeAttributeValue, diskStateUsedFreeAttributeValue)
	}
	return nil
}

// RecordSaphanaHostMemoryCurrentDataPoint adds a data point to saphana.host.memory.current metric.
func (mb *MetricsBuilder) RecordSaphanaHostMemoryCurrentDataPoint(ts pcommon.Timestamp, val string, memoryStateUsedFreeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaHostMemoryCurrent, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaHostMemoryCurrent.recordDataPoint(mb.startTime, ts, i, memoryStateUsedFreeAttributeValue)
	}
	return nil
}

// RecordSaphanaHostSwapCurrentDataPoint adds a data point to saphana.host.swap.current metric.
func (mb *MetricsBuilder) RecordSaphanaHostSwapCurrentDataPoint(ts pcommon.Timestamp, val string, hostSwapStateAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaHostSwapCurrent, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaHostSwapCurrent.recordDataPoint(mb.startTime, ts, i, hostSwapStateAttributeValue)
	}
	return nil
}

// RecordSaphanaInstanceCodeSizeDataPoint adds a data point to saphana.instance.code_size metric.
func (mb *MetricsBuilder) RecordSaphanaInstanceCodeSizeDataPoint(ts pcommon.Timestamp, val string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaInstanceCodeSize, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaInstanceCodeSize.recordDataPoint(mb.startTime, ts, i)
	}
	return nil
}

// RecordSaphanaInstanceMemoryCurrentDataPoint adds a data point to saphana.instance.memory.current metric.
func (mb *MetricsBuilder) RecordSaphanaInstanceMemoryCurrentDataPoint(ts pcommon.Timestamp, val string, memoryStateUsedFreeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaInstanceMemoryCurrent, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaInstanceMemoryCurrent.recordDataPoint(mb.startTime, ts, i, memoryStateUsedFreeAttributeValue)
	}
	return nil
}

// RecordSaphanaInstanceMemorySharedAllocatedDataPoint adds a data point to saphana.instance.memory.shared.allocated metric.
func (mb *MetricsBuilder) RecordSaphanaInstanceMemorySharedAllocatedDataPoint(ts pcommon.Timestamp, val string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaInstanceMemorySharedAllocated, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaInstanceMemorySharedAllocated.recordDataPoint(mb.startTime, ts, i)
	}
	return nil
}

// RecordSaphanaInstanceMemoryUsedPeakDataPoint adds a data point to saphana.instance.memory.used.peak metric.
func (mb *MetricsBuilder) RecordSaphanaInstanceMemoryUsedPeakDataPoint(ts pcommon.Timestamp, val string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaInstanceMemoryUsedPeak, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaInstanceMemoryUsedPeak.recordDataPoint(mb.startTime, ts, i)
	}
	return nil
}

// RecordSaphanaLicenseExpirationTimeDataPoint adds a data point to saphana.license.expiration.time metric.
func (mb *MetricsBuilder) RecordSaphanaLicenseExpirationTimeDataPoint(ts pcommon.Timestamp, val string, systemAttributeValue string, productAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaLicenseExpirationTime, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaLicenseExpirationTime.recordDataPoint(mb.startTime, ts, i, systemAttributeValue, productAttributeValue)
	}
	return nil
}

// RecordSaphanaLicenseLimitDataPoint adds a data point to saphana.license.limit metric.
func (mb *MetricsBuilder) RecordSaphanaLicenseLimitDataPoint(ts pcommon.Timestamp, val string, systemAttributeValue string, productAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaLicenseLimit, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaLicenseLimit.recordDataPoint(mb.startTime, ts, i, systemAttributeValue, productAttributeValue)
	}
	return nil
}

// RecordSaphanaLicensePeakDataPoint adds a data point to saphana.license.peak metric.
func (mb *MetricsBuilder) RecordSaphanaLicensePeakDataPoint(ts pcommon.Timestamp, val string, systemAttributeValue string, productAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaLicensePeak, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaLicensePeak.recordDataPoint(mb.startTime, ts, i, systemAttributeValue, productAttributeValue)
	}
	return nil
}

// RecordSaphanaNetworkRequestAverageTimeDataPoint adds a data point to saphana.network.request.average_time metric.
func (mb *MetricsBuilder) RecordSaphanaNetworkRequestAverageTimeDataPoint(ts pcommon.Timestamp, val string) error {
	if f, err := strconv.ParseFloat(val, 64); err != nil {
		return fmt.Errorf("failed to parse float for SaphanaNetworkRequestAverageTime, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaNetworkRequestAverageTime.recordDataPoint(mb.startTime, ts, f)
	}
	return nil
}

// RecordSaphanaNetworkRequestCountDataPoint adds a data point to saphana.network.request.count metric.
func (mb *MetricsBuilder) RecordSaphanaNetworkRequestCountDataPoint(ts pcommon.Timestamp, val string, activePendingRequestStateAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaNetworkRequestCount, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaNetworkRequestCount.recordDataPoint(mb.startTime, ts, i, activePendingRequestStateAttributeValue)
	}
	return nil
}

// RecordSaphanaNetworkRequestFinishedCountDataPoint adds a data point to saphana.network.request.finished.count metric.
func (mb *MetricsBuilder) RecordSaphanaNetworkRequestFinishedCountDataPoint(ts pcommon.Timestamp, val string, internalExternalRequestTypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaNetworkRequestFinishedCount, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaNetworkRequestFinishedCount.recordDataPoint(mb.startTime, ts, i, internalExternalRequestTypeAttributeValue)
	}
	return nil
}

// RecordSaphanaReplicationAverageTimeDataPoint adds a data point to saphana.replication.average_time metric.
func (mb *MetricsBuilder) RecordSaphanaReplicationAverageTimeDataPoint(ts pcommon.Timestamp, val string, primaryHostAttributeValue string, secondaryHostAttributeValue string, portAttributeValue string, replicationModeAttributeValue string) error {
	if f, err := strconv.ParseFloat(val, 64); err != nil {
		return fmt.Errorf("failed to parse float for SaphanaReplicationAverageTime, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaReplicationAverageTime.recordDataPoint(mb.startTime, ts, f, primaryHostAttributeValue, secondaryHostAttributeValue, portAttributeValue, replicationModeAttributeValue)
	}
	return nil
}

// RecordSaphanaReplicationBacklogSizeDataPoint adds a data point to saphana.replication.backlog.size metric.
func (mb *MetricsBuilder) RecordSaphanaReplicationBacklogSizeDataPoint(ts pcommon.Timestamp, val string, primaryHostAttributeValue string, secondaryHostAttributeValue string, portAttributeValue string, replicationModeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaReplicationBacklogSize, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaReplicationBacklogSize.recordDataPoint(mb.startTime, ts, i, primaryHostAttributeValue, secondaryHostAttributeValue, portAttributeValue, replicationModeAttributeValue)
	}
	return nil
}

// RecordSaphanaReplicationBacklogTimeDataPoint adds a data point to saphana.replication.backlog.time metric.
func (mb *MetricsBuilder) RecordSaphanaReplicationBacklogTimeDataPoint(ts pcommon.Timestamp, val string, primaryHostAttributeValue string, secondaryHostAttributeValue string, portAttributeValue string, replicationModeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaReplicationBacklogTime, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaReplicationBacklogTime.recordDataPoint(mb.startTime, ts, i, primaryHostAttributeValue, secondaryHostAttributeValue, portAttributeValue, replicationModeAttributeValue)
	}
	return nil
}

// RecordSaphanaRowStoreMemoryUsedDataPoint adds a data point to saphana.row_store.memory.used metric.
func (mb *MetricsBuilder) RecordSaphanaRowStoreMemoryUsedDataPoint(ts pcommon.Timestamp, val string, rowMemoryTypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaRowStoreMemoryUsed, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaRowStoreMemoryUsed.recordDataPoint(mb.startTime, ts, i, rowMemoryTypeAttributeValue)
	}
	return nil
}

// RecordSaphanaSchemaMemoryUsedCurrentDataPoint adds a data point to saphana.schema.memory.used.current metric.
func (mb *MetricsBuilder) RecordSaphanaSchemaMemoryUsedCurrentDataPoint(ts pcommon.Timestamp, val string, schemaAttributeValue string, schemaMemoryTypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaSchemaMemoryUsedCurrent, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaSchemaMemoryUsedCurrent.recordDataPoint(mb.startTime, ts, i, schemaAttributeValue, schemaMemoryTypeAttributeValue)
	}
	return nil
}

// RecordSaphanaSchemaMemoryUsedMaxDataPoint adds a data point to saphana.schema.memory.used.max metric.
func (mb *MetricsBuilder) RecordSaphanaSchemaMemoryUsedMaxDataPoint(ts pcommon.Timestamp, val string, schemaAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaSchemaMemoryUsedMax, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaSchemaMemoryUsedMax.recordDataPoint(mb.startTime, ts, i, schemaAttributeValue)
	}
	return nil
}

// RecordSaphanaSchemaOperationCountDataPoint adds a data point to saphana.schema.operation.count metric.
func (mb *MetricsBuilder) RecordSaphanaSchemaOperationCountDataPoint(ts pcommon.Timestamp, val string, schemaAttributeValue string, schemaOperationTypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaSchemaOperationCount, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaSchemaOperationCount.recordDataPoint(mb.startTime, ts, i, schemaAttributeValue, schemaOperationTypeAttributeValue)
	}
	return nil
}

// RecordSaphanaSchemaRecordCompressedCountDataPoint adds a data point to saphana.schema.record.compressed.count metric.
func (mb *MetricsBuilder) RecordSaphanaSchemaRecordCompressedCountDataPoint(ts pcommon.Timestamp, val string, schemaAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaSchemaRecordCompressedCount, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaSchemaRecordCompressedCount.recordDataPoint(mb.startTime, ts, i, schemaAttributeValue)
	}
	return nil
}

// RecordSaphanaSchemaRecordCountDataPoint adds a data point to saphana.schema.record.count metric.
func (mb *MetricsBuilder) RecordSaphanaSchemaRecordCountDataPoint(ts pcommon.Timestamp, val string, schemaAttributeValue string, schemaRecordTypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaSchemaRecordCount, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaSchemaRecordCount.recordDataPoint(mb.startTime, ts, i, schemaAttributeValue, schemaRecordTypeAttributeValue)
	}
	return nil
}

// RecordSaphanaServiceCodeSizeDataPoint adds a data point to saphana.service.code_size metric.
func (mb *MetricsBuilder) RecordSaphanaServiceCodeSizeDataPoint(ts pcommon.Timestamp, val string, serviceAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaServiceCodeSize, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaServiceCodeSize.recordDataPoint(mb.startTime, ts, i, serviceAttributeValue)
	}
	return nil
}

// RecordSaphanaServiceCountDataPoint adds a data point to saphana.service.count metric.
func (mb *MetricsBuilder) RecordSaphanaServiceCountDataPoint(ts pcommon.Timestamp, val string, serviceStatusAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaServiceCount, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaServiceCount.recordDataPoint(mb.startTime, ts, i, serviceStatusAttributeValue)
	}
	return nil
}

// RecordSaphanaServiceMemoryCompactorsAllocatedDataPoint adds a data point to saphana.service.memory.compactors.allocated metric.
func (mb *MetricsBuilder) RecordSaphanaServiceMemoryCompactorsAllocatedDataPoint(ts pcommon.Timestamp, val string, serviceAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaServiceMemoryCompactorsAllocated, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaServiceMemoryCompactorsAllocated.recordDataPoint(mb.startTime, ts, i, serviceAttributeValue)
	}
	return nil
}

// RecordSaphanaServiceMemoryCompactorsFreeableDataPoint adds a data point to saphana.service.memory.compactors.freeable metric.
func (mb *MetricsBuilder) RecordSaphanaServiceMemoryCompactorsFreeableDataPoint(ts pcommon.Timestamp, val string, serviceAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaServiceMemoryCompactorsFreeable, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaServiceMemoryCompactorsFreeable.recordDataPoint(mb.startTime, ts, i, serviceAttributeValue)
	}
	return nil
}

// RecordSaphanaServiceMemoryEffectiveLimitDataPoint adds a data point to saphana.service.memory.effective_limit metric.
func (mb *MetricsBuilder) RecordSaphanaServiceMemoryEffectiveLimitDataPoint(ts pcommon.Timestamp, val string, serviceAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaServiceMemoryEffectiveLimit, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaServiceMemoryEffectiveLimit.recordDataPoint(mb.startTime, ts, i, serviceAttributeValue)
	}
	return nil
}

// RecordSaphanaServiceMemoryHeapCurrentDataPoint adds a data point to saphana.service.memory.heap.current metric.
func (mb *MetricsBuilder) RecordSaphanaServiceMemoryHeapCurrentDataPoint(ts pcommon.Timestamp, val string, serviceAttributeValue string, memoryStateUsedFreeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaServiceMemoryHeapCurrent, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaServiceMemoryHeapCurrent.recordDataPoint(mb.startTime, ts, i, serviceAttributeValue, memoryStateUsedFreeAttributeValue)
	}
	return nil
}

// RecordSaphanaServiceMemoryLimitDataPoint adds a data point to saphana.service.memory.limit metric.
func (mb *MetricsBuilder) RecordSaphanaServiceMemoryLimitDataPoint(ts pcommon.Timestamp, val string, serviceAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaServiceMemoryLimit, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaServiceMemoryLimit.recordDataPoint(mb.startTime, ts, i, serviceAttributeValue)
	}
	return nil
}

// RecordSaphanaServiceMemorySharedCurrentDataPoint adds a data point to saphana.service.memory.shared.current metric.
func (mb *MetricsBuilder) RecordSaphanaServiceMemorySharedCurrentDataPoint(ts pcommon.Timestamp, val string, serviceAttributeValue string, memoryStateUsedFreeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaServiceMemorySharedCurrent, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaServiceMemorySharedCurrent.recordDataPoint(mb.startTime, ts, i, serviceAttributeValue, memoryStateUsedFreeAttributeValue)
	}
	return nil
}

// RecordSaphanaServiceMemoryUsedDataPoint adds a data point to saphana.service.memory.used metric.
func (mb *MetricsBuilder) RecordSaphanaServiceMemoryUsedDataPoint(ts pcommon.Timestamp, val string, serviceAttributeValue string, serviceMemoryUsedTypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaServiceMemoryUsed, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaServiceMemoryUsed.recordDataPoint(mb.startTime, ts, i, serviceAttributeValue, serviceMemoryUsedTypeAttributeValue)
	}
	return nil
}

// RecordSaphanaServiceStackSizeDataPoint adds a data point to saphana.service.stack_size metric.
func (mb *MetricsBuilder) RecordSaphanaServiceStackSizeDataPoint(ts pcommon.Timestamp, val string, serviceAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaServiceStackSize, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaServiceStackSize.recordDataPoint(mb.startTime, ts, i, serviceAttributeValue)
	}
	return nil
}

// RecordSaphanaServiceThreadCountDataPoint adds a data point to saphana.service.thread.count metric.
func (mb *MetricsBuilder) RecordSaphanaServiceThreadCountDataPoint(ts pcommon.Timestamp, val string, threadStatusAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaServiceThreadCount, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaServiceThreadCount.recordDataPoint(mb.startTime, ts, i, threadStatusAttributeValue)
	}
	return nil
}

// RecordSaphanaTransactionBlockedDataPoint adds a data point to saphana.transaction.blocked metric.
func (mb *MetricsBuilder) RecordSaphanaTransactionBlockedDataPoint(ts pcommon.Timestamp, val string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaTransactionBlocked, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaTransactionBlocked.recordDataPoint(mb.startTime, ts, i)
	}
	return nil
}

// RecordSaphanaTransactionCountDataPoint adds a data point to saphana.transaction.count metric.
func (mb *MetricsBuilder) RecordSaphanaTransactionCountDataPoint(ts pcommon.Timestamp, val string, transactionTypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaTransactionCount, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaTransactionCount.recordDataPoint(mb.startTime, ts, i, transactionTypeAttributeValue)
	}
	return nil
}

// RecordSaphanaUptimeDataPoint adds a data point to saphana.uptime metric.
func (mb *MetricsBuilder) RecordSaphanaUptimeDataPoint(ts pcommon.Timestamp, val string, systemAttributeValue string, databaseAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaUptime, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaUptime.recordDataPoint(mb.startTime, ts, i, systemAttributeValue, databaseAttributeValue)
	}
	return nil
}

// RecordSaphanaVolumeOperationCountDataPoint adds a data point to saphana.volume.operation.count metric.
func (mb *MetricsBuilder) RecordSaphanaVolumeOperationCountDataPoint(ts pcommon.Timestamp, val string, pathAttributeValue string, diskUsageTypeAttributeValue string, volumeOperationTypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaVolumeOperationCount, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaVolumeOperationCount.recordDataPoint(mb.startTime, ts, i, pathAttributeValue, diskUsageTypeAttributeValue, volumeOperationTypeAttributeValue)
	}
	return nil
}

// RecordSaphanaVolumeOperationSizeDataPoint adds a data point to saphana.volume.operation.size metric.
func (mb *MetricsBuilder) RecordSaphanaVolumeOperationSizeDataPoint(ts pcommon.Timestamp, val string, pathAttributeValue string, diskUsageTypeAttributeValue string, volumeOperationTypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaVolumeOperationSize, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaVolumeOperationSize.recordDataPoint(mb.startTime, ts, i, pathAttributeValue, diskUsageTypeAttributeValue, volumeOperationTypeAttributeValue)
	}
	return nil
}

// RecordSaphanaVolumeOperationTimeDataPoint adds a data point to saphana.volume.operation.time metric.
func (mb *MetricsBuilder) RecordSaphanaVolumeOperationTimeDataPoint(ts pcommon.Timestamp, val string, pathAttributeValue string, diskUsageTypeAttributeValue string, volumeOperationTypeAttributeValue string) error {
	if i, err := strconv.ParseInt(val, 10, 64); err != nil {
		return fmt.Errorf("failed to parse int for SaphanaVolumeOperationTime, value was %s: %w", val, err)
	} else {
		mb.metricSaphanaVolumeOperationTime.recordDataPoint(mb.startTime, ts, i, pathAttributeValue, diskUsageTypeAttributeValue, volumeOperationTypeAttributeValue)
	}
	return nil
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}

// Attributes contains the possible metric attributes that can be used.
var Attributes = struct {
	// ActivePendingRequestState (The state of network request.)
	ActivePendingRequestState string
	// AlertRating (The alert rating.)
	AlertRating string
	// ColumnMemorySubtype (The subtype of column store memory.)
	ColumnMemorySubtype string
	// ColumnMemoryType (The type of column store memory.)
	ColumnMemoryType string
	// Component (The SAP HANA component.)
	Component string
	// ConnectionStatus (The connection status.)
	ConnectionStatus string
	// CPUType (The type of cpu.)
	CPUType string
	// Database (The SAP HANA database.)
	Database string
	// DiskStateUsedFree (The state of the disk storage.)
	DiskStateUsedFree string
	// DiskUsageType (The SAP HANA disk & volume usage type.)
	DiskUsageType string
	// HostSwapState (The state of swap data.)
	HostSwapState string
	// InternalExternalRequestType (The type of network request.)
	InternalExternalRequestType string
	// MemoryStateUsedFree (The state of memory.)
	MemoryStateUsedFree string
	// Path (The SAP HANA disk path.)
	Path string
	// Port (The SAP HANA port.)
	Port string
	// PrimaryHost (The primary SAP HANA host in replication.)
	PrimaryHost string
	// Product (The SAP HANA product.)
	Product string
	// ReplicationMode (The replication mode.)
	ReplicationMode string
	// RowMemoryType (The type of row store memory.)
	RowMemoryType string
	// Schema (The SAP HANA schema.)
	Schema string
	// SchemaMemoryType (The type of schema memory.)
	SchemaMemoryType string
	// SchemaOperationType (The type of operation.)
	SchemaOperationType string
	// SchemaRecordType (The type of schema record.)
	SchemaRecordType string
	// SecondaryHost (The secondary SAP HANA host in replication.)
	SecondaryHost string
	// Service (The SAP HANA service.)
	Service string
	// ServiceMemoryUsedType (The type of service memory.)
	ServiceMemoryUsedType string
	// ServiceStatus (The status of services.)
	ServiceStatus string
	// System (The SAP HANA system.)
	System string
	// ThreadStatus (The status of threads.)
	ThreadStatus string
	// TransactionType (The transaction type.)
	TransactionType string
	// VolumeOperationType (The type of operation.)
	VolumeOperationType string
}{
	"state",
	"rating",
	"subtype",
	"type",
	"component",
	"status",
	"type",
	"database",
	"state",
	"usage_type",
	"state",
	"type",
	"state",
	"path",
	"port",
	"primary",
	"product",
	"mode",
	"type",
	"schema",
	"type",
	"type",
	"type",
	"secondary",
	"service",
	"type",
	"status",
	"system",
	"status",
	"type",
	"type",
}

// A is an alias for Attributes.
var A = Attributes

// AttributeActivePendingRequestState are the possible values that the attribute "active_pending_request_state" can have.
var AttributeActivePendingRequestState = struct {
	Active  string
	Pending string
}{
	"active",
	"pending",
}

// AttributeColumnMemorySubtype are the possible values that the attribute "column_memory_subtype" can have.
var AttributeColumnMemorySubtype = struct {
	Data  string
	Dict  string
	Index string
	Misc  string
}{
	"data",
	"dict",
	"index",
	"misc",
}

// AttributeColumnMemoryType are the possible values that the attribute "column_memory_type" can have.
var AttributeColumnMemoryType = struct {
	Main  string
	Delta string
}{
	"main",
	"delta",
}

// AttributeConnectionStatus are the possible values that the attribute "connection_status" can have.
var AttributeConnectionStatus = struct {
	Running  string
	Idle     string
	Queueing string
}{
	"running",
	"idle",
	"queueing",
}

// AttributeCPUType are the possible values that the attribute "cpu_type" can have.
var AttributeCPUType = struct {
	User   string
	System string
	IoWait string
	Idle   string
}{
	"user",
	"system",
	"io_wait",
	"idle",
}

// AttributeDiskStateUsedFree are the possible values that the attribute "disk_state_used_free" can have.
var AttributeDiskStateUsedFree = struct {
	Used string
	Free string
}{
	"used",
	"free",
}

// AttributeHostSwapState are the possible values that the attribute "host_swap_state" can have.
var AttributeHostSwapState = struct {
	Used string
	Free string
}{
	"used",
	"free",
}

// AttributeInternalExternalRequestType are the possible values that the attribute "internal_external_request_type" can have.
var AttributeInternalExternalRequestType = struct {
	Internal string
	External string
}{
	"internal",
	"external",
}

// AttributeMemoryStateUsedFree are the possible values that the attribute "memory_state_used_free" can have.
var AttributeMemoryStateUsedFree = struct {
	Used string
	Free string
}{
	"used",
	"free",
}

// AttributeRowMemoryType are the possible values that the attribute "row_memory_type" can have.
var AttributeRowMemoryType = struct {
	Fixed    string
	Variable string
}{
	"fixed",
	"variable",
}

// AttributeSchemaMemoryType are the possible values that the attribute "schema_memory_type" can have.
var AttributeSchemaMemoryType = struct {
	Main         string
	Delta        string
	HistoryMain  string
	HistoryDelta string
}{
	"main",
	"delta",
	"history_main",
	"history_delta",
}

// AttributeSchemaOperationType are the possible values that the attribute "schema_operation_type" can have.
var AttributeSchemaOperationType = struct {
	Read  string
	Write string
	Merge string
}{
	"read",
	"write",
	"merge",
}

// AttributeSchemaRecordType are the possible values that the attribute "schema_record_type" can have.
var AttributeSchemaRecordType = struct {
	Main         string
	Delta        string
	HistoryMain  string
	HistoryDelta string
}{
	"main",
	"delta",
	"history_main",
	"history_delta",
}

// AttributeServiceMemoryUsedType are the possible values that the attribute "service_memory_used_type" can have.
var AttributeServiceMemoryUsedType = struct {
	Logical  string
	Physical string
}{
	"logical",
	"physical",
}

// AttributeServiceStatus are the possible values that the attribute "service_status" can have.
var AttributeServiceStatus = struct {
	Active   string
	Inactive string
}{
	"active",
	"inactive",
}

// AttributeThreadStatus are the possible values that the attribute "thread_status" can have.
var AttributeThreadStatus = struct {
	Active   string
	Inactive string
}{
	"active",
	"inactive",
}

// AttributeTransactionType are the possible values that the attribute "transaction_type" can have.
var AttributeTransactionType = struct {
	Update   string
	Commit   string
	Rollback string
}{
	"update",
	"commit",
	"rollback",
}

// AttributeVolumeOperationType are the possible values that the attribute "volume_operation_type" can have.
var AttributeVolumeOperationType = struct {
	Read  string
	Write string
}{
	"read",
	"write",
}
