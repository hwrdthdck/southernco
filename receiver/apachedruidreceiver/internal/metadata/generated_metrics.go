// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver"
)

type metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.compact.segment_analyzer.fetch_and_process_millis metric with initial data.
func (m *metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis) init() {
	m.data.SetName("apachedruid.compact.segment_analyzer.fetch_and_process_millis")
	m.data.SetDescription("Time taken to fetch and process segments to infer the schema for the compaction task to run.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, compactTaskTypeAttributeValue string, compactDataSourceAttributeValue string, compactGroupIDAttributeValue string, compactTagsAttributeValue string, compactTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", compactTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", compactDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", compactGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", compactTagsAttributeValue)
	dp.Attributes().PutStr("task_id", compactTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis(cfg MetricConfig) metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis {
	m := metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidCompactTaskCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.compact.task.count metric with initial data.
func (m *metricApachedruidCompactTaskCount) init() {
	m.data.SetName("apachedruid.compact.task.count")
	m.data.SetDescription("Number of tasks issued in the auto compaction run.")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidCompactTaskCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidCompactTaskCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidCompactTaskCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidCompactTaskCount(cfg MetricConfig) metricApachedruidCompactTaskCount {
	m := metricApachedruidCompactTaskCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidCompactTaskAvailableSlotCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.compact_task.available_slot.count metric with initial data.
func (m *metricApachedruidCompactTaskAvailableSlotCount) init() {
	m.data.SetName("apachedruid.compact_task.available_slot.count")
	m.data.SetDescription("Number of available task slots that can be used for auto compaction tasks in the auto compaction run. This is the max number of task slots minus any currently running compaction tasks.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidCompactTaskAvailableSlotCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidCompactTaskAvailableSlotCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidCompactTaskAvailableSlotCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidCompactTaskAvailableSlotCount(cfg MetricConfig) metricApachedruidCompactTaskAvailableSlotCount {
	m := metricApachedruidCompactTaskAvailableSlotCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidCompactTaskMaxSlotCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.compact_task.max_slot.count metric with initial data.
func (m *metricApachedruidCompactTaskMaxSlotCount) init() {
	m.data.SetName("apachedruid.compact_task.max_slot.count")
	m.data.SetDescription("Maximum number of task slots available for auto compaction tasks in the auto compaction run.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidCompactTaskMaxSlotCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidCompactTaskMaxSlotCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidCompactTaskMaxSlotCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidCompactTaskMaxSlotCount(cfg MetricConfig) metricApachedruidCompactTaskMaxSlotCount {
	m := metricApachedruidCompactTaskMaxSlotCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidCoordinatorGlobalTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.coordinator.global.time metric with initial data.
func (m *metricApachedruidCoordinatorGlobalTime) init() {
	m.data.SetName("apachedruid.coordinator.global.time")
	m.data.SetDescription("Approximate runtime of a full coordination cycle in milliseconds. The `dutyGroup` dimension indicates what type of coordination this run was. For example, Historical Management or Indexing.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidCoordinatorGlobalTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, coordinatorDutyGroupAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("duty_group", coordinatorDutyGroupAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidCoordinatorGlobalTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidCoordinatorGlobalTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidCoordinatorGlobalTime(cfg MetricConfig) metricApachedruidCoordinatorGlobalTime {
	m := metricApachedruidCoordinatorGlobalTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidCoordinatorTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.coordinator.time metric with initial data.
func (m *metricApachedruidCoordinatorTime) init() {
	m.data.SetName("apachedruid.coordinator.time")
	m.data.SetDescription("Approximate Coordinator duty runtime in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidCoordinatorTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, coordinatorDutyAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("duty", coordinatorDutyAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidCoordinatorTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidCoordinatorTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidCoordinatorTime(cfg MetricConfig) metricApachedruidCoordinatorTime {
	m := metricApachedruidCoordinatorTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestBytesReceived struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.bytes.received metric with initial data.
func (m *metricApachedruidIngestBytesReceived) init() {
	m.data.SetName("apachedruid.ingest.bytes.received")
	m.data.SetDescription("Number of bytes received by the `EventReceiverFirehose`.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestBytesReceived) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestTaskIDAttributeValue string, ingestDataSourceAttributeValue string, ingestServiceNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("service_name", ingestServiceNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestBytesReceived) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestBytesReceived) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestBytesReceived(cfg MetricConfig) metricApachedruidIngestBytesReceived {
	m := metricApachedruidIngestBytesReceived{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.count metric with initial data.
func (m *metricApachedruidIngestCount) init() {
	m.data.SetName("apachedruid.ingest.count")
	m.data.SetDescription("Count of `1` every time an ingestion job runs (includes compaction jobs). Aggregate using dimensions.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string, ingestTaskIngestionModeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
	dp.Attributes().PutStr("task_ingestion_mode", ingestTaskIngestionModeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestCount(cfg MetricConfig) metricApachedruidIngestCount {
	m := metricApachedruidIngestCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestEventsBuffered struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.events.buffered metric with initial data.
func (m *metricApachedruidIngestEventsBuffered) init() {
	m.data.SetName("apachedruid.ingest.events.buffered")
	m.data.SetDescription("Number of events queued in the `EventReceiverFirehose` buffer.")
	m.data.SetUnit("{events}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestEventsBuffered) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestServiceNameAttributeValue string, ingestBufferCapacityAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("service_name", ingestServiceNameAttributeValue)
	dp.Attributes().PutStr("buffer_capacity", ingestBufferCapacityAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestEventsBuffered) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestEventsBuffered) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestEventsBuffered(cfg MetricConfig) metricApachedruidIngestEventsBuffered {
	m := metricApachedruidIngestEventsBuffered{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestEventsDuplicate struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.events.duplicate metric with initial data.
func (m *metricApachedruidIngestEventsDuplicate) init() {
	m.data.SetName("apachedruid.ingest.events.duplicate")
	m.data.SetDescription("Number of events rejected because the events are duplicated.")
	m.data.SetUnit("{events}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestEventsDuplicate) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestEventsDuplicate) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestEventsDuplicate) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestEventsDuplicate(cfg MetricConfig) metricApachedruidIngestEventsDuplicate {
	m := metricApachedruidIngestEventsDuplicate{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestEventsMessageGap struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.events.message_gap metric with initial data.
func (m *metricApachedruidIngestEventsMessageGap) init() {
	m.data.SetName("apachedruid.ingest.events.message_gap")
	m.data.SetDescription("Time gap in milliseconds between the latest ingested event timestamp and the current system timestamp of metrics emission. If the value is increasing but lag is low, Druid may not be receiving new data. This metric is reset as new tasks spawn up.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestEventsMessageGap) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestEventsMessageGap) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestEventsMessageGap) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestEventsMessageGap(cfg MetricConfig) metricApachedruidIngestEventsMessageGap {
	m := metricApachedruidIngestEventsMessageGap{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestEventsProcessed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.events.processed metric with initial data.
func (m *metricApachedruidIngestEventsProcessed) init() {
	m.data.SetName("apachedruid.ingest.events.processed")
	m.data.SetDescription("Number of events processed per emission period.")
	m.data.SetUnit("{events}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestEventsProcessed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestEventsProcessed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestEventsProcessed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestEventsProcessed(cfg MetricConfig) metricApachedruidIngestEventsProcessed {
	m := metricApachedruidIngestEventsProcessed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestEventsProcessedWithError struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.events.processed_with_error metric with initial data.
func (m *metricApachedruidIngestEventsProcessedWithError) init() {
	m.data.SetName("apachedruid.ingest.events.processed_with_error")
	m.data.SetDescription("Number of events processed with some partial errors per emission period. Events processed with partial errors are counted towards both this metric and `ingest/events/processed`.")
	m.data.SetUnit("{events}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestEventsProcessedWithError) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestEventsProcessedWithError) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestEventsProcessedWithError) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestEventsProcessedWithError(cfg MetricConfig) metricApachedruidIngestEventsProcessedWithError {
	m := metricApachedruidIngestEventsProcessedWithError{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestEventsThrownAway struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.events.thrown_away metric with initial data.
func (m *metricApachedruidIngestEventsThrownAway) init() {
	m.data.SetName("apachedruid.ingest.events.thrown_away")
	m.data.SetDescription("Number of events rejected because they are null, or filtered by `transformSpec`, or outside one of `lateMessageRejectionPeriod`, `earlyMessageRejectionPeriod`, or `windowPeriod`.")
	m.data.SetUnit("{events}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestEventsThrownAway) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestEventsThrownAway) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestEventsThrownAway) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestEventsThrownAway(cfg MetricConfig) metricApachedruidIngestEventsThrownAway {
	m := metricApachedruidIngestEventsThrownAway{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestEventsUnparseable struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.events.unparseable metric with initial data.
func (m *metricApachedruidIngestEventsUnparseable) init() {
	m.data.SetName("apachedruid.ingest.events.unparseable")
	m.data.SetDescription("Number of events rejected because the events are unparseable.")
	m.data.SetUnit("{events}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestEventsUnparseable) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestEventsUnparseable) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestEventsUnparseable) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestEventsUnparseable(cfg MetricConfig) metricApachedruidIngestEventsUnparseable {
	m := metricApachedruidIngestEventsUnparseable{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestHandoffCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.handoff.count metric with initial data.
func (m *metricApachedruidIngestHandoffCount) init() {
	m.data.SetName("apachedruid.ingest.handoff.count")
	m.data.SetDescription("Number of handoffs that happened.")
	m.data.SetUnit("{handoffs}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestHandoffCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestHandoffCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestHandoffCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestHandoffCount(cfg MetricConfig) metricApachedruidIngestHandoffCount {
	m := metricApachedruidIngestHandoffCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestHandoffFailed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.handoff.failed metric with initial data.
func (m *metricApachedruidIngestHandoffFailed) init() {
	m.data.SetName("apachedruid.ingest.handoff.failed")
	m.data.SetDescription("Number of handoffs that failed.")
	m.data.SetUnit("{handoffs}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestHandoffFailed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestHandoffFailed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestHandoffFailed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestHandoffFailed(cfg MetricConfig) metricApachedruidIngestHandoffFailed {
	m := metricApachedruidIngestHandoffFailed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestHandoffTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.handoff.time metric with initial data.
func (m *metricApachedruidIngestHandoffTime) init() {
	m.data.SetName("apachedruid.ingest.handoff.time")
	m.data.SetDescription("Total number of milliseconds taken to handoff a set of segments.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestHandoffTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestHandoffTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestHandoffTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestHandoffTime(cfg MetricConfig) metricApachedruidIngestHandoffTime {
	m := metricApachedruidIngestHandoffTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestInputBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.input.bytes metric with initial data.
func (m *metricApachedruidIngestInputBytes) init() {
	m.data.SetName("apachedruid.ingest.input.bytes")
	m.data.SetDescription("Number of bytes read from input sources, after decompression but prior to parsing. This covers all data read, including data that does not end up being fully processed and ingested. For example, this includes data that ends up being rejected for being unparseable or filtered out.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestInputBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestInputBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestInputBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestInputBytes(cfg MetricConfig) metricApachedruidIngestInputBytes {
	m := metricApachedruidIngestInputBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestKafkaAvgLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.kafka.avg_lag metric with initial data.
func (m *metricApachedruidIngestKafkaAvgLag) init() {
	m.data.SetName("apachedruid.ingest.kafka.avg_lag")
	m.data.SetDescription("Average lag between the offsets consumed by the Kafka indexing tasks and latest offsets in Kafka brokers across all partitions. Minimum emission period for this metric is a minute.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestKafkaAvgLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("stream", ingestStreamAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestKafkaAvgLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestKafkaAvgLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestKafkaAvgLag(cfg MetricConfig) metricApachedruidIngestKafkaAvgLag {
	m := metricApachedruidIngestKafkaAvgLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestKafkaLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.kafka.lag metric with initial data.
func (m *metricApachedruidIngestKafkaLag) init() {
	m.data.SetName("apachedruid.ingest.kafka.lag")
	m.data.SetDescription("Total lag between the offsets consumed by the Kafka indexing tasks and latest offsets in Kafka brokers across all partitions. Minimum emission period for this metric is a minute.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestKafkaLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("stream", ingestStreamAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestKafkaLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestKafkaLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestKafkaLag(cfg MetricConfig) metricApachedruidIngestKafkaLag {
	m := metricApachedruidIngestKafkaLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestKafkaMaxLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.kafka.max_lag metric with initial data.
func (m *metricApachedruidIngestKafkaMaxLag) init() {
	m.data.SetName("apachedruid.ingest.kafka.max_lag")
	m.data.SetDescription("Max lag between the offsets consumed by the Kafka indexing tasks and latest offsets in Kafka brokers across all partitions. Minimum emission period for this metric is a minute.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestKafkaMaxLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("stream", ingestStreamAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestKafkaMaxLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestKafkaMaxLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestKafkaMaxLag(cfg MetricConfig) metricApachedruidIngestKafkaMaxLag {
	m := metricApachedruidIngestKafkaMaxLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestKafkaPartitionLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.kafka.partition_lag metric with initial data.
func (m *metricApachedruidIngestKafkaPartitionLag) init() {
	m.data.SetName("apachedruid.ingest.kafka.partition_lag")
	m.data.SetDescription("Partition-wise lag between the offsets consumed by the Kafka indexing tasks and latest offsets in Kafka brokers. Minimum emission period for this metric is a minute.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestKafkaPartitionLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestPartitionAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("partition", ingestPartitionAttributeValue)
	dp.Attributes().PutStr("stream", ingestStreamAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestKafkaPartitionLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestKafkaPartitionLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestKafkaPartitionLag(cfg MetricConfig) metricApachedruidIngestKafkaPartitionLag {
	m := metricApachedruidIngestKafkaPartitionLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestKinesisAvgLagTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.kinesis.avg_lag.time metric with initial data.
func (m *metricApachedruidIngestKinesisAvgLagTime) init() {
	m.data.SetName("apachedruid.ingest.kinesis.avg_lag.time")
	m.data.SetDescription("Average lag time in milliseconds between the current message sequence number consumed by the Kinesis indexing tasks and latest sequence number in Kinesis across all shards. Minimum emission period for this metric is a minute.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestKinesisAvgLagTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("stream", ingestStreamAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestKinesisAvgLagTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestKinesisAvgLagTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestKinesisAvgLagTime(cfg MetricConfig) metricApachedruidIngestKinesisAvgLagTime {
	m := metricApachedruidIngestKinesisAvgLagTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestKinesisLagTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.kinesis.lag.time metric with initial data.
func (m *metricApachedruidIngestKinesisLagTime) init() {
	m.data.SetName("apachedruid.ingest.kinesis.lag.time")
	m.data.SetDescription("Total lag time in milliseconds between the current message sequence number consumed by the Kinesis indexing tasks and latest sequence number in Kinesis across all shards. Minimum emission period for this metric is a minute.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestKinesisLagTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("stream", ingestStreamAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestKinesisLagTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestKinesisLagTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestKinesisLagTime(cfg MetricConfig) metricApachedruidIngestKinesisLagTime {
	m := metricApachedruidIngestKinesisLagTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestKinesisMaxLagTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.kinesis.max_lag.time metric with initial data.
func (m *metricApachedruidIngestKinesisMaxLagTime) init() {
	m.data.SetName("apachedruid.ingest.kinesis.max_lag.time")
	m.data.SetDescription("Max lag time in milliseconds between the current message sequence number consumed by the Kinesis indexing tasks and latest sequence number in Kinesis across all shards. Minimum emission period for this metric is a minute.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestKinesisMaxLagTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("stream", ingestStreamAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestKinesisMaxLagTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestKinesisMaxLagTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestKinesisMaxLagTime(cfg MetricConfig) metricApachedruidIngestKinesisMaxLagTime {
	m := metricApachedruidIngestKinesisMaxLagTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestKinesisPartitionLagTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.kinesis.partition_lag.time metric with initial data.
func (m *metricApachedruidIngestKinesisPartitionLagTime) init() {
	m.data.SetName("apachedruid.ingest.kinesis.partition_lag.time")
	m.data.SetDescription("Partition-wise lag time in milliseconds between the current message sequence number consumed by the Kinesis indexing tasks and latest sequence number in Kinesis. Minimum emission period for this metric is a minute.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestKinesisPartitionLagTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestPartitionAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("partition", ingestPartitionAttributeValue)
	dp.Attributes().PutStr("stream", ingestStreamAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestKinesisPartitionLagTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestKinesisPartitionLagTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestKinesisPartitionLagTime(cfg MetricConfig) metricApachedruidIngestKinesisPartitionLagTime {
	m := metricApachedruidIngestKinesisPartitionLagTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestMergeCPU struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.merge.cpu metric with initial data.
func (m *metricApachedruidIngestMergeCPU) init() {
	m.data.SetName("apachedruid.ingest.merge.cpu")
	m.data.SetDescription("CPU time in Nanoseconds spent on merging intermediate segments.")
	m.data.SetUnit("ns")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestMergeCPU) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestMergeCPU) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestMergeCPU) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestMergeCPU(cfg MetricConfig) metricApachedruidIngestMergeCPU {
	m := metricApachedruidIngestMergeCPU{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestMergeTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.merge.time metric with initial data.
func (m *metricApachedruidIngestMergeTime) init() {
	m.data.SetName("apachedruid.ingest.merge.time")
	m.data.SetDescription("Milliseconds spent merging intermediate segments.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestMergeTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestMergeTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestMergeTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestMergeTime(cfg MetricConfig) metricApachedruidIngestMergeTime {
	m := metricApachedruidIngestMergeTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestNoticesQueueSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.notices.queue_size metric with initial data.
func (m *metricApachedruidIngestNoticesQueueSize) init() {
	m.data.SetName("apachedruid.ingest.notices.queue_size")
	m.data.SetDescription("Number of pending notices to be processed by the coordinator.")
	m.data.SetUnit("{notices}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestNoticesQueueSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestNoticesQueueSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestNoticesQueueSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestNoticesQueueSize(cfg MetricConfig) metricApachedruidIngestNoticesQueueSize {
	m := metricApachedruidIngestNoticesQueueSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestNoticesTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.notices.time metric with initial data.
func (m *metricApachedruidIngestNoticesTime) init() {
	m.data.SetName("apachedruid.ingest.notices.time")
	m.data.SetDescription("Milliseconds taken to process a notice by the supervisor.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestNoticesTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestNoticesTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestNoticesTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestNoticesTime(cfg MetricConfig) metricApachedruidIngestNoticesTime {
	m := metricApachedruidIngestNoticesTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestPauseTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.pause.time metric with initial data.
func (m *metricApachedruidIngestPauseTime) init() {
	m.data.SetName("apachedruid.ingest.pause.time")
	m.data.SetDescription("Milliseconds spent by a task in a paused state without ingesting.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestPauseTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string, ingestDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestPauseTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestPauseTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestPauseTime(cfg MetricConfig) metricApachedruidIngestPauseTime {
	m := metricApachedruidIngestPauseTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestPersistsBackPressure struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.persists.back_pressure metric with initial data.
func (m *metricApachedruidIngestPersistsBackPressure) init() {
	m.data.SetName("apachedruid.ingest.persists.back_pressure")
	m.data.SetDescription("Milliseconds spent creating persist tasks and blocking waiting for them to finish.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestPersistsBackPressure) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestPersistsBackPressure) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestPersistsBackPressure) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestPersistsBackPressure(cfg MetricConfig) metricApachedruidIngestPersistsBackPressure {
	m := metricApachedruidIngestPersistsBackPressure{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestPersistsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.persists.count metric with initial data.
func (m *metricApachedruidIngestPersistsCount) init() {
	m.data.SetName("apachedruid.ingest.persists.count")
	m.data.SetDescription("Number of times persist occurred.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestPersistsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestPersistsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestPersistsCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestPersistsCount(cfg MetricConfig) metricApachedruidIngestPersistsCount {
	m := metricApachedruidIngestPersistsCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestPersistsCPU struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.persists.cpu metric with initial data.
func (m *metricApachedruidIngestPersistsCPU) init() {
	m.data.SetName("apachedruid.ingest.persists.cpu")
	m.data.SetDescription("CPU time in nanoseconds spent on doing intermediate persist.")
	m.data.SetUnit("ns")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestPersistsCPU) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestPersistsCPU) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestPersistsCPU) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestPersistsCPU(cfg MetricConfig) metricApachedruidIngestPersistsCPU {
	m := metricApachedruidIngestPersistsCPU{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestPersistsFailed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.persists.failed metric with initial data.
func (m *metricApachedruidIngestPersistsFailed) init() {
	m.data.SetName("apachedruid.ingest.persists.failed")
	m.data.SetDescription("Number of persists that failed.")
	m.data.SetUnit("{persists}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestPersistsFailed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestPersistsFailed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestPersistsFailed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestPersistsFailed(cfg MetricConfig) metricApachedruidIngestPersistsFailed {
	m := metricApachedruidIngestPersistsFailed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestPersistsTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.persists.time metric with initial data.
func (m *metricApachedruidIngestPersistsTime) init() {
	m.data.SetName("apachedruid.ingest.persists.time")
	m.data.SetDescription("Milliseconds spent doing intermediate persist.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestPersistsTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestPersistsTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestPersistsTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestPersistsTime(cfg MetricConfig) metricApachedruidIngestPersistsTime {
	m := metricApachedruidIngestPersistsTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestRowsOutput struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.rows.output metric with initial data.
func (m *metricApachedruidIngestRowsOutput) init() {
	m.data.SetName("apachedruid.ingest.rows.output")
	m.data.SetDescription("Number of Druid rows persisted.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestRowsOutput) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestTaskIDAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestRowsOutput) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestRowsOutput) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestRowsOutput(cfg MetricConfig) metricApachedruidIngestRowsOutput {
	m := metricApachedruidIngestRowsOutput{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestSegmentsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.segments.count metric with initial data.
func (m *metricApachedruidIngestSegmentsCount) init() {
	m.data.SetName("apachedruid.ingest.segments.count")
	m.data.SetDescription("Count of final segments created by job (includes tombstones).")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestSegmentsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string, ingestTaskIngestionModeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
	dp.Attributes().PutStr("task_ingestion_mode", ingestTaskIngestionModeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestSegmentsCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestSegmentsCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestSegmentsCount(cfg MetricConfig) metricApachedruidIngestSegmentsCount {
	m := metricApachedruidIngestSegmentsCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestShuffleBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.shuffle.bytes metric with initial data.
func (m *metricApachedruidIngestShuffleBytes) init() {
	m.data.SetName("apachedruid.ingest.shuffle.bytes")
	m.data.SetDescription("Number of bytes shuffled per emission period.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestShuffleBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestSupervisorTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("supervisor_task_id", ingestSupervisorTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestShuffleBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestShuffleBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestShuffleBytes(cfg MetricConfig) metricApachedruidIngestShuffleBytes {
	m := metricApachedruidIngestShuffleBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestShuffleRequests struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.shuffle.requests metric with initial data.
func (m *metricApachedruidIngestShuffleRequests) init() {
	m.data.SetName("apachedruid.ingest.shuffle.requests")
	m.data.SetDescription("Number of shuffle requests per emission period.")
	m.data.SetUnit("{requests}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestShuffleRequests) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestSupervisorTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("supervisor_task_id", ingestSupervisorTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestShuffleRequests) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestShuffleRequests) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestShuffleRequests(cfg MetricConfig) metricApachedruidIngestShuffleRequests {
	m := metricApachedruidIngestShuffleRequests{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestSinkCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.sink.count metric with initial data.
func (m *metricApachedruidIngestSinkCount) init() {
	m.data.SetName("apachedruid.ingest.sink.count")
	m.data.SetDescription("Number of sinks not handed off.")
	m.data.SetUnit("{sinks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestSinkCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestSinkCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestSinkCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestSinkCount(cfg MetricConfig) metricApachedruidIngestSinkCount {
	m := metricApachedruidIngestSinkCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIngestTombstonesCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.ingest.tombstones.count metric with initial data.
func (m *metricApachedruidIngestTombstonesCount) init() {
	m.data.SetName("apachedruid.ingest.tombstones.count")
	m.data.SetDescription("Count of tombstones created by job.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIngestTombstonesCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string, ingestTaskIngestionModeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", ingestTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", ingestDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", ingestGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", ingestTagsAttributeValue)
	dp.Attributes().PutStr("task_id", ingestTaskIDAttributeValue)
	dp.Attributes().PutStr("task_ingestion_mode", ingestTaskIngestionModeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIngestTombstonesCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIngestTombstonesCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIngestTombstonesCount(cfg MetricConfig) metricApachedruidIngestTombstonesCount {
	m := metricApachedruidIngestTombstonesCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIntervalCompactedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.interval.compacted.count metric with initial data.
func (m *metricApachedruidIntervalCompactedCount) init() {
	m.data.SetName("apachedruid.interval.compacted.count")
	m.data.SetDescription("Total number of intervals of this datasource that are already compacted with the spec set in the auto compaction config.")
	m.data.SetUnit("{intervals}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIntervalCompactedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, intervalDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", intervalDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIntervalCompactedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIntervalCompactedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIntervalCompactedCount(cfg MetricConfig) metricApachedruidIntervalCompactedCount {
	m := metricApachedruidIntervalCompactedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIntervalSkipCompactCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.interval.skip_compact.count metric with initial data.
func (m *metricApachedruidIntervalSkipCompactCount) init() {
	m.data.SetName("apachedruid.interval.skip_compact.count")
	m.data.SetDescription("Total number of intervals of this datasource that are skipped (not eligible for auto compaction) by the auto compaction.")
	m.data.SetUnit("{intervals}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIntervalSkipCompactCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, intervalDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", intervalDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIntervalSkipCompactCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIntervalSkipCompactCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIntervalSkipCompactCount(cfg MetricConfig) metricApachedruidIntervalSkipCompactCount {
	m := metricApachedruidIntervalSkipCompactCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidIntervalWaitCompactCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.interval.wait_compact.count metric with initial data.
func (m *metricApachedruidIntervalWaitCompactCount) init() {
	m.data.SetName("apachedruid.interval.wait_compact.count")
	m.data.SetDescription("Total number of intervals of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction).")
	m.data.SetUnit("{intervals}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidIntervalWaitCompactCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, intervalDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", intervalDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidIntervalWaitCompactCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidIntervalWaitCompactCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidIntervalWaitCompactCount(cfg MetricConfig) metricApachedruidIntervalWaitCompactCount {
	m := metricApachedruidIntervalWaitCompactCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJettyNumOpenConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jetty.num_open_connections metric with initial data.
func (m *metricApachedruidJettyNumOpenConnections) init() {
	m.data.SetName("apachedruid.jetty.num_open_connections")
	m.data.SetDescription("Number of open jetty connections.")
	m.data.SetUnit("{connections}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidJettyNumOpenConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJettyNumOpenConnections) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJettyNumOpenConnections) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJettyNumOpenConnections(cfg MetricConfig) metricApachedruidJettyNumOpenConnections {
	m := metricApachedruidJettyNumOpenConnections{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJettyThreadPoolBusy struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jetty.thread_pool.busy metric with initial data.
func (m *metricApachedruidJettyThreadPoolBusy) init() {
	m.data.SetName("apachedruid.jetty.thread_pool.busy")
	m.data.SetDescription("Number of busy threads that has work to do from the worker queue.")
	m.data.SetUnit("{threads}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidJettyThreadPoolBusy) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJettyThreadPoolBusy) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJettyThreadPoolBusy) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJettyThreadPoolBusy(cfg MetricConfig) metricApachedruidJettyThreadPoolBusy {
	m := metricApachedruidJettyThreadPoolBusy{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJettyThreadPoolIdle struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jetty.thread_pool.idle metric with initial data.
func (m *metricApachedruidJettyThreadPoolIdle) init() {
	m.data.SetName("apachedruid.jetty.thread_pool.idle")
	m.data.SetDescription("Number of idle threads.")
	m.data.SetUnit("{threads}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidJettyThreadPoolIdle) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJettyThreadPoolIdle) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJettyThreadPoolIdle) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJettyThreadPoolIdle(cfg MetricConfig) metricApachedruidJettyThreadPoolIdle {
	m := metricApachedruidJettyThreadPoolIdle{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJettyThreadPoolIsLowOnThreads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jetty.thread_pool.is_low_on_threads metric with initial data.
func (m *metricApachedruidJettyThreadPoolIsLowOnThreads) init() {
	m.data.SetName("apachedruid.jetty.thread_pool.is_low_on_threads")
	m.data.SetDescription("A rough indicator of whether number of total workable threads allocated is enough to handle the works in the work queue.")
	m.data.SetUnit("{threads}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidJettyThreadPoolIsLowOnThreads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJettyThreadPoolIsLowOnThreads) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJettyThreadPoolIsLowOnThreads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJettyThreadPoolIsLowOnThreads(cfg MetricConfig) metricApachedruidJettyThreadPoolIsLowOnThreads {
	m := metricApachedruidJettyThreadPoolIsLowOnThreads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJettyThreadPoolMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jetty.thread_pool.max metric with initial data.
func (m *metricApachedruidJettyThreadPoolMax) init() {
	m.data.SetName("apachedruid.jetty.thread_pool.max")
	m.data.SetDescription("Number of maximum threads allocatable.")
	m.data.SetUnit("{threads}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidJettyThreadPoolMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJettyThreadPoolMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJettyThreadPoolMax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJettyThreadPoolMax(cfg MetricConfig) metricApachedruidJettyThreadPoolMax {
	m := metricApachedruidJettyThreadPoolMax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJettyThreadPoolMin struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jetty.thread_pool.min metric with initial data.
func (m *metricApachedruidJettyThreadPoolMin) init() {
	m.data.SetName("apachedruid.jetty.thread_pool.min")
	m.data.SetDescription("Number of minimum threads allocatable.")
	m.data.SetUnit("{threads}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidJettyThreadPoolMin) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJettyThreadPoolMin) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJettyThreadPoolMin) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJettyThreadPoolMin(cfg MetricConfig) metricApachedruidJettyThreadPoolMin {
	m := metricApachedruidJettyThreadPoolMin{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJettyThreadPoolQueueSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jetty.thread_pool.queue_size metric with initial data.
func (m *metricApachedruidJettyThreadPoolQueueSize) init() {
	m.data.SetName("apachedruid.jetty.thread_pool.queue_size")
	m.data.SetDescription("Size of the worker queue.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidJettyThreadPoolQueueSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJettyThreadPoolQueueSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJettyThreadPoolQueueSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJettyThreadPoolQueueSize(cfg MetricConfig) metricApachedruidJettyThreadPoolQueueSize {
	m := metricApachedruidJettyThreadPoolQueueSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJettyThreadPoolTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jetty.thread_pool.total metric with initial data.
func (m *metricApachedruidJettyThreadPoolTotal) init() {
	m.data.SetName("apachedruid.jetty.thread_pool.total")
	m.data.SetDescription("Number of total workable threads allocated.")
	m.data.SetUnit("{threads}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidJettyThreadPoolTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJettyThreadPoolTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJettyThreadPoolTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJettyThreadPoolTotal(cfg MetricConfig) metricApachedruidJettyThreadPoolTotal {
	m := metricApachedruidJettyThreadPoolTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmBufferpoolCapacity struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.bufferpool.capacity metric with initial data.
func (m *metricApachedruidJvmBufferpoolCapacity) init() {
	m.data.SetName("apachedruid.jvm.bufferpool.capacity")
	m.data.SetDescription("Bufferpool capacity.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmBufferpoolCapacity) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmBufferpoolNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("bufferpool_name", jvmBufferpoolNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmBufferpoolCapacity) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmBufferpoolCapacity) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmBufferpoolCapacity(cfg MetricConfig) metricApachedruidJvmBufferpoolCapacity {
	m := metricApachedruidJvmBufferpoolCapacity{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmBufferpoolCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.bufferpool.count metric with initial data.
func (m *metricApachedruidJvmBufferpoolCount) init() {
	m.data.SetName("apachedruid.jvm.bufferpool.count")
	m.data.SetDescription("Bufferpool count.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmBufferpoolCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmBufferpoolNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("bufferpool_name", jvmBufferpoolNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmBufferpoolCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmBufferpoolCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmBufferpoolCount(cfg MetricConfig) metricApachedruidJvmBufferpoolCount {
	m := metricApachedruidJvmBufferpoolCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmBufferpoolUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.bufferpool.used metric with initial data.
func (m *metricApachedruidJvmBufferpoolUsed) init() {
	m.data.SetName("apachedruid.jvm.bufferpool.used")
	m.data.SetDescription("Bufferpool used.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmBufferpoolUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmBufferpoolNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("bufferpool_name", jvmBufferpoolNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmBufferpoolUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmBufferpoolUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmBufferpoolUsed(cfg MetricConfig) metricApachedruidJvmBufferpoolUsed {
	m := metricApachedruidJvmBufferpoolUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmGcCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.gc.count metric with initial data.
func (m *metricApachedruidJvmGcCount) init() {
	m.data.SetName("apachedruid.jvm.gc.count")
	m.data.SetDescription("Garbage collection count.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmGcCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmGcGenAttributeValue string, jvmGcNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("gc_gen", jvmGcGenAttributeValue)
	dp.Attributes().PutStr("gc_name", jvmGcNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmGcCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmGcCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmGcCount(cfg MetricConfig) metricApachedruidJvmGcCount {
	m := metricApachedruidJvmGcCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmGcCPU struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.gc.cpu metric with initial data.
func (m *metricApachedruidJvmGcCPU) init() {
	m.data.SetName("apachedruid.jvm.gc.cpu")
	m.data.SetDescription("Count of CPU time in Nanoseconds spent on garbage collection. Note, `jvm/gc/cpu` represents the total time over multiple GC cycles; divide by `jvm/gc/count` to get the mean GC time per cycle.")
	m.data.SetUnit("ns")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmGcCPU) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmGcGenAttributeValue string, jvmGcNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("gc_gen", jvmGcGenAttributeValue)
	dp.Attributes().PutStr("gc_name", jvmGcNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmGcCPU) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmGcCPU) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmGcCPU(cfg MetricConfig) metricApachedruidJvmGcCPU {
	m := metricApachedruidJvmGcCPU{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmMemCommitted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.mem.committed metric with initial data.
func (m *metricApachedruidJvmMemCommitted) init() {
	m.data.SetName("apachedruid.jvm.mem.committed")
	m.data.SetDescription("Committed memory.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmMemCommitted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmMemKindAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("mem_kind", jvmMemKindAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmMemCommitted) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmMemCommitted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmMemCommitted(cfg MetricConfig) metricApachedruidJvmMemCommitted {
	m := metricApachedruidJvmMemCommitted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmMemInit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.mem.init metric with initial data.
func (m *metricApachedruidJvmMemInit) init() {
	m.data.SetName("apachedruid.jvm.mem.init")
	m.data.SetDescription("Initial memory.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmMemInit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmMemKindAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("mem_kind", jvmMemKindAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmMemInit) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmMemInit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmMemInit(cfg MetricConfig) metricApachedruidJvmMemInit {
	m := metricApachedruidJvmMemInit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmMemMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.mem.max metric with initial data.
func (m *metricApachedruidJvmMemMax) init() {
	m.data.SetName("apachedruid.jvm.mem.max")
	m.data.SetDescription("Max memory.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmMemMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmMemKindAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("mem_kind", jvmMemKindAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmMemMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmMemMax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmMemMax(cfg MetricConfig) metricApachedruidJvmMemMax {
	m := metricApachedruidJvmMemMax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmMemUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.mem.used metric with initial data.
func (m *metricApachedruidJvmMemUsed) init() {
	m.data.SetName("apachedruid.jvm.mem.used")
	m.data.SetDescription("Used memory.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmMemUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmMemKindAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("mem_kind", jvmMemKindAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmMemUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmMemUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmMemUsed(cfg MetricConfig) metricApachedruidJvmMemUsed {
	m := metricApachedruidJvmMemUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmPoolCommitted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.pool.committed metric with initial data.
func (m *metricApachedruidJvmPoolCommitted) init() {
	m.data.SetName("apachedruid.jvm.pool.committed")
	m.data.SetDescription("Committed pool.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmPoolCommitted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmPoolNameAttributeValue string, jvmPoolKindAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("pool_name", jvmPoolNameAttributeValue)
	dp.Attributes().PutStr("pool_kind", jvmPoolKindAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmPoolCommitted) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmPoolCommitted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmPoolCommitted(cfg MetricConfig) metricApachedruidJvmPoolCommitted {
	m := metricApachedruidJvmPoolCommitted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmPoolInit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.pool.init metric with initial data.
func (m *metricApachedruidJvmPoolInit) init() {
	m.data.SetName("apachedruid.jvm.pool.init")
	m.data.SetDescription("Initial pool.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmPoolInit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmPoolNameAttributeValue string, jvmPoolKindAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("pool_name", jvmPoolNameAttributeValue)
	dp.Attributes().PutStr("pool_kind", jvmPoolKindAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmPoolInit) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmPoolInit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmPoolInit(cfg MetricConfig) metricApachedruidJvmPoolInit {
	m := metricApachedruidJvmPoolInit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmPoolMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.pool.max metric with initial data.
func (m *metricApachedruidJvmPoolMax) init() {
	m.data.SetName("apachedruid.jvm.pool.max")
	m.data.SetDescription("Max pool.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmPoolMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmPoolNameAttributeValue string, jvmPoolKindAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("pool_name", jvmPoolNameAttributeValue)
	dp.Attributes().PutStr("pool_kind", jvmPoolKindAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmPoolMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmPoolMax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmPoolMax(cfg MetricConfig) metricApachedruidJvmPoolMax {
	m := metricApachedruidJvmPoolMax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidJvmPoolUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.jvm.pool.used metric with initial data.
func (m *metricApachedruidJvmPoolUsed) init() {
	m.data.SetName("apachedruid.jvm.pool.used")
	m.data.SetDescription("Pool used.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidJvmPoolUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jvmPoolNameAttributeValue string, jvmPoolKindAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("pool_name", jvmPoolNameAttributeValue)
	dp.Attributes().PutStr("pool_kind", jvmPoolKindAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidJvmPoolUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidJvmPoolUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidJvmPoolUsed(cfg MetricConfig) metricApachedruidJvmPoolUsed {
	m := metricApachedruidJvmPoolUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidKillPendingSegmentsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.kill.pending_segments.count metric with initial data.
func (m *metricApachedruidKillPendingSegmentsCount) init() {
	m.data.SetName("apachedruid.kill.pending_segments.count")
	m.data.SetDescription("Number of stale pending segments deleted from the metadata store.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidKillPendingSegmentsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, killDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", killDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidKillPendingSegmentsCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidKillPendingSegmentsCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidKillPendingSegmentsCount(cfg MetricConfig) metricApachedruidKillPendingSegmentsCount {
	m := metricApachedruidKillPendingSegmentsCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidKillTaskCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.kill.task.count metric with initial data.
func (m *metricApachedruidKillTaskCount) init() {
	m.data.SetName("apachedruid.kill.task.count")
	m.data.SetDescription("Number of tasks issued in the auto kill run.")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidKillTaskCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidKillTaskCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidKillTaskCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidKillTaskCount(cfg MetricConfig) metricApachedruidKillTaskCount {
	m := metricApachedruidKillTaskCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidKillTaskAvailableSlotCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.kill_task.available_slot.count metric with initial data.
func (m *metricApachedruidKillTaskAvailableSlotCount) init() {
	m.data.SetName("apachedruid.kill_task.available_slot.count")
	m.data.SetDescription("Number of available task slots that can be used for auto kill tasks in the auto kill run. This is the max number of task slots minus any currently running auto kill tasks.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidKillTaskAvailableSlotCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidKillTaskAvailableSlotCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidKillTaskAvailableSlotCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidKillTaskAvailableSlotCount(cfg MetricConfig) metricApachedruidKillTaskAvailableSlotCount {
	m := metricApachedruidKillTaskAvailableSlotCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidKillTaskMaxSlotCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.kill_task.max_slot.count metric with initial data.
func (m *metricApachedruidKillTaskMaxSlotCount) init() {
	m.data.SetName("apachedruid.kill_task.max_slot.count")
	m.data.SetDescription("Maximum number of task slots available for auto kill tasks in the auto kill run.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidKillTaskMaxSlotCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidKillTaskMaxSlotCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidKillTaskMaxSlotCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidKillTaskMaxSlotCount(cfg MetricConfig) metricApachedruidKillTaskMaxSlotCount {
	m := metricApachedruidKillTaskMaxSlotCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidMergeBufferPendingRequests struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.merge_buffer.pending_requests metric with initial data.
func (m *metricApachedruidMergeBufferPendingRequests) init() {
	m.data.SetName("apachedruid.merge_buffer.pending_requests")
	m.data.SetDescription("Number of requests waiting to acquire a batch of buffers from the merge buffer pool.")
	m.data.SetUnit("{requests}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidMergeBufferPendingRequests) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidMergeBufferPendingRequests) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidMergeBufferPendingRequests) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidMergeBufferPendingRequests(cfg MetricConfig) metricApachedruidMergeBufferPendingRequests {
	m := metricApachedruidMergeBufferPendingRequests{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidMetadataKillAuditCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.metadata.kill.audit.count metric with initial data.
func (m *metricApachedruidMetadataKillAuditCount) init() {
	m.data.SetName("apachedruid.metadata.kill.audit.count")
	m.data.SetDescription("Total number of audit logs that were automatically deleted from metadata store per each Coordinator kill audit duty run. This metric can help adjust `druid.coordinator.kill.audit.durationToRetain` configuration based on whether more or less audit logs need to be deleted per cycle. This metric is emitted only when `druid.coordinator.kill.audit.on` is set to true.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidMetadataKillAuditCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidMetadataKillAuditCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidMetadataKillAuditCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidMetadataKillAuditCount(cfg MetricConfig) metricApachedruidMetadataKillAuditCount {
	m := metricApachedruidMetadataKillAuditCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidMetadataKillCompactionCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.metadata.kill.compaction.count metric with initial data.
func (m *metricApachedruidMetadataKillCompactionCount) init() {
	m.data.SetName("apachedruid.metadata.kill.compaction.count")
	m.data.SetDescription("Total number of compaction configurations that were automatically deleted from metadata store per each Coordinator kill compaction configuration duty run. This metric is only emitted when `druid.coordinator.kill.compaction.on` is set to true.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidMetadataKillCompactionCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidMetadataKillCompactionCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidMetadataKillCompactionCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidMetadataKillCompactionCount(cfg MetricConfig) metricApachedruidMetadataKillCompactionCount {
	m := metricApachedruidMetadataKillCompactionCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidMetadataKillDatasourceCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.metadata.kill.datasource.count metric with initial data.
func (m *metricApachedruidMetadataKillDatasourceCount) init() {
	m.data.SetName("apachedruid.metadata.kill.datasource.count")
	m.data.SetDescription("Total number of datasource metadata that were automatically deleted from metadata store per each Coordinator kill datasource duty run. Note that datasource metadata only exists for datasource created from supervisor. This metric can help adjust `druid.coordinator.kill.datasource.durationToRetain` configuration based on whether more or less datasource metadata need to be deleted per cycle. This metric is only emitted when `druid.coordinator.kill.datasource.on` is set to true.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidMetadataKillDatasourceCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidMetadataKillDatasourceCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidMetadataKillDatasourceCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidMetadataKillDatasourceCount(cfg MetricConfig) metricApachedruidMetadataKillDatasourceCount {
	m := metricApachedruidMetadataKillDatasourceCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidMetadataKillRuleCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.metadata.kill.rule.count metric with initial data.
func (m *metricApachedruidMetadataKillRuleCount) init() {
	m.data.SetName("apachedruid.metadata.kill.rule.count")
	m.data.SetDescription("Total number of rules that were automatically deleted from metadata store per each Coordinator kill rule duty run. This metric can help adjust `druid.coordinator.kill.rule.durationToRetain` configuration based on whether more or less rules need to be deleted per cycle. This metric is only emitted when `druid.coordinator.kill.rule.on` is set to true.")
	m.data.SetUnit("{rules}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidMetadataKillRuleCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidMetadataKillRuleCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidMetadataKillRuleCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidMetadataKillRuleCount(cfg MetricConfig) metricApachedruidMetadataKillRuleCount {
	m := metricApachedruidMetadataKillRuleCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidMetadataKillSupervisorCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.metadata.kill.supervisor.count metric with initial data.
func (m *metricApachedruidMetadataKillSupervisorCount) init() {
	m.data.SetName("apachedruid.metadata.kill.supervisor.count")
	m.data.SetDescription("Total number of terminated supervisors that were automatically deleted from metadata store per each Coordinator kill supervisor duty run. This metric can help adjust `druid.coordinator.kill.supervisor.durationToRetain` configuration based on whether more or less terminated supervisors need to be deleted per cycle. This metric is only emitted when `druid.coordinator.kill.supervisor.on` is set to true.")
	m.data.SetUnit("{supervisors}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidMetadataKillSupervisorCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidMetadataKillSupervisorCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidMetadataKillSupervisorCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidMetadataKillSupervisorCount(cfg MetricConfig) metricApachedruidMetadataKillSupervisorCount {
	m := metricApachedruidMetadataKillSupervisorCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidMetadatacacheInitTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.metadatacache.init.time metric with initial data.
func (m *metricApachedruidMetadatacacheInitTime) init() {
	m.data.SetName("apachedruid.metadatacache.init.time")
	m.data.SetDescription("Time taken to initialize the broker segment metadata cache. Useful to detect if brokers are taking too long to start.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidMetadatacacheInitTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidMetadatacacheInitTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidMetadatacacheInitTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidMetadatacacheInitTime(cfg MetricConfig) metricApachedruidMetadatacacheInitTime {
	m := metricApachedruidMetadatacacheInitTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidMetadatacacheRefreshCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.metadatacache.refresh.count metric with initial data.
func (m *metricApachedruidMetadatacacheRefreshCount) init() {
	m.data.SetName("apachedruid.metadatacache.refresh.count")
	m.data.SetDescription("Number of segments to refresh in broker segment metadata cache.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidMetadatacacheRefreshCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidMetadatacacheRefreshCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidMetadatacacheRefreshCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidMetadatacacheRefreshCount(cfg MetricConfig) metricApachedruidMetadatacacheRefreshCount {
	m := metricApachedruidMetadatacacheRefreshCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidMetadatacacheRefreshTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.metadatacache.refresh.time metric with initial data.
func (m *metricApachedruidMetadatacacheRefreshTime) init() {
	m.data.SetName("apachedruid.metadatacache.refresh.time")
	m.data.SetDescription("Time taken to refresh segments in broker segment metadata cache.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidMetadatacacheRefreshTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidMetadatacacheRefreshTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidMetadatacacheRefreshTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidMetadatacacheRefreshTime(cfg MetricConfig) metricApachedruidMetadatacacheRefreshTime {
	m := metricApachedruidMetadatacacheRefreshTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryByteLimitExceededCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.byte_limit.exceeded.count metric with initial data.
func (m *metricApachedruidQueryByteLimitExceededCount) init() {
	m.data.SetName("apachedruid.query.byte_limit.exceeded.count")
	m.data.SetDescription("Number of queries whose inlined subquery results exceeded the given byte limit.")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryByteLimitExceededCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryByteLimitExceededCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryByteLimitExceededCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryByteLimitExceededCount(cfg MetricConfig) metricApachedruidQueryByteLimitExceededCount {
	m := metricApachedruidQueryByteLimitExceededCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.bytes metric with initial data.
func (m *metricApachedruidQueryBytes) init() {
	m.data.SetName("apachedruid.query.bytes")
	m.data.SetDescription("The total number of bytes returned to the requesting client in the query response from the broker. Other services report the total bytes for their portion of the query.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidQueryBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, queryDataSourceAttributeValue string, queryNumMetricsAttributeValue string, queryDimensionAttributeValue string, queryHasFiltersAttributeValue string, queryThresholdAttributeValue int64, queryNumComplexMetricsAttributeValue int64, queryTypeAttributeValue string, queryRemoteAddressAttributeValue string, queryIDAttributeValue string, queryContextAttributeValue string, queryNumDimensionsAttributeValue string, queryIntervalAttributeValue string, queryDurationAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", queryDataSourceAttributeValue)
	dp.Attributes().PutStr("num_metrics", queryNumMetricsAttributeValue)
	dp.Attributes().PutStr("dimension", queryDimensionAttributeValue)
	dp.Attributes().PutStr("has_filters", queryHasFiltersAttributeValue)
	dp.Attributes().PutInt("threshold", queryThresholdAttributeValue)
	dp.Attributes().PutInt("num_complex_metrics", queryNumComplexMetricsAttributeValue)
	dp.Attributes().PutStr("type", queryTypeAttributeValue)
	dp.Attributes().PutStr("remote_address", queryRemoteAddressAttributeValue)
	dp.Attributes().PutStr("id", queryIDAttributeValue)
	dp.Attributes().PutStr("context", queryContextAttributeValue)
	dp.Attributes().PutStr("num_dimensions", queryNumDimensionsAttributeValue)
	dp.Attributes().PutStr("interval", queryIntervalAttributeValue)
	dp.Attributes().PutStr("duration", queryDurationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryBytes(cfg MetricConfig) metricApachedruidQueryBytes {
	m := metricApachedruidQueryBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaAverageBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.average_bytes metric with initial data.
func (m *metricApachedruidQueryCacheDeltaAverageBytes) init() {
	m.data.SetName("apachedruid.query.cache.delta.average_bytes")
	m.data.SetDescription("Average cache entry byte size.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaAverageBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaAverageBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaAverageBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaAverageBytes(cfg MetricConfig) metricApachedruidQueryCacheDeltaAverageBytes {
	m := metricApachedruidQueryCacheDeltaAverageBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaErrors struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.errors metric with initial data.
func (m *metricApachedruidQueryCacheDeltaErrors) init() {
	m.data.SetName("apachedruid.query.cache.delta.errors")
	m.data.SetDescription("Number of cache errors.")
	m.data.SetUnit("{errors}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaErrors) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaErrors) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaErrors) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaErrors(cfg MetricConfig) metricApachedruidQueryCacheDeltaErrors {
	m := metricApachedruidQueryCacheDeltaErrors{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaEvictions struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.evictions metric with initial data.
func (m *metricApachedruidQueryCacheDeltaEvictions) init() {
	m.data.SetName("apachedruid.query.cache.delta.evictions")
	m.data.SetDescription("Number of cache evictions.")
	m.data.SetUnit("{evictions}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaEvictions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaEvictions) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaEvictions) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaEvictions(cfg MetricConfig) metricApachedruidQueryCacheDeltaEvictions {
	m := metricApachedruidQueryCacheDeltaEvictions{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaHitRate struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.hit_rate metric with initial data.
func (m *metricApachedruidQueryCacheDeltaHitRate) init() {
	m.data.SetName("apachedruid.query.cache.delta.hit_rate")
	m.data.SetDescription("Cache hit rate.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaHitRate) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaHitRate) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaHitRate) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaHitRate(cfg MetricConfig) metricApachedruidQueryCacheDeltaHitRate {
	m := metricApachedruidQueryCacheDeltaHitRate{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaHits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.hits metric with initial data.
func (m *metricApachedruidQueryCacheDeltaHits) init() {
	m.data.SetName("apachedruid.query.cache.delta.hits")
	m.data.SetDescription("Number of cache hits.")
	m.data.SetUnit("{hits}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaHits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaHits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaHits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaHits(cfg MetricConfig) metricApachedruidQueryCacheDeltaHits {
	m := metricApachedruidQueryCacheDeltaHits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaMisses struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.misses metric with initial data.
func (m *metricApachedruidQueryCacheDeltaMisses) init() {
	m.data.SetName("apachedruid.query.cache.delta.misses")
	m.data.SetDescription("Number of cache misses.")
	m.data.SetUnit("{misses}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaMisses) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaMisses) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaMisses) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaMisses(cfg MetricConfig) metricApachedruidQueryCacheDeltaMisses {
	m := metricApachedruidQueryCacheDeltaMisses{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaNumEntries struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.num_entries metric with initial data.
func (m *metricApachedruidQueryCacheDeltaNumEntries) init() {
	m.data.SetName("apachedruid.query.cache.delta.num_entries")
	m.data.SetDescription("Number of cache entries.")
	m.data.SetUnit("{entries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaNumEntries) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaNumEntries) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaNumEntries) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaNumEntries(cfg MetricConfig) metricApachedruidQueryCacheDeltaNumEntries {
	m := metricApachedruidQueryCacheDeltaNumEntries{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaPutError struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.put.error metric with initial data.
func (m *metricApachedruidQueryCacheDeltaPutError) init() {
	m.data.SetName("apachedruid.query.cache.delta.put.error")
	m.data.SetDescription("Number of new cache entries that could not be cached due to errors.")
	m.data.SetUnit("{errors}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaPutError) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaPutError) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaPutError) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaPutError(cfg MetricConfig) metricApachedruidQueryCacheDeltaPutError {
	m := metricApachedruidQueryCacheDeltaPutError{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaPutOk struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.put.ok metric with initial data.
func (m *metricApachedruidQueryCacheDeltaPutOk) init() {
	m.data.SetName("apachedruid.query.cache.delta.put.ok")
	m.data.SetDescription("Number of new cache entries successfully cached.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaPutOk) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaPutOk) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaPutOk) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaPutOk(cfg MetricConfig) metricApachedruidQueryCacheDeltaPutOk {
	m := metricApachedruidQueryCacheDeltaPutOk{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaPutOversized struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.put.oversized metric with initial data.
func (m *metricApachedruidQueryCacheDeltaPutOversized) init() {
	m.data.SetName("apachedruid.query.cache.delta.put.oversized")
	m.data.SetDescription("Number of potential new cache entries that were skipped due to being too large (based on `druid.{broker,historical,realtime}.cache.maxEntrySize` properties).")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaPutOversized) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaPutOversized) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaPutOversized) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaPutOversized(cfg MetricConfig) metricApachedruidQueryCacheDeltaPutOversized {
	m := metricApachedruidQueryCacheDeltaPutOversized{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaSizeBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.size_bytes metric with initial data.
func (m *metricApachedruidQueryCacheDeltaSizeBytes) init() {
	m.data.SetName("apachedruid.query.cache.delta.size_bytes")
	m.data.SetDescription("Size in bytes of cache entries.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaSizeBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaSizeBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaSizeBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaSizeBytes(cfg MetricConfig) metricApachedruidQueryCacheDeltaSizeBytes {
	m := metricApachedruidQueryCacheDeltaSizeBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheDeltaTimeouts struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.delta.timeouts metric with initial data.
func (m *metricApachedruidQueryCacheDeltaTimeouts) init() {
	m.data.SetName("apachedruid.query.cache.delta.timeouts")
	m.data.SetDescription("Number of cache timeouts.")
	m.data.SetUnit("{timeouts}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheDeltaTimeouts) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheDeltaTimeouts) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheDeltaTimeouts) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheDeltaTimeouts(cfg MetricConfig) metricApachedruidQueryCacheDeltaTimeouts {
	m := metricApachedruidQueryCacheDeltaTimeouts{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheMemcachedDelta struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.memcached.delta metric with initial data.
func (m *metricApachedruidQueryCacheMemcachedDelta) init() {
	m.data.SetName("apachedruid.query.cache.memcached.delta")
	m.data.SetDescription("Cache metrics unique to memcached (only if `druid.cache.type=memcached`) as their delta from the prior event emission.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCacheMemcachedDelta) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheMemcachedDelta) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheMemcachedDelta) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheMemcachedDelta(cfg MetricConfig) metricApachedruidQueryCacheMemcachedDelta {
	m := metricApachedruidQueryCacheMemcachedDelta{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheMemcachedTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.memcached.total metric with initial data.
func (m *metricApachedruidQueryCacheMemcachedTotal) init() {
	m.data.SetName("apachedruid.query.cache.memcached.total")
	m.data.SetDescription("Cache metrics unique to memcached (only if `druid.cache.type=memcached`) as their actual values.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheMemcachedTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheMemcachedTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheMemcachedTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheMemcachedTotal(cfg MetricConfig) metricApachedruidQueryCacheMemcachedTotal {
	m := metricApachedruidQueryCacheMemcachedTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalAverageBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.average_bytes metric with initial data.
func (m *metricApachedruidQueryCacheTotalAverageBytes) init() {
	m.data.SetName("apachedruid.query.cache.total.average_bytes")
	m.data.SetDescription("Average cache entry byte size.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalAverageBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalAverageBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalAverageBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalAverageBytes(cfg MetricConfig) metricApachedruidQueryCacheTotalAverageBytes {
	m := metricApachedruidQueryCacheTotalAverageBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalErrors struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.errors metric with initial data.
func (m *metricApachedruidQueryCacheTotalErrors) init() {
	m.data.SetName("apachedruid.query.cache.total.errors")
	m.data.SetDescription("Number of cache errors.")
	m.data.SetUnit("{errors}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalErrors) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalErrors) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalErrors) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalErrors(cfg MetricConfig) metricApachedruidQueryCacheTotalErrors {
	m := metricApachedruidQueryCacheTotalErrors{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalEvictions struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.evictions metric with initial data.
func (m *metricApachedruidQueryCacheTotalEvictions) init() {
	m.data.SetName("apachedruid.query.cache.total.evictions")
	m.data.SetDescription("Number of cache evictions.")
	m.data.SetUnit("{evictions}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalEvictions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalEvictions) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalEvictions) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalEvictions(cfg MetricConfig) metricApachedruidQueryCacheTotalEvictions {
	m := metricApachedruidQueryCacheTotalEvictions{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalHitRate struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.hit_rate metric with initial data.
func (m *metricApachedruidQueryCacheTotalHitRate) init() {
	m.data.SetName("apachedruid.query.cache.total.hit_rate")
	m.data.SetDescription("Cache hit rate.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalHitRate) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalHitRate) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalHitRate) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalHitRate(cfg MetricConfig) metricApachedruidQueryCacheTotalHitRate {
	m := metricApachedruidQueryCacheTotalHitRate{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalHits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.hits metric with initial data.
func (m *metricApachedruidQueryCacheTotalHits) init() {
	m.data.SetName("apachedruid.query.cache.total.hits")
	m.data.SetDescription("Number of cache hits.")
	m.data.SetUnit("{hits}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalHits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalHits) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalHits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalHits(cfg MetricConfig) metricApachedruidQueryCacheTotalHits {
	m := metricApachedruidQueryCacheTotalHits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalMisses struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.misses metric with initial data.
func (m *metricApachedruidQueryCacheTotalMisses) init() {
	m.data.SetName("apachedruid.query.cache.total.misses")
	m.data.SetDescription("Number of cache misses.")
	m.data.SetUnit("{misses}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalMisses) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalMisses) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalMisses) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalMisses(cfg MetricConfig) metricApachedruidQueryCacheTotalMisses {
	m := metricApachedruidQueryCacheTotalMisses{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalNumEntries struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.num_entries metric with initial data.
func (m *metricApachedruidQueryCacheTotalNumEntries) init() {
	m.data.SetName("apachedruid.query.cache.total.num_entries")
	m.data.SetDescription("Number of cache entries.")
	m.data.SetUnit("{entries}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalNumEntries) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalNumEntries) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalNumEntries) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalNumEntries(cfg MetricConfig) metricApachedruidQueryCacheTotalNumEntries {
	m := metricApachedruidQueryCacheTotalNumEntries{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalPutError struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.put.error metric with initial data.
func (m *metricApachedruidQueryCacheTotalPutError) init() {
	m.data.SetName("apachedruid.query.cache.total.put.error")
	m.data.SetDescription("Number of new cache entries that could not be cached due to errors.")
	m.data.SetUnit("{errors}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalPutError) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalPutError) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalPutError) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalPutError(cfg MetricConfig) metricApachedruidQueryCacheTotalPutError {
	m := metricApachedruidQueryCacheTotalPutError{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalPutOk struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.put.ok metric with initial data.
func (m *metricApachedruidQueryCacheTotalPutOk) init() {
	m.data.SetName("apachedruid.query.cache.total.put.ok")
	m.data.SetDescription("Number of new cache entries successfully cached.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalPutOk) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalPutOk) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalPutOk) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalPutOk(cfg MetricConfig) metricApachedruidQueryCacheTotalPutOk {
	m := metricApachedruidQueryCacheTotalPutOk{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalPutOversized struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.put.oversized metric with initial data.
func (m *metricApachedruidQueryCacheTotalPutOversized) init() {
	m.data.SetName("apachedruid.query.cache.total.put.oversized")
	m.data.SetDescription("Number of potential new cache entries that were skipped due to being too large (based on `druid.{broker,historical,realtime}.cache.maxEntrySize` properties).")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalPutOversized) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalPutOversized) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalPutOversized) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalPutOversized(cfg MetricConfig) metricApachedruidQueryCacheTotalPutOversized {
	m := metricApachedruidQueryCacheTotalPutOversized{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalSizeBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.size_bytes metric with initial data.
func (m *metricApachedruidQueryCacheTotalSizeBytes) init() {
	m.data.SetName("apachedruid.query.cache.total.size_bytes")
	m.data.SetDescription("Size in bytes of cache entries.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalSizeBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalSizeBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalSizeBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalSizeBytes(cfg MetricConfig) metricApachedruidQueryCacheTotalSizeBytes {
	m := metricApachedruidQueryCacheTotalSizeBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCacheTotalTimeouts struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cache.total.timeouts metric with initial data.
func (m *metricApachedruidQueryCacheTotalTimeouts) init() {
	m.data.SetName("apachedruid.query.cache.total.timeouts")
	m.data.SetDescription("Number of cache timeouts.")
	m.data.SetUnit("{timeouts}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidQueryCacheTotalTimeouts) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCacheTotalTimeouts) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCacheTotalTimeouts) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCacheTotalTimeouts(cfg MetricConfig) metricApachedruidQueryCacheTotalTimeouts {
	m := metricApachedruidQueryCacheTotalTimeouts{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.count metric with initial data.
func (m *metricApachedruidQueryCount) init() {
	m.data.SetName("apachedruid.query.count")
	m.data.SetDescription("Number of total queries.")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCount(cfg MetricConfig) metricApachedruidQueryCount {
	m := metricApachedruidQueryCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCPUTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.cpu.time metric with initial data.
func (m *metricApachedruidQueryCPUTime) init() {
	m.data.SetName("apachedruid.query.cpu.time")
	m.data.SetDescription("Microseconds of CPU time taken to complete a query.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidQueryCPUTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, queryDataSourceAttributeValue string, queryNumMetricsAttributeValue string, queryDimensionAttributeValue string, queryHasFiltersAttributeValue string, queryThresholdAttributeValue int64, queryNumComplexMetricsAttributeValue int64, queryTypeAttributeValue string, queryRemoteAddressAttributeValue string, queryIDAttributeValue string, queryContextAttributeValue string, queryNumDimensionsAttributeValue string, queryIntervalAttributeValue string, queryDurationAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", queryDataSourceAttributeValue)
	dp.Attributes().PutStr("num_metrics", queryNumMetricsAttributeValue)
	dp.Attributes().PutStr("dimension", queryDimensionAttributeValue)
	dp.Attributes().PutStr("has_filters", queryHasFiltersAttributeValue)
	dp.Attributes().PutInt("threshold", queryThresholdAttributeValue)
	dp.Attributes().PutInt("num_complex_metrics", queryNumComplexMetricsAttributeValue)
	dp.Attributes().PutStr("type", queryTypeAttributeValue)
	dp.Attributes().PutStr("remote_address", queryRemoteAddressAttributeValue)
	dp.Attributes().PutStr("id", queryIDAttributeValue)
	dp.Attributes().PutStr("context", queryContextAttributeValue)
	dp.Attributes().PutStr("num_dimensions", queryNumDimensionsAttributeValue)
	dp.Attributes().PutStr("interval", queryIntervalAttributeValue)
	dp.Attributes().PutStr("duration", queryDurationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCPUTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCPUTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCPUTime(cfg MetricConfig) metricApachedruidQueryCPUTime {
	m := metricApachedruidQueryCPUTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryFailedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.failed.count metric with initial data.
func (m *metricApachedruidQueryFailedCount) init() {
	m.data.SetName("apachedruid.query.failed.count")
	m.data.SetDescription("Number of failed queries.")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryFailedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryFailedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryFailedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryFailedCount(cfg MetricConfig) metricApachedruidQueryFailedCount {
	m := metricApachedruidQueryFailedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryInterruptedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.interrupted.count metric with initial data.
func (m *metricApachedruidQueryInterruptedCount) init() {
	m.data.SetName("apachedruid.query.interrupted.count")
	m.data.SetDescription("Number of queries interrupted due to cancellation.")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryInterruptedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryInterruptedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryInterruptedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryInterruptedCount(cfg MetricConfig) metricApachedruidQueryInterruptedCount {
	m := metricApachedruidQueryInterruptedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryNodeBackpressure struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.node.backpressure metric with initial data.
func (m *metricApachedruidQueryNodeBackpressure) init() {
	m.data.SetName("apachedruid.query.node.backpressure")
	m.data.SetDescription("Milliseconds that the channel to this process has spent suspended due to backpressure.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidQueryNodeBackpressure) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, queryStatusAttributeValue string, queryServerAttributeValue string, queryIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("status", queryStatusAttributeValue)
	dp.Attributes().PutStr("server", queryServerAttributeValue)
	dp.Attributes().PutStr("id", queryIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryNodeBackpressure) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryNodeBackpressure) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryNodeBackpressure(cfg MetricConfig) metricApachedruidQueryNodeBackpressure {
	m := metricApachedruidQueryNodeBackpressure{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryNodeBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.node.bytes metric with initial data.
func (m *metricApachedruidQueryNodeBytes) init() {
	m.data.SetName("apachedruid.query.node.bytes")
	m.data.SetDescription("Number of bytes returned from querying individual historical/realtime processes.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidQueryNodeBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, queryStatusAttributeValue string, queryServerAttributeValue string, queryIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("status", queryStatusAttributeValue)
	dp.Attributes().PutStr("server", queryServerAttributeValue)
	dp.Attributes().PutStr("id", queryIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryNodeBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryNodeBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryNodeBytes(cfg MetricConfig) metricApachedruidQueryNodeBytes {
	m := metricApachedruidQueryNodeBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryNodeTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.node.time metric with initial data.
func (m *metricApachedruidQueryNodeTime) init() {
	m.data.SetName("apachedruid.query.node.time")
	m.data.SetDescription("Milliseconds taken to query individual historical/realtime processes.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidQueryNodeTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, queryStatusAttributeValue string, queryServerAttributeValue string, queryIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("status", queryStatusAttributeValue)
	dp.Attributes().PutStr("server", queryServerAttributeValue)
	dp.Attributes().PutStr("id", queryIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryNodeTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryNodeTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryNodeTime(cfg MetricConfig) metricApachedruidQueryNodeTime {
	m := metricApachedruidQueryNodeTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryNodeTtfb struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.node.ttfb metric with initial data.
func (m *metricApachedruidQueryNodeTtfb) init() {
	m.data.SetName("apachedruid.query.node.ttfb")
	m.data.SetDescription("Time to first byte. Milliseconds elapsed until Broker starts receiving the response from individual historical/realtime processes.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidQueryNodeTtfb) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, queryStatusAttributeValue string, queryServerAttributeValue string, queryIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("status", queryStatusAttributeValue)
	dp.Attributes().PutStr("server", queryServerAttributeValue)
	dp.Attributes().PutStr("id", queryIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryNodeTtfb) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryNodeTtfb) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryNodeTtfb(cfg MetricConfig) metricApachedruidQueryNodeTtfb {
	m := metricApachedruidQueryNodeTtfb{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryPriority struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.priority metric with initial data.
func (m *metricApachedruidQueryPriority) init() {
	m.data.SetName("apachedruid.query.priority")
	m.data.SetDescription("Assigned lane and priority, only if Laning strategy is enabled. Refer to [Laning strategies](https,//druid.apache.org/docs/latest/configuration#laning-strategies).")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidQueryPriority) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, queryTypeAttributeValue string, queryDataSourceAttributeValue string, queryLaneAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("type", queryTypeAttributeValue)
	dp.Attributes().PutStr("data_source", queryDataSourceAttributeValue)
	dp.Attributes().PutStr("lane", queryLaneAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryPriority) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryPriority) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryPriority(cfg MetricConfig) metricApachedruidQueryPriority {
	m := metricApachedruidQueryPriority{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryRowLimitExceededCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.row_limit.exceeded.count metric with initial data.
func (m *metricApachedruidQueryRowLimitExceededCount) init() {
	m.data.SetName("apachedruid.query.row_limit.exceeded.count")
	m.data.SetDescription("Number of queries whose inlined subquery results exceeded the given row limit.")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryRowLimitExceededCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryRowLimitExceededCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryRowLimitExceededCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryRowLimitExceededCount(cfg MetricConfig) metricApachedruidQueryRowLimitExceededCount {
	m := metricApachedruidQueryRowLimitExceededCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQuerySegmentTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.segment.time metric with initial data.
func (m *metricApachedruidQuerySegmentTime) init() {
	m.data.SetName("apachedruid.query.segment.time")
	m.data.SetDescription("Milliseconds taken to query individual segment. Includes time to page in the segment from disk.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidQuerySegmentTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, queryStatusAttributeValue string, querySegmentAttributeValue string, queryIDAttributeValue string, queryVectorizedAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("status", queryStatusAttributeValue)
	dp.Attributes().PutStr("segment", querySegmentAttributeValue)
	dp.Attributes().PutStr("id", queryIDAttributeValue)
	dp.Attributes().PutStr("vectorized", queryVectorizedAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQuerySegmentTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQuerySegmentTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQuerySegmentTime(cfg MetricConfig) metricApachedruidQuerySegmentTime {
	m := metricApachedruidQuerySegmentTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQuerySegmentAndCacheTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.segment_and_cache.time metric with initial data.
func (m *metricApachedruidQuerySegmentAndCacheTime) init() {
	m.data.SetName("apachedruid.query.segment_and_cache.time")
	m.data.SetDescription("Milliseconds taken to query individual segment or hit the cache (if it is enabled on the Historical process).")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidQuerySegmentAndCacheTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, querySegmentAttributeValue string, queryIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("segment", querySegmentAttributeValue)
	dp.Attributes().PutStr("id", queryIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQuerySegmentAndCacheTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQuerySegmentAndCacheTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQuerySegmentAndCacheTime(cfg MetricConfig) metricApachedruidQuerySegmentAndCacheTime {
	m := metricApachedruidQuerySegmentAndCacheTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQuerySegmentsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.segments.count metric with initial data.
func (m *metricApachedruidQuerySegmentsCount) init() {
	m.data.SetName("apachedruid.query.segments.count")
	m.data.SetDescription("This metric is not enabled by default. See the `QueryMetrics` Interface for reference regarding enabling this metric. Number of segments that will be touched by the query. In the broker, it makes a plan to distribute the query to realtime tasks and historicals based on a snapshot of segment distribution state. If there are some segments moved after this snapshot is created, certain historicals and realtime tasks can report those segments as missing to the broker. The broker will resend the query to the new servers that serve those segments after move. In this case, those segments can be counted more than once in this metric.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQuerySegmentsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQuerySegmentsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQuerySegmentsCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQuerySegmentsCount(cfg MetricConfig) metricApachedruidQuerySegmentsCount {
	m := metricApachedruidQuerySegmentsCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQuerySuccessCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.success.count metric with initial data.
func (m *metricApachedruidQuerySuccessCount) init() {
	m.data.SetName("apachedruid.query.success.count")
	m.data.SetDescription("Number of queries successfully processed.")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQuerySuccessCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQuerySuccessCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQuerySuccessCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQuerySuccessCount(cfg MetricConfig) metricApachedruidQuerySuccessCount {
	m := metricApachedruidQuerySuccessCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.time metric with initial data.
func (m *metricApachedruidQueryTime) init() {
	m.data.SetName("apachedruid.query.time")
	m.data.SetDescription("Milliseconds taken to complete a query.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidQueryTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, queryDataSourceAttributeValue string, queryNumMetricsAttributeValue string, queryDimensionAttributeValue string, queryHasFiltersAttributeValue string, queryThresholdAttributeValue int64, queryNumComplexMetricsAttributeValue int64, queryTypeAttributeValue string, queryRemoteAddressAttributeValue string, queryIDAttributeValue string, queryContextAttributeValue string, queryNumDimensionsAttributeValue string, queryIntervalAttributeValue string, queryDurationAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", queryDataSourceAttributeValue)
	dp.Attributes().PutStr("num_metrics", queryNumMetricsAttributeValue)
	dp.Attributes().PutStr("dimension", queryDimensionAttributeValue)
	dp.Attributes().PutStr("has_filters", queryHasFiltersAttributeValue)
	dp.Attributes().PutInt("threshold", queryThresholdAttributeValue)
	dp.Attributes().PutInt("num_complex_metrics", queryNumComplexMetricsAttributeValue)
	dp.Attributes().PutStr("type", queryTypeAttributeValue)
	dp.Attributes().PutStr("remote_address", queryRemoteAddressAttributeValue)
	dp.Attributes().PutStr("id", queryIDAttributeValue)
	dp.Attributes().PutStr("context", queryContextAttributeValue)
	dp.Attributes().PutStr("num_dimensions", queryNumDimensionsAttributeValue)
	dp.Attributes().PutStr("interval", queryIntervalAttributeValue)
	dp.Attributes().PutStr("duration", queryDurationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryTime(cfg MetricConfig) metricApachedruidQueryTime {
	m := metricApachedruidQueryTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryTimeoutCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.timeout.count metric with initial data.
func (m *metricApachedruidQueryTimeoutCount) init() {
	m.data.SetName("apachedruid.query.timeout.count")
	m.data.SetDescription("Number of timed out queries.")
	m.data.SetUnit("{queries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidQueryTimeoutCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryTimeoutCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryTimeoutCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryTimeoutCount(cfg MetricConfig) metricApachedruidQueryTimeoutCount {
	m := metricApachedruidQueryTimeoutCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryWaitTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query.wait.time metric with initial data.
func (m *metricApachedruidQueryWaitTime) init() {
	m.data.SetName("apachedruid.query.wait.time")
	m.data.SetDescription("Milliseconds spent waiting for a segment to be scanned.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidQueryWaitTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, querySegmentAttributeValue string, queryIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("segment", querySegmentAttributeValue)
	dp.Attributes().PutStr("id", queryIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryWaitTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryWaitTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryWaitTime(cfg MetricConfig) metricApachedruidQueryWaitTime {
	m := metricApachedruidQueryWaitTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentAddedBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.added.bytes metric with initial data.
func (m *metricApachedruidSegmentAddedBytes) init() {
	m.data.SetName("apachedruid.segment.added.bytes")
	m.data.SetDescription("Size in bytes of new segments created.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentAddedBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentTaskTypeAttributeValue string, segmentDataSourceAttributeValue string, segmentGroupIDAttributeValue string, segmentTagsAttributeValue string, segmentTaskIDAttributeValue string, segmentIntervalAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", segmentTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", segmentGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", segmentTagsAttributeValue)
	dp.Attributes().PutStr("task_id", segmentTaskIDAttributeValue)
	dp.Attributes().PutStr("interval", segmentIntervalAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentAddedBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentAddedBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentAddedBytes(cfg MetricConfig) metricApachedruidSegmentAddedBytes {
	m := metricApachedruidSegmentAddedBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentAssignSkippedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.assign_skipped.count metric with initial data.
func (m *metricApachedruidSegmentAssignSkippedCount) init() {
	m.data.SetName("apachedruid.segment.assign_skipped.count")
	m.data.SetDescription("Number of segments that could not be assigned to any server for loading. This can occur due to replication throttling, no available disk space, or a full load queue.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentAssignSkippedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDescriptionAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("description", segmentDescriptionAttributeValue)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentAssignSkippedCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentAssignSkippedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentAssignSkippedCount(cfg MetricConfig) metricApachedruidSegmentAssignSkippedCount {
	m := metricApachedruidSegmentAssignSkippedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentAssignedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.assigned.count metric with initial data.
func (m *metricApachedruidSegmentAssignedCount) init() {
	m.data.SetName("apachedruid.segment.assigned.count")
	m.data.SetDescription("Number of segments assigned to be loaded in the cluster.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentAssignedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentAssignedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentAssignedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentAssignedCount(cfg MetricConfig) metricApachedruidSegmentAssignedCount {
	m := metricApachedruidSegmentAssignedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentCompactedBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.compacted.bytes metric with initial data.
func (m *metricApachedruidSegmentCompactedBytes) init() {
	m.data.SetName("apachedruid.segment.compacted.bytes")
	m.data.SetDescription("Total bytes of this datasource that are already compacted with the spec set in the auto compaction config.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentCompactedBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentCompactedBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentCompactedBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentCompactedBytes(cfg MetricConfig) metricApachedruidSegmentCompactedBytes {
	m := metricApachedruidSegmentCompactedBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentCompactedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.compacted.count metric with initial data.
func (m *metricApachedruidSegmentCompactedCount) init() {
	m.data.SetName("apachedruid.segment.compacted.count")
	m.data.SetDescription("Total number of segments of this datasource that are already compacted with the spec set in the auto compaction config.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentCompactedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentCompactedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentCompactedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentCompactedCount(cfg MetricConfig) metricApachedruidSegmentCompactedCount {
	m := metricApachedruidSegmentCompactedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.count metric with initial data.
func (m *metricApachedruidSegmentCount) init() {
	m.data.SetName("apachedruid.segment.count")
	m.data.SetDescription("Number of used segments belonging to a data source. Emitted only for data sources to which at least one used segment belongs.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentPriorityAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("priority", segmentPriorityAttributeValue)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentCount(cfg MetricConfig) metricApachedruidSegmentCount {
	m := metricApachedruidSegmentCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentDeletedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.deleted.count metric with initial data.
func (m *metricApachedruidSegmentDeletedCount) init() {
	m.data.SetName("apachedruid.segment.deleted.count")
	m.data.SetDescription("Number of segments marked as unused due to drop rules.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentDeletedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentDeletedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentDeletedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentDeletedCount(cfg MetricConfig) metricApachedruidSegmentDeletedCount {
	m := metricApachedruidSegmentDeletedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentDropQueueCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.drop_queue.count metric with initial data.
func (m *metricApachedruidSegmentDropQueueCount) init() {
	m.data.SetName("apachedruid.segment.drop_queue.count")
	m.data.SetDescription("Number of segments to drop.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentDropQueueCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentServerAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("server", segmentServerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentDropQueueCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentDropQueueCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentDropQueueCount(cfg MetricConfig) metricApachedruidSegmentDropQueueCount {
	m := metricApachedruidSegmentDropQueueCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentDropSkippedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.drop_skipped.count metric with initial data.
func (m *metricApachedruidSegmentDropSkippedCount) init() {
	m.data.SetName("apachedruid.segment.drop_skipped.count")
	m.data.SetDescription("Number of segments that could not be dropped from any server.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentDropSkippedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDescriptionAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("description", segmentDescriptionAttributeValue)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentDropSkippedCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentDropSkippedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentDropSkippedCount(cfg MetricConfig) metricApachedruidSegmentDropSkippedCount {
	m := metricApachedruidSegmentDropSkippedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentDroppedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.dropped.count metric with initial data.
func (m *metricApachedruidSegmentDroppedCount) init() {
	m.data.SetName("apachedruid.segment.dropped.count")
	m.data.SetDescription("Number of segments chosen to be dropped from the cluster due to being over-replicated.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentDroppedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentDroppedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentDroppedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentDroppedCount(cfg MetricConfig) metricApachedruidSegmentDroppedCount {
	m := metricApachedruidSegmentDroppedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentLoadQueueAssigned struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.load_queue.assigned metric with initial data.
func (m *metricApachedruidSegmentLoadQueueAssigned) init() {
	m.data.SetName("apachedruid.segment.load_queue.assigned")
	m.data.SetDescription("Number of segments assigned for load or drop to the load queue of a server.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentLoadQueueAssigned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentServerAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("server", segmentServerAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentLoadQueueAssigned) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentLoadQueueAssigned) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentLoadQueueAssigned(cfg MetricConfig) metricApachedruidSegmentLoadQueueAssigned {
	m := metricApachedruidSegmentLoadQueueAssigned{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentLoadQueueCancelled struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.load_queue.cancelled metric with initial data.
func (m *metricApachedruidSegmentLoadQueueCancelled) init() {
	m.data.SetName("apachedruid.segment.load_queue.cancelled")
	m.data.SetDescription("Number of segment assignments that were canceled before completion.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentLoadQueueCancelled) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentServerAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("server", segmentServerAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentLoadQueueCancelled) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentLoadQueueCancelled) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentLoadQueueCancelled(cfg MetricConfig) metricApachedruidSegmentLoadQueueCancelled {
	m := metricApachedruidSegmentLoadQueueCancelled{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentLoadQueueCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.load_queue.count metric with initial data.
func (m *metricApachedruidSegmentLoadQueueCount) init() {
	m.data.SetName("apachedruid.segment.load_queue.count")
	m.data.SetDescription("Number of segments to load.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentLoadQueueCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentServerAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("server", segmentServerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentLoadQueueCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentLoadQueueCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentLoadQueueCount(cfg MetricConfig) metricApachedruidSegmentLoadQueueCount {
	m := metricApachedruidSegmentLoadQueueCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentLoadQueueFailed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.load_queue.failed metric with initial data.
func (m *metricApachedruidSegmentLoadQueueFailed) init() {
	m.data.SetName("apachedruid.segment.load_queue.failed")
	m.data.SetDescription("Number of segment assignments that failed to complete.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentLoadQueueFailed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentServerAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("server", segmentServerAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentLoadQueueFailed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentLoadQueueFailed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentLoadQueueFailed(cfg MetricConfig) metricApachedruidSegmentLoadQueueFailed {
	m := metricApachedruidSegmentLoadQueueFailed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentLoadQueueSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.load_queue.size metric with initial data.
func (m *metricApachedruidSegmentLoadQueueSize) init() {
	m.data.SetName("apachedruid.segment.load_queue.size")
	m.data.SetDescription("Size in bytes of segments to load.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentLoadQueueSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentServerAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("server", segmentServerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentLoadQueueSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentLoadQueueSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentLoadQueueSize(cfg MetricConfig) metricApachedruidSegmentLoadQueueSize {
	m := metricApachedruidSegmentLoadQueueSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentLoadQueueSuccess struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.load_queue.success metric with initial data.
func (m *metricApachedruidSegmentLoadQueueSuccess) init() {
	m.data.SetName("apachedruid.segment.load_queue.success")
	m.data.SetDescription("Number of segment assignments that completed successfully.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentLoadQueueSuccess) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentServerAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("server", segmentServerAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentLoadQueueSuccess) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentLoadQueueSuccess) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentLoadQueueSuccess(cfg MetricConfig) metricApachedruidSegmentLoadQueueSuccess {
	m := metricApachedruidSegmentLoadQueueSuccess{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.max metric with initial data.
func (m *metricApachedruidSegmentMax) init() {
	m.data.SetName("apachedruid.segment.max")
	m.data.SetDescription("Maximum byte limit available for segments.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSegmentMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentMax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentMax(cfg MetricConfig) metricApachedruidSegmentMax {
	m := metricApachedruidSegmentMax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentMoveSkippedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.move_skipped.count metric with initial data.
func (m *metricApachedruidSegmentMoveSkippedCount) init() {
	m.data.SetName("apachedruid.segment.move_skipped.count")
	m.data.SetDescription("Number of segments that were chosen for balancing but could not be moved. This can occur when segments are already optimally placed.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentMoveSkippedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDescriptionAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("description", segmentDescriptionAttributeValue)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentMoveSkippedCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentMoveSkippedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentMoveSkippedCount(cfg MetricConfig) metricApachedruidSegmentMoveSkippedCount {
	m := metricApachedruidSegmentMoveSkippedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentMovedBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.moved.bytes metric with initial data.
func (m *metricApachedruidSegmentMovedBytes) init() {
	m.data.SetName("apachedruid.segment.moved.bytes")
	m.data.SetDescription("Size in bytes of segments moved/archived via the Move Task.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentMovedBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentTaskTypeAttributeValue string, segmentDataSourceAttributeValue string, segmentGroupIDAttributeValue string, segmentTagsAttributeValue string, segmentTaskIDAttributeValue string, segmentIntervalAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", segmentTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", segmentGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", segmentTagsAttributeValue)
	dp.Attributes().PutStr("task_id", segmentTaskIDAttributeValue)
	dp.Attributes().PutStr("interval", segmentIntervalAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentMovedBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentMovedBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentMovedBytes(cfg MetricConfig) metricApachedruidSegmentMovedBytes {
	m := metricApachedruidSegmentMovedBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentMovedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.moved.count metric with initial data.
func (m *metricApachedruidSegmentMovedCount) init() {
	m.data.SetName("apachedruid.segment.moved.count")
	m.data.SetDescription("Number of segments moved in the cluster.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentMovedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentMovedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentMovedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentMovedCount(cfg MetricConfig) metricApachedruidSegmentMovedCount {
	m := metricApachedruidSegmentMovedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentNukedBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.nuked.bytes metric with initial data.
func (m *metricApachedruidSegmentNukedBytes) init() {
	m.data.SetName("apachedruid.segment.nuked.bytes")
	m.data.SetDescription("Size in bytes of segments deleted via the Kill Task.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentNukedBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentTaskTypeAttributeValue string, segmentDataSourceAttributeValue string, segmentGroupIDAttributeValue string, segmentTagsAttributeValue string, segmentTaskIDAttributeValue string, segmentIntervalAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", segmentTaskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", segmentGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", segmentTagsAttributeValue)
	dp.Attributes().PutStr("task_id", segmentTaskIDAttributeValue)
	dp.Attributes().PutStr("interval", segmentIntervalAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentNukedBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentNukedBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentNukedBytes(cfg MetricConfig) metricApachedruidSegmentNukedBytes {
	m := metricApachedruidSegmentNukedBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentOverShadowedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.over_shadowed.count metric with initial data.
func (m *metricApachedruidSegmentOverShadowedCount) init() {
	m.data.SetName("apachedruid.segment.over_shadowed.count")
	m.data.SetDescription("Number of segments marked as unused due to being overshadowed.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSegmentOverShadowedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentOverShadowedCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentOverShadowedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentOverShadowedCount(cfg MetricConfig) metricApachedruidSegmentOverShadowedCount {
	m := metricApachedruidSegmentOverShadowedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentPendingDelete struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.pending_delete metric with initial data.
func (m *metricApachedruidSegmentPendingDelete) init() {
	m.data.SetName("apachedruid.segment.pending_delete")
	m.data.SetDescription("On-disk size in bytes of segments that are waiting to be cleared out.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSegmentPendingDelete) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentPendingDelete) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentPendingDelete) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentPendingDelete(cfg MetricConfig) metricApachedruidSegmentPendingDelete {
	m := metricApachedruidSegmentPendingDelete{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentRowCountAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.row_count.avg metric with initial data.
func (m *metricApachedruidSegmentRowCountAvg) init() {
	m.data.SetName("apachedruid.segment.row_count.avg")
	m.data.SetDescription("The average number of rows per segment on a historical. `SegmentStatsMonitor` must be enabled.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentRowCountAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentPriorityAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("priority", segmentPriorityAttributeValue)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentRowCountAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentRowCountAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentRowCountAvg(cfg MetricConfig) metricApachedruidSegmentRowCountAvg {
	m := metricApachedruidSegmentRowCountAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentRowCountRangeCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.row_count.range.count metric with initial data.
func (m *metricApachedruidSegmentRowCountRangeCount) init() {
	m.data.SetName("apachedruid.segment.row_count.range.count")
	m.data.SetDescription("The number of segments in a bucket. `SegmentStatsMonitor` must be enabled.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentRowCountRangeCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentPriorityAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string, segmentRangeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("priority", segmentPriorityAttributeValue)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
	dp.Attributes().PutStr("range", segmentRangeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentRowCountRangeCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentRowCountRangeCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentRowCountRangeCount(cfg MetricConfig) metricApachedruidSegmentRowCountRangeCount {
	m := metricApachedruidSegmentRowCountRangeCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentScanActive struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.scan.active metric with initial data.
func (m *metricApachedruidSegmentScanActive) init() {
	m.data.SetName("apachedruid.segment.scan.active")
	m.data.SetDescription("Number of segments currently scanned. This metric also indicates how many threads from `druid.processing.numThreads` are currently being used.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSegmentScanActive) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentScanActive) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentScanActive) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentScanActive(cfg MetricConfig) metricApachedruidSegmentScanActive {
	m := metricApachedruidSegmentScanActive{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentScanPending struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.scan.pending metric with initial data.
func (m *metricApachedruidSegmentScanPending) init() {
	m.data.SetName("apachedruid.segment.scan.pending")
	m.data.SetDescription("Number of segments in queue waiting to be scanned.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSegmentScanPending) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentScanPending) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentScanPending) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentScanPending(cfg MetricConfig) metricApachedruidSegmentScanPending {
	m := metricApachedruidSegmentScanPending{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.size metric with initial data.
func (m *metricApachedruidSegmentSize) init() {
	m.data.SetName("apachedruid.segment.size")
	m.data.SetDescription("Total size of used segments in a data source. Emitted only for data sources to which at least one used segment belongs.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentSize(cfg MetricConfig) metricApachedruidSegmentSize {
	m := metricApachedruidSegmentSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentSkipCompactBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.skip_compact.bytes metric with initial data.
func (m *metricApachedruidSegmentSkipCompactBytes) init() {
	m.data.SetName("apachedruid.segment.skip_compact.bytes")
	m.data.SetDescription("Total bytes of this datasource that are skipped (not eligible for auto compaction) by the auto compaction.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentSkipCompactBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentSkipCompactBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentSkipCompactBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentSkipCompactBytes(cfg MetricConfig) metricApachedruidSegmentSkipCompactBytes {
	m := metricApachedruidSegmentSkipCompactBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentSkipCompactCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.skip_compact.count metric with initial data.
func (m *metricApachedruidSegmentSkipCompactCount) init() {
	m.data.SetName("apachedruid.segment.skip_compact.count")
	m.data.SetDescription("Total number of segments of this datasource that are skipped (not eligible for auto compaction) by the auto compaction.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentSkipCompactCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentSkipCompactCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentSkipCompactCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentSkipCompactCount(cfg MetricConfig) metricApachedruidSegmentSkipCompactCount {
	m := metricApachedruidSegmentSkipCompactCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentUnavailableCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.unavailable.count metric with initial data.
func (m *metricApachedruidSegmentUnavailableCount) init() {
	m.data.SetName("apachedruid.segment.unavailable.count")
	m.data.SetDescription("Number of unique segments left to load until all used segments are available for queries.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentUnavailableCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentUnavailableCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentUnavailableCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentUnavailableCount(cfg MetricConfig) metricApachedruidSegmentUnavailableCount {
	m := metricApachedruidSegmentUnavailableCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentUnderReplicatedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.under_replicated.count metric with initial data.
func (m *metricApachedruidSegmentUnderReplicatedCount) init() {
	m.data.SetName("apachedruid.segment.under_replicated.count")
	m.data.SetDescription("Number of segments, including replicas, left to load until all used segments are available for queries.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentUnderReplicatedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentUnderReplicatedCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentUnderReplicatedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentUnderReplicatedCount(cfg MetricConfig) metricApachedruidSegmentUnderReplicatedCount {
	m := metricApachedruidSegmentUnderReplicatedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentUnneededCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.unneeded.count metric with initial data.
func (m *metricApachedruidSegmentUnneededCount) init() {
	m.data.SetName("apachedruid.segment.unneeded.count")
	m.data.SetDescription("Number of segments dropped due to being marked as unused.")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentUnneededCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentUnneededCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentUnneededCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentUnneededCount(cfg MetricConfig) metricApachedruidSegmentUnneededCount {
	m := metricApachedruidSegmentUnneededCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.used metric with initial data.
func (m *metricApachedruidSegmentUsed) init() {
	m.data.SetName("apachedruid.segment.used")
	m.data.SetDescription("Bytes used for served segments.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentPriorityAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("priority", segmentPriorityAttributeValue)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentUsed(cfg MetricConfig) metricApachedruidSegmentUsed {
	m := metricApachedruidSegmentUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentUsedPercent struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.used_percent metric with initial data.
func (m *metricApachedruidSegmentUsedPercent) init() {
	m.data.SetName("apachedruid.segment.used_percent")
	m.data.SetDescription("Percentage of space used by served segments.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentUsedPercent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, segmentPriorityAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("priority", segmentPriorityAttributeValue)
	dp.Attributes().PutStr("tier", segmentTierAttributeValue)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentUsedPercent) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentUsedPercent) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentUsedPercent(cfg MetricConfig) metricApachedruidSegmentUsedPercent {
	m := metricApachedruidSegmentUsedPercent{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentWaitCompactBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.wait_compact.bytes metric with initial data.
func (m *metricApachedruidSegmentWaitCompactBytes) init() {
	m.data.SetName("apachedruid.segment.wait_compact.bytes")
	m.data.SetDescription("Total bytes of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction).")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentWaitCompactBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentWaitCompactBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentWaitCompactBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentWaitCompactBytes(cfg MetricConfig) metricApachedruidSegmentWaitCompactBytes {
	m := metricApachedruidSegmentWaitCompactBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSegmentWaitCompactCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.segment.wait_compact.count metric with initial data.
func (m *metricApachedruidSegmentWaitCompactCount) init() {
	m.data.SetName("apachedruid.segment.wait_compact.count")
	m.data.SetDescription("Total number of segments of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction).")
	m.data.SetUnit("{segments}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSegmentWaitCompactCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", segmentDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSegmentWaitCompactCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSegmentWaitCompactCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSegmentWaitCompactCount(cfg MetricConfig) metricApachedruidSegmentWaitCompactCount {
	m := metricApachedruidSegmentWaitCompactCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidServerviewInitTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.serverview.init.time metric with initial data.
func (m *metricApachedruidServerviewInitTime) init() {
	m.data.SetName("apachedruid.serverview.init.time")
	m.data.SetDescription("Time taken to initialize the broker server view. Useful to detect if brokers are taking too long to start.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidServerviewInitTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidServerviewInitTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidServerviewInitTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidServerviewInitTime(cfg MetricConfig) metricApachedruidServerviewInitTime {
	m := metricApachedruidServerviewInitTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidServerviewSyncHealthy struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.serverview.sync.healthy metric with initial data.
func (m *metricApachedruidServerviewSyncHealthy) init() {
	m.data.SetName("apachedruid.serverview.sync.healthy")
	m.data.SetDescription("Sync status of the Broker with a segment-loading server such as a Historical or Peon. Emitted only when [HTTP-based server view](https,//druid.apache.org/docs/latest/configuration#segment-management) is enabled. This metric can be used in conjunction with `serverview/sync/unstableTime` to debug slow startup of Brokers.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidServerviewSyncHealthy) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serverviewTierAttributeValue string, serverviewServerAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tier", serverviewTierAttributeValue)
	dp.Attributes().PutStr("server", serverviewServerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidServerviewSyncHealthy) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidServerviewSyncHealthy) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidServerviewSyncHealthy(cfg MetricConfig) metricApachedruidServerviewSyncHealthy {
	m := metricApachedruidServerviewSyncHealthy{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidServerviewSyncUnstableTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.serverview.sync.unstable_time metric with initial data.
func (m *metricApachedruidServerviewSyncUnstableTime) init() {
	m.data.SetName("apachedruid.serverview.sync.unstable_time")
	m.data.SetDescription("Time in milliseconds for which the Broker has been failing to sync with a segment-loading server. Emitted only when [HTTP-based server view](https,//druid.apache.org/docs/latest/configuration#segment-management) is enabled.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidServerviewSyncUnstableTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, serverviewTierAttributeValue string, serverviewServerAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tier", serverviewTierAttributeValue)
	dp.Attributes().PutStr("server", serverviewServerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidServerviewSyncUnstableTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidServerviewSyncUnstableTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidServerviewSyncUnstableTime(cfg MetricConfig) metricApachedruidServerviewSyncUnstableTime {
	m := metricApachedruidServerviewSyncUnstableTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSQLQueryBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sql_query.bytes metric with initial data.
func (m *metricApachedruidSQLQueryBytes) init() {
	m.data.SetName("apachedruid.sql_query.bytes")
	m.data.SetDescription("Number of bytes returned in the SQL query response.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSQLQueryBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sqlQueryDataSourceAttributeValue string, sqlQueryNativeQueryIdsAttributeValue string, sqlQueryEngineAttributeValue string, sqlQueryRemoteAddressAttributeValue string, sqlQueryIDAttributeValue string, sqlQuerySuccessAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", sqlQueryDataSourceAttributeValue)
	dp.Attributes().PutStr("native_query_ids", sqlQueryNativeQueryIdsAttributeValue)
	dp.Attributes().PutStr("engine", sqlQueryEngineAttributeValue)
	dp.Attributes().PutStr("remote_address", sqlQueryRemoteAddressAttributeValue)
	dp.Attributes().PutStr("id", sqlQueryIDAttributeValue)
	dp.Attributes().PutStr("success", sqlQuerySuccessAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSQLQueryBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSQLQueryBytes) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSQLQueryBytes(cfg MetricConfig) metricApachedruidSQLQueryBytes {
	m := metricApachedruidSQLQueryBytes{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSQLQueryPlanningTimeMs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sql_query.planning_time_ms metric with initial data.
func (m *metricApachedruidSQLQueryPlanningTimeMs) init() {
	m.data.SetName("apachedruid.sql_query.planning_time_ms")
	m.data.SetDescription("Milliseconds taken to plan a SQL to native query.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSQLQueryPlanningTimeMs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sqlQueryDataSourceAttributeValue string, sqlQueryNativeQueryIdsAttributeValue string, sqlQueryEngineAttributeValue string, sqlQueryRemoteAddressAttributeValue string, sqlQueryIDAttributeValue string, sqlQuerySuccessAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", sqlQueryDataSourceAttributeValue)
	dp.Attributes().PutStr("native_query_ids", sqlQueryNativeQueryIdsAttributeValue)
	dp.Attributes().PutStr("engine", sqlQueryEngineAttributeValue)
	dp.Attributes().PutStr("remote_address", sqlQueryRemoteAddressAttributeValue)
	dp.Attributes().PutStr("id", sqlQueryIDAttributeValue)
	dp.Attributes().PutStr("success", sqlQuerySuccessAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSQLQueryPlanningTimeMs) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSQLQueryPlanningTimeMs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSQLQueryPlanningTimeMs(cfg MetricConfig) metricApachedruidSQLQueryPlanningTimeMs {
	m := metricApachedruidSQLQueryPlanningTimeMs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSQLQueryTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sql_query.time metric with initial data.
func (m *metricApachedruidSQLQueryTime) init() {
	m.data.SetName("apachedruid.sql_query.time")
	m.data.SetDescription("Milliseconds taken to complete a SQL query.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSQLQueryTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sqlQueryDataSourceAttributeValue string, sqlQueryNativeQueryIdsAttributeValue string, sqlQueryEngineAttributeValue string, sqlQueryRemoteAddressAttributeValue string, sqlQueryIDAttributeValue string, sqlQuerySuccessAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", sqlQueryDataSourceAttributeValue)
	dp.Attributes().PutStr("native_query_ids", sqlQueryNativeQueryIdsAttributeValue)
	dp.Attributes().PutStr("engine", sqlQueryEngineAttributeValue)
	dp.Attributes().PutStr("remote_address", sqlQueryRemoteAddressAttributeValue)
	dp.Attributes().PutStr("id", sqlQueryIDAttributeValue)
	dp.Attributes().PutStr("success", sqlQuerySuccessAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSQLQueryTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSQLQueryTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSQLQueryTime(cfg MetricConfig) metricApachedruidSQLQueryTime {
	m := metricApachedruidSQLQueryTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSubqueryByteLimitCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.subquery.byte_limit.count metric with initial data.
func (m *metricApachedruidSubqueryByteLimitCount) init() {
	m.data.SetName("apachedruid.subquery.byte_limit.count")
	m.data.SetDescription("Number of subqueries whose results are materialized as frames (Druid's internal byte representation of rows).")
	m.data.SetUnit("{subqueries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidSubqueryByteLimitCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSubqueryByteLimitCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSubqueryByteLimitCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSubqueryByteLimitCount(cfg MetricConfig) metricApachedruidSubqueryByteLimitCount {
	m := metricApachedruidSubqueryByteLimitCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSubqueryFallbackCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.subquery.fallback.count metric with initial data.
func (m *metricApachedruidSubqueryFallbackCount) init() {
	m.data.SetName("apachedruid.subquery.fallback.count")
	m.data.SetDescription("Number of subqueries which cannot be materialized as frames.")
	m.data.SetUnit("{subqueries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidSubqueryFallbackCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSubqueryFallbackCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSubqueryFallbackCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSubqueryFallbackCount(cfg MetricConfig) metricApachedruidSubqueryFallbackCount {
	m := metricApachedruidSubqueryFallbackCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSubqueryFallbackInsufficientTypeCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.subquery.fallback.insufficient_type.count metric with initial data.
func (m *metricApachedruidSubqueryFallbackInsufficientTypeCount) init() {
	m.data.SetName("apachedruid.subquery.fallback.insufficient_type.count")
	m.data.SetDescription("Number of subqueries which cannot be materialized as frames due to insufficient type information in the row signature.")
	m.data.SetUnit("{subqueries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidSubqueryFallbackInsufficientTypeCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSubqueryFallbackInsufficientTypeCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSubqueryFallbackInsufficientTypeCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSubqueryFallbackInsufficientTypeCount(cfg MetricConfig) metricApachedruidSubqueryFallbackInsufficientTypeCount {
	m := metricApachedruidSubqueryFallbackInsufficientTypeCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSubqueryFallbackUnknownReasonCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.subquery.fallback.unknown_reason.count metric with initial data.
func (m *metricApachedruidSubqueryFallbackUnknownReasonCount) init() {
	m.data.SetName("apachedruid.subquery.fallback.unknown_reason.count")
	m.data.SetDescription("Number of subqueries which cannot be materialized as frames due other reasons.")
	m.data.SetUnit("{subqueries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidSubqueryFallbackUnknownReasonCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSubqueryFallbackUnknownReasonCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSubqueryFallbackUnknownReasonCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSubqueryFallbackUnknownReasonCount(cfg MetricConfig) metricApachedruidSubqueryFallbackUnknownReasonCount {
	m := metricApachedruidSubqueryFallbackUnknownReasonCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSubqueryRowLimitCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.subquery.row_limit.count metric with initial data.
func (m *metricApachedruidSubqueryRowLimitCount) init() {
	m.data.SetName("apachedruid.subquery.row_limit.count")
	m.data.SetDescription("Number of subqueries whose results are materialized as rows (Java objects on heap).")
	m.data.SetUnit("{subqueries}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
}

func (m *metricApachedruidSubqueryRowLimitCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSubqueryRowLimitCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSubqueryRowLimitCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSubqueryRowLimitCount(cfg MetricConfig) metricApachedruidSubqueryRowLimitCount {
	m := metricApachedruidSubqueryRowLimitCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysCPU struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.cpu metric with initial data.
func (m *metricApachedruidSysCPU) init() {
	m.data.SetName("apachedruid.sys.cpu")
	m.data.SetDescription("CPU used.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysCPU) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysCPUTimeAttributeValue string, sysCPUNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cpu_time", sysCPUTimeAttributeValue)
	dp.Attributes().PutStr("cpu_name", sysCPUNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysCPU) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysCPU) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysCPU(cfg MetricConfig) metricApachedruidSysCPU {
	m := metricApachedruidSysCPU{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysDiskQueue struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.disk.queue metric with initial data.
func (m *metricApachedruidSysDiskQueue) init() {
	m.data.SetName("apachedruid.sys.disk.queue")
	m.data.SetDescription("Disk queue length. Measures number of requests waiting to be processed by disk.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysDiskQueue) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("disk_name", sysDiskNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysDiskQueue) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysDiskQueue) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysDiskQueue(cfg MetricConfig) metricApachedruidSysDiskQueue {
	m := metricApachedruidSysDiskQueue{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysDiskReadCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.disk.read.count metric with initial data.
func (m *metricApachedruidSysDiskReadCount) init() {
	m.data.SetName("apachedruid.sys.disk.read.count")
	m.data.SetDescription("Reads from disk.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysDiskReadCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("disk_name", sysDiskNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysDiskReadCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysDiskReadCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysDiskReadCount(cfg MetricConfig) metricApachedruidSysDiskReadCount {
	m := metricApachedruidSysDiskReadCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysDiskReadSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.disk.read.size metric with initial data.
func (m *metricApachedruidSysDiskReadSize) init() {
	m.data.SetName("apachedruid.sys.disk.read.size")
	m.data.SetDescription("Bytes read from disk. One indicator of the amount of paging occurring for segments.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysDiskReadSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("disk_name", sysDiskNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysDiskReadSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysDiskReadSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysDiskReadSize(cfg MetricConfig) metricApachedruidSysDiskReadSize {
	m := metricApachedruidSysDiskReadSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysDiskTransferTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.disk.transfer_time metric with initial data.
func (m *metricApachedruidSysDiskTransferTime) init() {
	m.data.SetName("apachedruid.sys.disk.transfer_time")
	m.data.SetDescription("Transfer time to read from or write to disk.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysDiskTransferTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("disk_name", sysDiskNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysDiskTransferTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysDiskTransferTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysDiskTransferTime(cfg MetricConfig) metricApachedruidSysDiskTransferTime {
	m := metricApachedruidSysDiskTransferTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysDiskWriteCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.disk.write.count metric with initial data.
func (m *metricApachedruidSysDiskWriteCount) init() {
	m.data.SetName("apachedruid.sys.disk.write.count")
	m.data.SetDescription("Writes to disk.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysDiskWriteCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("disk_name", sysDiskNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysDiskWriteCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysDiskWriteCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysDiskWriteCount(cfg MetricConfig) metricApachedruidSysDiskWriteCount {
	m := metricApachedruidSysDiskWriteCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysDiskWriteSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.disk.write.size metric with initial data.
func (m *metricApachedruidSysDiskWriteSize) init() {
	m.data.SetName("apachedruid.sys.disk.write.size")
	m.data.SetDescription("Bytes written to disk. One indicator of the amount of paging occurring for segments.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysDiskWriteSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("disk_name", sysDiskNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysDiskWriteSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysDiskWriteSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysDiskWriteSize(cfg MetricConfig) metricApachedruidSysDiskWriteSize {
	m := metricApachedruidSysDiskWriteSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysFsFilesCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.fs.files.count metric with initial data.
func (m *metricApachedruidSysFsFilesCount) init() {
	m.data.SetName("apachedruid.sys.fs.files.count")
	m.data.SetDescription("Filesystem total IO nodes.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysFsFilesCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysFsDirNameAttributeValue string, sysFsDevNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("fs_dir_name", sysFsDirNameAttributeValue)
	dp.Attributes().PutStr("fs_dev_name", sysFsDevNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysFsFilesCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysFsFilesCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysFsFilesCount(cfg MetricConfig) metricApachedruidSysFsFilesCount {
	m := metricApachedruidSysFsFilesCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysFsFilesFree struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.fs.files.free metric with initial data.
func (m *metricApachedruidSysFsFilesFree) init() {
	m.data.SetName("apachedruid.sys.fs.files.free")
	m.data.SetDescription("Filesystem free IO nodes.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysFsFilesFree) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysFsDirNameAttributeValue string, sysFsDevNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("fs_dir_name", sysFsDirNameAttributeValue)
	dp.Attributes().PutStr("fs_dev_name", sysFsDevNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysFsFilesFree) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysFsFilesFree) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysFsFilesFree(cfg MetricConfig) metricApachedruidSysFsFilesFree {
	m := metricApachedruidSysFsFilesFree{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysFsMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.fs.max metric with initial data.
func (m *metricApachedruidSysFsMax) init() {
	m.data.SetName("apachedruid.sys.fs.max")
	m.data.SetDescription("Filesystem bytes max.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysFsMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysFsDirNameAttributeValue string, sysFsDevNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("fs_dir_name", sysFsDirNameAttributeValue)
	dp.Attributes().PutStr("fs_dev_name", sysFsDevNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysFsMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysFsMax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysFsMax(cfg MetricConfig) metricApachedruidSysFsMax {
	m := metricApachedruidSysFsMax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysFsUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.fs.used metric with initial data.
func (m *metricApachedruidSysFsUsed) init() {
	m.data.SetName("apachedruid.sys.fs.used")
	m.data.SetDescription("Filesystem bytes used.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysFsUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysFsDirNameAttributeValue string, sysFsDevNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("fs_dir_name", sysFsDirNameAttributeValue)
	dp.Attributes().PutStr("fs_dev_name", sysFsDevNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysFsUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysFsUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysFsUsed(cfg MetricConfig) metricApachedruidSysFsUsed {
	m := metricApachedruidSysFsUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysLa1 struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.la.1 metric with initial data.
func (m *metricApachedruidSysLa1) init() {
	m.data.SetName("apachedruid.sys.la.1")
	m.data.SetDescription("System CPU load averages over past `i` minutes, where `i={1,5,15}`.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysLa1) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysLa1) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysLa1) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysLa1(cfg MetricConfig) metricApachedruidSysLa1 {
	m := metricApachedruidSysLa1{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysLa15 struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.la.15 metric with initial data.
func (m *metricApachedruidSysLa15) init() {
	m.data.SetName("apachedruid.sys.la.15")
	m.data.SetDescription("System CPU load averages over past `i` minutes, where `i={1,5,15}`.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysLa15) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysLa15) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysLa15) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysLa15(cfg MetricConfig) metricApachedruidSysLa15 {
	m := metricApachedruidSysLa15{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysLa5 struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.la.5 metric with initial data.
func (m *metricApachedruidSysLa5) init() {
	m.data.SetName("apachedruid.sys.la.5")
	m.data.SetDescription("System CPU load averages over past `i` minutes, where `i={1,5,15}`.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysLa5) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysLa5) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysLa5) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysLa5(cfg MetricConfig) metricApachedruidSysLa5 {
	m := metricApachedruidSysLa5{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysMemFree struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.mem.free metric with initial data.
func (m *metricApachedruidSysMemFree) init() {
	m.data.SetName("apachedruid.sys.mem.free")
	m.data.SetDescription("Memory free.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysMemFree) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysMemFree) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysMemFree) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysMemFree(cfg MetricConfig) metricApachedruidSysMemFree {
	m := metricApachedruidSysMemFree{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysMemMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.mem.max metric with initial data.
func (m *metricApachedruidSysMemMax) init() {
	m.data.SetName("apachedruid.sys.mem.max")
	m.data.SetDescription("Memory max.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysMemMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysMemMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysMemMax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysMemMax(cfg MetricConfig) metricApachedruidSysMemMax {
	m := metricApachedruidSysMemMax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysMemUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.mem.used metric with initial data.
func (m *metricApachedruidSysMemUsed) init() {
	m.data.SetName("apachedruid.sys.mem.used")
	m.data.SetDescription("Memory used.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysMemUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysMemUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysMemUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysMemUsed(cfg MetricConfig) metricApachedruidSysMemUsed {
	m := metricApachedruidSysMemUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysNetReadDropped struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.net.read.dropped metric with initial data.
func (m *metricApachedruidSysNetReadDropped) init() {
	m.data.SetName("apachedruid.sys.net.read.dropped")
	m.data.SetDescription("Total packets dropped coming from network.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysNetReadDropped) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("net_hwaddr", sysNetHwaddrAttributeValue)
	dp.Attributes().PutStr("net_name", sysNetNameAttributeValue)
	dp.Attributes().PutStr("net_address", sysNetAddressAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysNetReadDropped) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysNetReadDropped) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysNetReadDropped(cfg MetricConfig) metricApachedruidSysNetReadDropped {
	m := metricApachedruidSysNetReadDropped{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysNetReadErrors struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.net.read.errors metric with initial data.
func (m *metricApachedruidSysNetReadErrors) init() {
	m.data.SetName("apachedruid.sys.net.read.errors")
	m.data.SetDescription("Total network read errors.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysNetReadErrors) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("net_hwaddr", sysNetHwaddrAttributeValue)
	dp.Attributes().PutStr("net_name", sysNetNameAttributeValue)
	dp.Attributes().PutStr("net_address", sysNetAddressAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysNetReadErrors) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysNetReadErrors) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysNetReadErrors(cfg MetricConfig) metricApachedruidSysNetReadErrors {
	m := metricApachedruidSysNetReadErrors{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysNetReadPackets struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.net.read.packets metric with initial data.
func (m *metricApachedruidSysNetReadPackets) init() {
	m.data.SetName("apachedruid.sys.net.read.packets")
	m.data.SetDescription("Total packets read from the network.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysNetReadPackets) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("net_hwaddr", sysNetHwaddrAttributeValue)
	dp.Attributes().PutStr("net_name", sysNetNameAttributeValue)
	dp.Attributes().PutStr("net_address", sysNetAddressAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysNetReadPackets) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysNetReadPackets) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysNetReadPackets(cfg MetricConfig) metricApachedruidSysNetReadPackets {
	m := metricApachedruidSysNetReadPackets{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysNetReadSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.net.read.size metric with initial data.
func (m *metricApachedruidSysNetReadSize) init() {
	m.data.SetName("apachedruid.sys.net.read.size")
	m.data.SetDescription("Bytes read from the network.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysNetReadSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("net_hwaddr", sysNetHwaddrAttributeValue)
	dp.Attributes().PutStr("net_name", sysNetNameAttributeValue)
	dp.Attributes().PutStr("net_address", sysNetAddressAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysNetReadSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysNetReadSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysNetReadSize(cfg MetricConfig) metricApachedruidSysNetReadSize {
	m := metricApachedruidSysNetReadSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysNetWriteCollisions struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.net.write.collisions metric with initial data.
func (m *metricApachedruidSysNetWriteCollisions) init() {
	m.data.SetName("apachedruid.sys.net.write.collisions")
	m.data.SetDescription("Total network write collisions.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysNetWriteCollisions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("net_hwaddr", sysNetHwaddrAttributeValue)
	dp.Attributes().PutStr("net_name", sysNetNameAttributeValue)
	dp.Attributes().PutStr("net_address", sysNetAddressAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysNetWriteCollisions) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysNetWriteCollisions) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysNetWriteCollisions(cfg MetricConfig) metricApachedruidSysNetWriteCollisions {
	m := metricApachedruidSysNetWriteCollisions{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysNetWriteErrors struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.net.write.errors metric with initial data.
func (m *metricApachedruidSysNetWriteErrors) init() {
	m.data.SetName("apachedruid.sys.net.write.errors")
	m.data.SetDescription("Total network write errors.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysNetWriteErrors) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("net_hwaddr", sysNetHwaddrAttributeValue)
	dp.Attributes().PutStr("net_name", sysNetNameAttributeValue)
	dp.Attributes().PutStr("net_address", sysNetAddressAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysNetWriteErrors) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysNetWriteErrors) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysNetWriteErrors(cfg MetricConfig) metricApachedruidSysNetWriteErrors {
	m := metricApachedruidSysNetWriteErrors{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysNetWritePackets struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.net.write.packets metric with initial data.
func (m *metricApachedruidSysNetWritePackets) init() {
	m.data.SetName("apachedruid.sys.net.write.packets")
	m.data.SetDescription("Total packets written to the network.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysNetWritePackets) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("net_hwaddr", sysNetHwaddrAttributeValue)
	dp.Attributes().PutStr("net_name", sysNetNameAttributeValue)
	dp.Attributes().PutStr("net_address", sysNetAddressAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysNetWritePackets) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysNetWritePackets) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysNetWritePackets(cfg MetricConfig) metricApachedruidSysNetWritePackets {
	m := metricApachedruidSysNetWritePackets{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysNetWriteSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.net.write.size metric with initial data.
func (m *metricApachedruidSysNetWriteSize) init() {
	m.data.SetName("apachedruid.sys.net.write.size")
	m.data.SetDescription("Bytes written to the network.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysNetWriteSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("net_hwaddr", sysNetHwaddrAttributeValue)
	dp.Attributes().PutStr("net_name", sysNetNameAttributeValue)
	dp.Attributes().PutStr("net_address", sysNetAddressAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysNetWriteSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysNetWriteSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysNetWriteSize(cfg MetricConfig) metricApachedruidSysNetWriteSize {
	m := metricApachedruidSysNetWriteSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysStorageUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.storage.used metric with initial data.
func (m *metricApachedruidSysStorageUsed) init() {
	m.data.SetName("apachedruid.sys.storage.used")
	m.data.SetDescription("Disk space used.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSysStorageUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, sysFsDirNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("fs_dir_name", sysFsDirNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysStorageUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysStorageUsed) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysStorageUsed(cfg MetricConfig) metricApachedruidSysStorageUsed {
	m := metricApachedruidSysStorageUsed{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysSwapFree struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.swap.free metric with initial data.
func (m *metricApachedruidSysSwapFree) init() {
	m.data.SetName("apachedruid.sys.swap.free")
	m.data.SetDescription("Free swap.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysSwapFree) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysSwapFree) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysSwapFree) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysSwapFree(cfg MetricConfig) metricApachedruidSysSwapFree {
	m := metricApachedruidSysSwapFree{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysSwapMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.swap.max metric with initial data.
func (m *metricApachedruidSysSwapMax) init() {
	m.data.SetName("apachedruid.sys.swap.max")
	m.data.SetDescription("Max swap.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysSwapMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysSwapMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysSwapMax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysSwapMax(cfg MetricConfig) metricApachedruidSysSwapMax {
	m := metricApachedruidSysSwapMax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysSwapPageIn struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.swap.page_in metric with initial data.
func (m *metricApachedruidSysSwapPageIn) init() {
	m.data.SetName("apachedruid.sys.swap.page_in")
	m.data.SetDescription("Paged in swap.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysSwapPageIn) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysSwapPageIn) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysSwapPageIn) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysSwapPageIn(cfg MetricConfig) metricApachedruidSysSwapPageIn {
	m := metricApachedruidSysSwapPageIn{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysSwapPageOut struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.swap.page_out metric with initial data.
func (m *metricApachedruidSysSwapPageOut) init() {
	m.data.SetName("apachedruid.sys.swap.page_out")
	m.data.SetDescription("Paged out swap.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysSwapPageOut) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysSwapPageOut) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysSwapPageOut) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysSwapPageOut(cfg MetricConfig) metricApachedruidSysSwapPageOut {
	m := metricApachedruidSysSwapPageOut{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysTcpv4ActiveOpens struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.tcpv4.active_opens metric with initial data.
func (m *metricApachedruidSysTcpv4ActiveOpens) init() {
	m.data.SetName("apachedruid.sys.tcpv4.active_opens")
	m.data.SetDescription("Total TCP active open connections.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysTcpv4ActiveOpens) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysTcpv4ActiveOpens) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysTcpv4ActiveOpens) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysTcpv4ActiveOpens(cfg MetricConfig) metricApachedruidSysTcpv4ActiveOpens {
	m := metricApachedruidSysTcpv4ActiveOpens{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysTcpv4AttemptFails struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.tcpv4.attempt_fails metric with initial data.
func (m *metricApachedruidSysTcpv4AttemptFails) init() {
	m.data.SetName("apachedruid.sys.tcpv4.attempt_fails")
	m.data.SetDescription("Total TCP active connection failures.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysTcpv4AttemptFails) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysTcpv4AttemptFails) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysTcpv4AttemptFails) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysTcpv4AttemptFails(cfg MetricConfig) metricApachedruidSysTcpv4AttemptFails {
	m := metricApachedruidSysTcpv4AttemptFails{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysTcpv4EstabResets struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.tcpv4.estab_resets metric with initial data.
func (m *metricApachedruidSysTcpv4EstabResets) init() {
	m.data.SetName("apachedruid.sys.tcpv4.estab_resets")
	m.data.SetDescription("Total TCP connection resets.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysTcpv4EstabResets) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysTcpv4EstabResets) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysTcpv4EstabResets) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysTcpv4EstabResets(cfg MetricConfig) metricApachedruidSysTcpv4EstabResets {
	m := metricApachedruidSysTcpv4EstabResets{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysTcpv4InErrs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.tcpv4.in.errs metric with initial data.
func (m *metricApachedruidSysTcpv4InErrs) init() {
	m.data.SetName("apachedruid.sys.tcpv4.in.errs")
	m.data.SetDescription("Errors while reading segments.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysTcpv4InErrs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysTcpv4InErrs) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysTcpv4InErrs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysTcpv4InErrs(cfg MetricConfig) metricApachedruidSysTcpv4InErrs {
	m := metricApachedruidSysTcpv4InErrs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysTcpv4InSegs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.tcpv4.in.segs metric with initial data.
func (m *metricApachedruidSysTcpv4InSegs) init() {
	m.data.SetName("apachedruid.sys.tcpv4.in.segs")
	m.data.SetDescription("Total segments received in connection.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysTcpv4InSegs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysTcpv4InSegs) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysTcpv4InSegs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysTcpv4InSegs(cfg MetricConfig) metricApachedruidSysTcpv4InSegs {
	m := metricApachedruidSysTcpv4InSegs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysTcpv4OutRsts struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.tcpv4.out.rsts metric with initial data.
func (m *metricApachedruidSysTcpv4OutRsts) init() {
	m.data.SetName("apachedruid.sys.tcpv4.out.rsts")
	m.data.SetDescription("Total `out reset` packets sent to reset the connection.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysTcpv4OutRsts) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysTcpv4OutRsts) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysTcpv4OutRsts) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysTcpv4OutRsts(cfg MetricConfig) metricApachedruidSysTcpv4OutRsts {
	m := metricApachedruidSysTcpv4OutRsts{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysTcpv4OutSegs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.tcpv4.out.segs metric with initial data.
func (m *metricApachedruidSysTcpv4OutSegs) init() {
	m.data.SetName("apachedruid.sys.tcpv4.out.segs")
	m.data.SetDescription("Total segments sent.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysTcpv4OutSegs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysTcpv4OutSegs) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysTcpv4OutSegs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysTcpv4OutSegs(cfg MetricConfig) metricApachedruidSysTcpv4OutSegs {
	m := metricApachedruidSysTcpv4OutSegs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysTcpv4PassiveOpens struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.tcpv4.passive_opens metric with initial data.
func (m *metricApachedruidSysTcpv4PassiveOpens) init() {
	m.data.SetName("apachedruid.sys.tcpv4.passive_opens")
	m.data.SetDescription("Total TCP passive open connections.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysTcpv4PassiveOpens) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysTcpv4PassiveOpens) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysTcpv4PassiveOpens) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysTcpv4PassiveOpens(cfg MetricConfig) metricApachedruidSysTcpv4PassiveOpens {
	m := metricApachedruidSysTcpv4PassiveOpens{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysTcpv4RetransSegs struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.tcpv4.retrans.segs metric with initial data.
func (m *metricApachedruidSysTcpv4RetransSegs) init() {
	m.data.SetName("apachedruid.sys.tcpv4.retrans.segs")
	m.data.SetDescription("Total segments re-transmitted.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysTcpv4RetransSegs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysTcpv4RetransSegs) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysTcpv4RetransSegs) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysTcpv4RetransSegs(cfg MetricConfig) metricApachedruidSysTcpv4RetransSegs {
	m := metricApachedruidSysTcpv4RetransSegs{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSysUptime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sys.uptime metric with initial data.
func (m *metricApachedruidSysUptime) init() {
	m.data.SetName("apachedruid.sys.uptime")
	m.data.SetDescription("Total system uptime.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidSysUptime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSysUptime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSysUptime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSysUptime(cfg MetricConfig) metricApachedruidSysUptime {
	m := metricApachedruidSysUptime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskActionBatchAttempts struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.action.batch.attempts metric with initial data.
func (m *metricApachedruidTaskActionBatchAttempts) init() {
	m.data.SetName("apachedruid.task.action.batch.attempts")
	m.data.SetDescription("Number of execution attempts for a single batch of task actions. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).")
	m.data.SetUnit("{attempts}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskActionBatchAttempts) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskIntervalAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("interval", taskIntervalAttributeValue)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
	dp.Attributes().PutStr("task_action_type", taskActionTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskActionBatchAttempts) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskActionBatchAttempts) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskActionBatchAttempts(cfg MetricConfig) metricApachedruidTaskActionBatchAttempts {
	m := metricApachedruidTaskActionBatchAttempts{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskActionBatchQueueTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.action.batch.queue_time metric with initial data.
func (m *metricApachedruidTaskActionBatchQueueTime) init() {
	m.data.SetName("apachedruid.task.action.batch.queue_time")
	m.data.SetDescription("Milliseconds spent by a batch of task actions in queue. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskActionBatchQueueTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskIntervalAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("interval", taskIntervalAttributeValue)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
	dp.Attributes().PutStr("task_action_type", taskActionTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskActionBatchQueueTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskActionBatchQueueTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskActionBatchQueueTime(cfg MetricConfig) metricApachedruidTaskActionBatchQueueTime {
	m := metricApachedruidTaskActionBatchQueueTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskActionBatchRunTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.action.batch.run_time metric with initial data.
func (m *metricApachedruidTaskActionBatchRunTime) init() {
	m.data.SetName("apachedruid.task.action.batch.run_time")
	m.data.SetDescription("Milliseconds taken to execute a batch of task actions. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskActionBatchRunTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskIntervalAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("interval", taskIntervalAttributeValue)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
	dp.Attributes().PutStr("task_action_type", taskActionTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskActionBatchRunTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskActionBatchRunTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskActionBatchRunTime(cfg MetricConfig) metricApachedruidTaskActionBatchRunTime {
	m := metricApachedruidTaskActionBatchRunTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskActionBatchSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.action.batch.size metric with initial data.
func (m *metricApachedruidTaskActionBatchSize) init() {
	m.data.SetName("apachedruid.task.action.batch.size")
	m.data.SetDescription("Number of task actions in a batch that was executed during the emission period. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).")
	m.data.SetUnit("{actions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskActionBatchSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskIntervalAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("interval", taskIntervalAttributeValue)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
	dp.Attributes().PutStr("task_action_type", taskActionTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskActionBatchSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskActionBatchSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskActionBatchSize(cfg MetricConfig) metricApachedruidTaskActionBatchSize {
	m := metricApachedruidTaskActionBatchSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskActionFailedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.action.failed.count metric with initial data.
func (m *metricApachedruidTaskActionFailedCount) init() {
	m.data.SetName("apachedruid.task.action.failed.count")
	m.data.SetDescription("Number of task actions that failed during the emission period. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).")
	m.data.SetUnit("{actions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskActionFailedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string, taskGroupIDAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", taskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
	dp.Attributes().PutStr("task_action_type", taskActionTypeAttributeValue)
	dp.Attributes().PutStr("group_id", taskGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", taskTagsAttributeValue)
	dp.Attributes().PutStr("task_id", taskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskActionFailedCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskActionFailedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskActionFailedCount(cfg MetricConfig) metricApachedruidTaskActionFailedCount {
	m := metricApachedruidTaskActionFailedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskActionLogTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.action.log.time metric with initial data.
func (m *metricApachedruidTaskActionLogTime) init() {
	m.data.SetName("apachedruid.task.action.log.time")
	m.data.SetDescription("Milliseconds taken to log a task action to the audit log.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskActionLogTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string, taskGroupIDAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", taskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
	dp.Attributes().PutStr("task_action_type", taskActionTypeAttributeValue)
	dp.Attributes().PutStr("group_id", taskGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", taskTagsAttributeValue)
	dp.Attributes().PutStr("task_id", taskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskActionLogTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskActionLogTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskActionLogTime(cfg MetricConfig) metricApachedruidTaskActionLogTime {
	m := metricApachedruidTaskActionLogTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskActionRunTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.action.run.time metric with initial data.
func (m *metricApachedruidTaskActionRunTime) init() {
	m.data.SetName("apachedruid.task.action.run.time")
	m.data.SetDescription("Milliseconds taken to execute a task action.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskActionRunTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string, taskGroupIDAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", taskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
	dp.Attributes().PutStr("task_action_type", taskActionTypeAttributeValue)
	dp.Attributes().PutStr("group_id", taskGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", taskTagsAttributeValue)
	dp.Attributes().PutStr("task_id", taskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskActionRunTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskActionRunTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskActionRunTime(cfg MetricConfig) metricApachedruidTaskActionRunTime {
	m := metricApachedruidTaskActionRunTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskActionSuccessCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.action.success.count metric with initial data.
func (m *metricApachedruidTaskActionSuccessCount) init() {
	m.data.SetName("apachedruid.task.action.success.count")
	m.data.SetDescription("Number of task actions that were executed successfully during the emission period. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).")
	m.data.SetUnit("{actions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskActionSuccessCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string, taskGroupIDAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", taskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
	dp.Attributes().PutStr("task_action_type", taskActionTypeAttributeValue)
	dp.Attributes().PutStr("group_id", taskGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", taskTagsAttributeValue)
	dp.Attributes().PutStr("task_id", taskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskActionSuccessCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskActionSuccessCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskActionSuccessCount(cfg MetricConfig) metricApachedruidTaskActionSuccessCount {
	m := metricApachedruidTaskActionSuccessCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskFailedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.failed.count metric with initial data.
func (m *metricApachedruidTaskFailedCount) init() {
	m.data.SetName("apachedruid.task.failed.count")
	m.data.SetDescription("Number of failed tasks per emission period. This metric is only available if the `TaskCountStatsMonitor` module is included.")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskFailedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskFailedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskFailedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskFailedCount(cfg MetricConfig) metricApachedruidTaskFailedCount {
	m := metricApachedruidTaskFailedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskPendingCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.pending.count metric with initial data.
func (m *metricApachedruidTaskPendingCount) init() {
	m.data.SetName("apachedruid.task.pending.count")
	m.data.SetDescription("Number of current pending tasks. This metric is only available if the `TaskCountStatsMonitor` module is included.")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskPendingCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskPendingCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskPendingCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskPendingCount(cfg MetricConfig) metricApachedruidTaskPendingCount {
	m := metricApachedruidTaskPendingCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskPendingTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.pending.time metric with initial data.
func (m *metricApachedruidTaskPendingTime) init() {
	m.data.SetName("apachedruid.task.pending.time")
	m.data.SetDescription("Milliseconds taken for a task to wait for running.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskPendingTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskGroupIDAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", taskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", taskGroupIDAttributeValue)
	dp.Attributes().PutStr("tags", taskTagsAttributeValue)
	dp.Attributes().PutStr("task_id", taskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskPendingTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskPendingTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskPendingTime(cfg MetricConfig) metricApachedruidTaskPendingTime {
	m := metricApachedruidTaskPendingTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskRunTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.run.time metric with initial data.
func (m *metricApachedruidTaskRunTime) init() {
	m.data.SetName("apachedruid.task.run.time")
	m.data.SetDescription("Milliseconds taken to run a task.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskRunTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskGroupIDAttributeValue string, taskStatusAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", taskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", taskGroupIDAttributeValue)
	dp.Attributes().PutStr("task_status", taskStatusAttributeValue)
	dp.Attributes().PutStr("tags", taskTagsAttributeValue)
	dp.Attributes().PutStr("task_id", taskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskRunTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskRunTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskRunTime(cfg MetricConfig) metricApachedruidTaskRunTime {
	m := metricApachedruidTaskRunTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskRunningCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.running.count metric with initial data.
func (m *metricApachedruidTaskRunningCount) init() {
	m.data.SetName("apachedruid.task.running.count")
	m.data.SetDescription("Number of current running tasks. This metric is only available if the `TaskCountStatsMonitor` module is included.")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskRunningCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskRunningCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskRunningCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskRunningCount(cfg MetricConfig) metricApachedruidTaskRunningCount {
	m := metricApachedruidTaskRunningCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskSegmentAvailabilityWaitTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.segment_availability.wait.time metric with initial data.
func (m *metricApachedruidTaskSegmentAvailabilityWaitTime) init() {
	m.data.SetName("apachedruid.task.segment_availability.wait.time")
	m.data.SetDescription("The amount of milliseconds a batch indexing task waited for newly created segments to become available for querying.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskSegmentAvailabilityWaitTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskGroupIDAttributeValue string, taskSegmentAvailabilityConfirmedAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("task_type", taskTypeAttributeValue)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
	dp.Attributes().PutStr("group_id", taskGroupIDAttributeValue)
	dp.Attributes().PutStr("segment_availability_confirmed", taskSegmentAvailabilityConfirmedAttributeValue)
	dp.Attributes().PutStr("tags", taskTagsAttributeValue)
	dp.Attributes().PutStr("task_id", taskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskSegmentAvailabilityWaitTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskSegmentAvailabilityWaitTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskSegmentAvailabilityWaitTime(cfg MetricConfig) metricApachedruidTaskSegmentAvailabilityWaitTime {
	m := metricApachedruidTaskSegmentAvailabilityWaitTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskSuccessCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.success.count metric with initial data.
func (m *metricApachedruidTaskSuccessCount) init() {
	m.data.SetName("apachedruid.task.success.count")
	m.data.SetDescription("Number of successful tasks per emission period. This metric is only available if the `TaskCountStatsMonitor` module is included.")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskSuccessCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskSuccessCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskSuccessCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskSuccessCount(cfg MetricConfig) metricApachedruidTaskSuccessCount {
	m := metricApachedruidTaskSuccessCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskWaitingCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task.waiting.count metric with initial data.
func (m *metricApachedruidTaskWaitingCount) init() {
	m.data.SetName("apachedruid.task.waiting.count")
	m.data.SetDescription("Number of current waiting tasks. This metric is only available if the `TaskCountStatsMonitor` module is included.")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskWaitingCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskDataSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", taskDataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskWaitingCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskWaitingCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskWaitingCount(cfg MetricConfig) metricApachedruidTaskWaitingCount {
	m := metricApachedruidTaskWaitingCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskSlotBlacklistedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task_slot.blacklisted.count metric with initial data.
func (m *metricApachedruidTaskSlotBlacklistedCount) init() {
	m.data.SetName("apachedruid.task_slot.blacklisted.count")
	m.data.SetDescription("Number of total task slots in blacklisted Middle Managers and Indexers per emission period. This metric is only available if the `TaskSlotCountStatsMonitor` module is included.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskSlotBlacklistedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskSlotCategoryAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("category", taskSlotCategoryAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskSlotBlacklistedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskSlotBlacklistedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskSlotBlacklistedCount(cfg MetricConfig) metricApachedruidTaskSlotBlacklistedCount {
	m := metricApachedruidTaskSlotBlacklistedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskSlotIdleCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task_slot.idle.count metric with initial data.
func (m *metricApachedruidTaskSlotIdleCount) init() {
	m.data.SetName("apachedruid.task_slot.idle.count")
	m.data.SetDescription("Number of idle task slots per emission period. This metric is only available if the `TaskSlotCountStatsMonitor` module is included.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskSlotIdleCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskSlotCategoryAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("category", taskSlotCategoryAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskSlotIdleCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskSlotIdleCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskSlotIdleCount(cfg MetricConfig) metricApachedruidTaskSlotIdleCount {
	m := metricApachedruidTaskSlotIdleCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskSlotLazyCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task_slot.lazy.count metric with initial data.
func (m *metricApachedruidTaskSlotLazyCount) init() {
	m.data.SetName("apachedruid.task_slot.lazy.count")
	m.data.SetDescription("Number of total task slots in lazy marked Middle Managers and Indexers per emission period. This metric is only available if the `TaskSlotCountStatsMonitor` module is included.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskSlotLazyCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskSlotCategoryAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("category", taskSlotCategoryAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskSlotLazyCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskSlotLazyCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskSlotLazyCount(cfg MetricConfig) metricApachedruidTaskSlotLazyCount {
	m := metricApachedruidTaskSlotLazyCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskSlotTotalCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task_slot.total.count metric with initial data.
func (m *metricApachedruidTaskSlotTotalCount) init() {
	m.data.SetName("apachedruid.task_slot.total.count")
	m.data.SetDescription("Number of total task slots per emission period. This metric is only available if the `TaskSlotCountStatsMonitor` module is included.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskSlotTotalCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskSlotCategoryAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("category", taskSlotCategoryAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskSlotTotalCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskSlotTotalCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskSlotTotalCount(cfg MetricConfig) metricApachedruidTaskSlotTotalCount {
	m := metricApachedruidTaskSlotTotalCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTaskSlotUsedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.task_slot.used.count metric with initial data.
func (m *metricApachedruidTaskSlotUsedCount) init() {
	m.data.SetName("apachedruid.task_slot.used.count")
	m.data.SetDescription("Number of busy task slots per emission period. This metric is only available if the `TaskSlotCountStatsMonitor` module is included.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTaskSlotUsedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, taskSlotCategoryAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("category", taskSlotCategoryAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTaskSlotUsedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTaskSlotUsedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTaskSlotUsedCount(cfg MetricConfig) metricApachedruidTaskSlotUsedCount {
	m := metricApachedruidTaskSlotUsedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTierHistoricalCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.tier.historical.count metric with initial data.
func (m *metricApachedruidTierHistoricalCount) init() {
	m.data.SetName("apachedruid.tier.historical.count")
	m.data.SetDescription("Number of available historical nodes in each tier.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTierHistoricalCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, tierAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tier", tierAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTierHistoricalCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTierHistoricalCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTierHistoricalCount(cfg MetricConfig) metricApachedruidTierHistoricalCount {
	m := metricApachedruidTierHistoricalCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTierReplicationFactor struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.tier.replication.factor metric with initial data.
func (m *metricApachedruidTierReplicationFactor) init() {
	m.data.SetName("apachedruid.tier.replication.factor")
	m.data.SetDescription("Configured maximum replication factor in each tier.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTierReplicationFactor) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, tierAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tier", tierAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTierReplicationFactor) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTierReplicationFactor) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTierReplicationFactor(cfg MetricConfig) metricApachedruidTierReplicationFactor {
	m := metricApachedruidTierReplicationFactor{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTierRequiredCapacity struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.tier.required.capacity metric with initial data.
func (m *metricApachedruidTierRequiredCapacity) init() {
	m.data.SetName("apachedruid.tier.required.capacity")
	m.data.SetDescription("Total capacity in bytes required in each tier.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTierRequiredCapacity) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, tierAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tier", tierAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTierRequiredCapacity) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTierRequiredCapacity) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTierRequiredCapacity(cfg MetricConfig) metricApachedruidTierRequiredCapacity {
	m := metricApachedruidTierRequiredCapacity{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTierTotalCapacity struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.tier.total.capacity metric with initial data.
func (m *metricApachedruidTierTotalCapacity) init() {
	m.data.SetName("apachedruid.tier.total.capacity")
	m.data.SetDescription("Total capacity in bytes available in each tier.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidTierTotalCapacity) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, tierAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("tier", tierAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTierTotalCapacity) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTierTotalCapacity) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTierTotalCapacity(cfg MetricConfig) metricApachedruidTierTotalCapacity {
	m := metricApachedruidTierTotalCapacity{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidWorkerTaskFailedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.worker.task.failed.count metric with initial data.
func (m *metricApachedruidWorkerTaskFailedCount) init() {
	m.data.SetName("apachedruid.worker.task.failed.count")
	m.data.SetDescription("Number of failed tasks run on the reporting worker per emission period. This metric is only available if the `WorkerTaskCountStatsMonitor` module is included, and is only supported for Middle Manager nodes.")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidWorkerTaskFailedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, workerCategoryAttributeValue string, workerVersionAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("category", workerCategoryAttributeValue)
	dp.Attributes().PutStr("worker_version", workerVersionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidWorkerTaskFailedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidWorkerTaskFailedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidWorkerTaskFailedCount(cfg MetricConfig) metricApachedruidWorkerTaskFailedCount {
	m := metricApachedruidWorkerTaskFailedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidWorkerTaskSuccessCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.worker.task.success.count metric with initial data.
func (m *metricApachedruidWorkerTaskSuccessCount) init() {
	m.data.SetName("apachedruid.worker.task.success.count")
	m.data.SetDescription("Number of successful tasks run on the reporting worker per emission period. This metric is only available if the `WorkerTaskCountStatsMonitor` module is included, and is only supported for Middle Manager nodes.")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidWorkerTaskSuccessCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, workerCategoryAttributeValue string, workerVersionAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("category", workerCategoryAttributeValue)
	dp.Attributes().PutStr("worker_version", workerVersionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidWorkerTaskSuccessCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidWorkerTaskSuccessCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidWorkerTaskSuccessCount(cfg MetricConfig) metricApachedruidWorkerTaskSuccessCount {
	m := metricApachedruidWorkerTaskSuccessCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidWorkerTaskSlotIdleCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.worker.task_slot.idle.count metric with initial data.
func (m *metricApachedruidWorkerTaskSlotIdleCount) init() {
	m.data.SetName("apachedruid.worker.task_slot.idle.count")
	m.data.SetDescription("Number of idle task slots on the reporting worker per emission period. This metric is only available if the `WorkerTaskCountStatsMonitor` module is included, and is only supported for Middle Manager nodes.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidWorkerTaskSlotIdleCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, workerCategoryAttributeValue string, workerVersionAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("category", workerCategoryAttributeValue)
	dp.Attributes().PutStr("worker_version", workerVersionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidWorkerTaskSlotIdleCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidWorkerTaskSlotIdleCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidWorkerTaskSlotIdleCount(cfg MetricConfig) metricApachedruidWorkerTaskSlotIdleCount {
	m := metricApachedruidWorkerTaskSlotIdleCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidWorkerTaskSlotTotalCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.worker.task_slot.total.count metric with initial data.
func (m *metricApachedruidWorkerTaskSlotTotalCount) init() {
	m.data.SetName("apachedruid.worker.task_slot.total.count")
	m.data.SetDescription("Number of total task slots on the reporting worker per emission period. This metric is only available if the `WorkerTaskCountStatsMonitor` module is included.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidWorkerTaskSlotTotalCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, workerCategoryAttributeValue string, workerVersionAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("category", workerCategoryAttributeValue)
	dp.Attributes().PutStr("worker_version", workerVersionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidWorkerTaskSlotTotalCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidWorkerTaskSlotTotalCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidWorkerTaskSlotTotalCount(cfg MetricConfig) metricApachedruidWorkerTaskSlotTotalCount {
	m := metricApachedruidWorkerTaskSlotTotalCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidWorkerTaskSlotUsedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.worker.task_slot.used.count metric with initial data.
func (m *metricApachedruidWorkerTaskSlotUsedCount) init() {
	m.data.SetName("apachedruid.worker.task_slot.used.count")
	m.data.SetDescription("Number of busy task slots on the reporting worker per emission period. This metric is only available if the `WorkerTaskCountStatsMonitor` module is included.")
	m.data.SetUnit("{slots}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidWorkerTaskSlotUsedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, workerCategoryAttributeValue string, workerVersionAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("category", workerCategoryAttributeValue)
	dp.Attributes().PutStr("worker_version", workerVersionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidWorkerTaskSlotUsedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidWorkerTaskSlotUsedCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidWorkerTaskSlotUsedCount(cfg MetricConfig) metricApachedruidWorkerTaskSlotUsedCount {
	m := metricApachedruidWorkerTaskSlotUsedCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidZkConnected struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.zk.connected metric with initial data.
func (m *metricApachedruidZkConnected) init() {
	m.data.SetName("apachedruid.zk.connected")
	m.data.SetDescription("Indicator of connection status. `1` for connected, `0` for disconnected. Emitted once per monitor period.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidZkConnected) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidZkConnected) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidZkConnected) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidZkConnected(cfg MetricConfig) metricApachedruidZkConnected {
	m := metricApachedruidZkConnected{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidZkReconnectTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.zk.reconnect.time metric with initial data.
func (m *metricApachedruidZkReconnectTime) init() {
	m.data.SetName("apachedruid.zk.reconnect.time")
	m.data.SetDescription("Amount of time, in milliseconds, that a server was disconnected from ZooKeeper before reconnecting. Emitted on reconnection. Not emitted if connection to ZooKeeper is permanently lost, because in this case, there is no reconnection.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
}

func (m *metricApachedruidZkReconnectTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidZkReconnectTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidZkReconnectTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidZkReconnectTime(cfg MetricConfig) metricApachedruidZkReconnectTime {
	m := metricApachedruidZkReconnectTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user config.
type MetricsBuilder struct {
	config                                                       MetricsBuilderConfig // config of the metrics builder.
	startTime                                                    pcommon.Timestamp    // start time that will be applied to all recorded data points.
	metricsCapacity                                              int                  // maximum observed number of metrics per resource.
	metricsBuffer                                                pmetric.Metrics      // accumulates metrics data before emitting.
	buildInfo                                                    component.BuildInfo  // contains version information.
	metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis
	metricApachedruidCompactTaskCount                            metricApachedruidCompactTaskCount
	metricApachedruidCompactTaskAvailableSlotCount               metricApachedruidCompactTaskAvailableSlotCount
	metricApachedruidCompactTaskMaxSlotCount                     metricApachedruidCompactTaskMaxSlotCount
	metricApachedruidCoordinatorGlobalTime                       metricApachedruidCoordinatorGlobalTime
	metricApachedruidCoordinatorTime                             metricApachedruidCoordinatorTime
	metricApachedruidIngestBytesReceived                         metricApachedruidIngestBytesReceived
	metricApachedruidIngestCount                                 metricApachedruidIngestCount
	metricApachedruidIngestEventsBuffered                        metricApachedruidIngestEventsBuffered
	metricApachedruidIngestEventsDuplicate                       metricApachedruidIngestEventsDuplicate
	metricApachedruidIngestEventsMessageGap                      metricApachedruidIngestEventsMessageGap
	metricApachedruidIngestEventsProcessed                       metricApachedruidIngestEventsProcessed
	metricApachedruidIngestEventsProcessedWithError              metricApachedruidIngestEventsProcessedWithError
	metricApachedruidIngestEventsThrownAway                      metricApachedruidIngestEventsThrownAway
	metricApachedruidIngestEventsUnparseable                     metricApachedruidIngestEventsUnparseable
	metricApachedruidIngestHandoffCount                          metricApachedruidIngestHandoffCount
	metricApachedruidIngestHandoffFailed                         metricApachedruidIngestHandoffFailed
	metricApachedruidIngestHandoffTime                           metricApachedruidIngestHandoffTime
	metricApachedruidIngestInputBytes                            metricApachedruidIngestInputBytes
	metricApachedruidIngestKafkaAvgLag                           metricApachedruidIngestKafkaAvgLag
	metricApachedruidIngestKafkaLag                              metricApachedruidIngestKafkaLag
	metricApachedruidIngestKafkaMaxLag                           metricApachedruidIngestKafkaMaxLag
	metricApachedruidIngestKafkaPartitionLag                     metricApachedruidIngestKafkaPartitionLag
	metricApachedruidIngestKinesisAvgLagTime                     metricApachedruidIngestKinesisAvgLagTime
	metricApachedruidIngestKinesisLagTime                        metricApachedruidIngestKinesisLagTime
	metricApachedruidIngestKinesisMaxLagTime                     metricApachedruidIngestKinesisMaxLagTime
	metricApachedruidIngestKinesisPartitionLagTime               metricApachedruidIngestKinesisPartitionLagTime
	metricApachedruidIngestMergeCPU                              metricApachedruidIngestMergeCPU
	metricApachedruidIngestMergeTime                             metricApachedruidIngestMergeTime
	metricApachedruidIngestNoticesQueueSize                      metricApachedruidIngestNoticesQueueSize
	metricApachedruidIngestNoticesTime                           metricApachedruidIngestNoticesTime
	metricApachedruidIngestPauseTime                             metricApachedruidIngestPauseTime
	metricApachedruidIngestPersistsBackPressure                  metricApachedruidIngestPersistsBackPressure
	metricApachedruidIngestPersistsCount                         metricApachedruidIngestPersistsCount
	metricApachedruidIngestPersistsCPU                           metricApachedruidIngestPersistsCPU
	metricApachedruidIngestPersistsFailed                        metricApachedruidIngestPersistsFailed
	metricApachedruidIngestPersistsTime                          metricApachedruidIngestPersistsTime
	metricApachedruidIngestRowsOutput                            metricApachedruidIngestRowsOutput
	metricApachedruidIngestSegmentsCount                         metricApachedruidIngestSegmentsCount
	metricApachedruidIngestShuffleBytes                          metricApachedruidIngestShuffleBytes
	metricApachedruidIngestShuffleRequests                       metricApachedruidIngestShuffleRequests
	metricApachedruidIngestSinkCount                             metricApachedruidIngestSinkCount
	metricApachedruidIngestTombstonesCount                       metricApachedruidIngestTombstonesCount
	metricApachedruidIntervalCompactedCount                      metricApachedruidIntervalCompactedCount
	metricApachedruidIntervalSkipCompactCount                    metricApachedruidIntervalSkipCompactCount
	metricApachedruidIntervalWaitCompactCount                    metricApachedruidIntervalWaitCompactCount
	metricApachedruidJettyNumOpenConnections                     metricApachedruidJettyNumOpenConnections
	metricApachedruidJettyThreadPoolBusy                         metricApachedruidJettyThreadPoolBusy
	metricApachedruidJettyThreadPoolIdle                         metricApachedruidJettyThreadPoolIdle
	metricApachedruidJettyThreadPoolIsLowOnThreads               metricApachedruidJettyThreadPoolIsLowOnThreads
	metricApachedruidJettyThreadPoolMax                          metricApachedruidJettyThreadPoolMax
	metricApachedruidJettyThreadPoolMin                          metricApachedruidJettyThreadPoolMin
	metricApachedruidJettyThreadPoolQueueSize                    metricApachedruidJettyThreadPoolQueueSize
	metricApachedruidJettyThreadPoolTotal                        metricApachedruidJettyThreadPoolTotal
	metricApachedruidJvmBufferpoolCapacity                       metricApachedruidJvmBufferpoolCapacity
	metricApachedruidJvmBufferpoolCount                          metricApachedruidJvmBufferpoolCount
	metricApachedruidJvmBufferpoolUsed                           metricApachedruidJvmBufferpoolUsed
	metricApachedruidJvmGcCount                                  metricApachedruidJvmGcCount
	metricApachedruidJvmGcCPU                                    metricApachedruidJvmGcCPU
	metricApachedruidJvmMemCommitted                             metricApachedruidJvmMemCommitted
	metricApachedruidJvmMemInit                                  metricApachedruidJvmMemInit
	metricApachedruidJvmMemMax                                   metricApachedruidJvmMemMax
	metricApachedruidJvmMemUsed                                  metricApachedruidJvmMemUsed
	metricApachedruidJvmPoolCommitted                            metricApachedruidJvmPoolCommitted
	metricApachedruidJvmPoolInit                                 metricApachedruidJvmPoolInit
	metricApachedruidJvmPoolMax                                  metricApachedruidJvmPoolMax
	metricApachedruidJvmPoolUsed                                 metricApachedruidJvmPoolUsed
	metricApachedruidKillPendingSegmentsCount                    metricApachedruidKillPendingSegmentsCount
	metricApachedruidKillTaskCount                               metricApachedruidKillTaskCount
	metricApachedruidKillTaskAvailableSlotCount                  metricApachedruidKillTaskAvailableSlotCount
	metricApachedruidKillTaskMaxSlotCount                        metricApachedruidKillTaskMaxSlotCount
	metricApachedruidMergeBufferPendingRequests                  metricApachedruidMergeBufferPendingRequests
	metricApachedruidMetadataKillAuditCount                      metricApachedruidMetadataKillAuditCount
	metricApachedruidMetadataKillCompactionCount                 metricApachedruidMetadataKillCompactionCount
	metricApachedruidMetadataKillDatasourceCount                 metricApachedruidMetadataKillDatasourceCount
	metricApachedruidMetadataKillRuleCount                       metricApachedruidMetadataKillRuleCount
	metricApachedruidMetadataKillSupervisorCount                 metricApachedruidMetadataKillSupervisorCount
	metricApachedruidMetadatacacheInitTime                       metricApachedruidMetadatacacheInitTime
	metricApachedruidMetadatacacheRefreshCount                   metricApachedruidMetadatacacheRefreshCount
	metricApachedruidMetadatacacheRefreshTime                    metricApachedruidMetadatacacheRefreshTime
	metricApachedruidQueryByteLimitExceededCount                 metricApachedruidQueryByteLimitExceededCount
	metricApachedruidQueryBytes                                  metricApachedruidQueryBytes
	metricApachedruidQueryCacheDeltaAverageBytes                 metricApachedruidQueryCacheDeltaAverageBytes
	metricApachedruidQueryCacheDeltaErrors                       metricApachedruidQueryCacheDeltaErrors
	metricApachedruidQueryCacheDeltaEvictions                    metricApachedruidQueryCacheDeltaEvictions
	metricApachedruidQueryCacheDeltaHitRate                      metricApachedruidQueryCacheDeltaHitRate
	metricApachedruidQueryCacheDeltaHits                         metricApachedruidQueryCacheDeltaHits
	metricApachedruidQueryCacheDeltaMisses                       metricApachedruidQueryCacheDeltaMisses
	metricApachedruidQueryCacheDeltaNumEntries                   metricApachedruidQueryCacheDeltaNumEntries
	metricApachedruidQueryCacheDeltaPutError                     metricApachedruidQueryCacheDeltaPutError
	metricApachedruidQueryCacheDeltaPutOk                        metricApachedruidQueryCacheDeltaPutOk
	metricApachedruidQueryCacheDeltaPutOversized                 metricApachedruidQueryCacheDeltaPutOversized
	metricApachedruidQueryCacheDeltaSizeBytes                    metricApachedruidQueryCacheDeltaSizeBytes
	metricApachedruidQueryCacheDeltaTimeouts                     metricApachedruidQueryCacheDeltaTimeouts
	metricApachedruidQueryCacheMemcachedDelta                    metricApachedruidQueryCacheMemcachedDelta
	metricApachedruidQueryCacheMemcachedTotal                    metricApachedruidQueryCacheMemcachedTotal
	metricApachedruidQueryCacheTotalAverageBytes                 metricApachedruidQueryCacheTotalAverageBytes
	metricApachedruidQueryCacheTotalErrors                       metricApachedruidQueryCacheTotalErrors
	metricApachedruidQueryCacheTotalEvictions                    metricApachedruidQueryCacheTotalEvictions
	metricApachedruidQueryCacheTotalHitRate                      metricApachedruidQueryCacheTotalHitRate
	metricApachedruidQueryCacheTotalHits                         metricApachedruidQueryCacheTotalHits
	metricApachedruidQueryCacheTotalMisses                       metricApachedruidQueryCacheTotalMisses
	metricApachedruidQueryCacheTotalNumEntries                   metricApachedruidQueryCacheTotalNumEntries
	metricApachedruidQueryCacheTotalPutError                     metricApachedruidQueryCacheTotalPutError
	metricApachedruidQueryCacheTotalPutOk                        metricApachedruidQueryCacheTotalPutOk
	metricApachedruidQueryCacheTotalPutOversized                 metricApachedruidQueryCacheTotalPutOversized
	metricApachedruidQueryCacheTotalSizeBytes                    metricApachedruidQueryCacheTotalSizeBytes
	metricApachedruidQueryCacheTotalTimeouts                     metricApachedruidQueryCacheTotalTimeouts
	metricApachedruidQueryCount                                  metricApachedruidQueryCount
	metricApachedruidQueryCPUTime                                metricApachedruidQueryCPUTime
	metricApachedruidQueryFailedCount                            metricApachedruidQueryFailedCount
	metricApachedruidQueryInterruptedCount                       metricApachedruidQueryInterruptedCount
	metricApachedruidQueryNodeBackpressure                       metricApachedruidQueryNodeBackpressure
	metricApachedruidQueryNodeBytes                              metricApachedruidQueryNodeBytes
	metricApachedruidQueryNodeTime                               metricApachedruidQueryNodeTime
	metricApachedruidQueryNodeTtfb                               metricApachedruidQueryNodeTtfb
	metricApachedruidQueryPriority                               metricApachedruidQueryPriority
	metricApachedruidQueryRowLimitExceededCount                  metricApachedruidQueryRowLimitExceededCount
	metricApachedruidQuerySegmentTime                            metricApachedruidQuerySegmentTime
	metricApachedruidQuerySegmentAndCacheTime                    metricApachedruidQuerySegmentAndCacheTime
	metricApachedruidQuerySegmentsCount                          metricApachedruidQuerySegmentsCount
	metricApachedruidQuerySuccessCount                           metricApachedruidQuerySuccessCount
	metricApachedruidQueryTime                                   metricApachedruidQueryTime
	metricApachedruidQueryTimeoutCount                           metricApachedruidQueryTimeoutCount
	metricApachedruidQueryWaitTime                               metricApachedruidQueryWaitTime
	metricApachedruidSegmentAddedBytes                           metricApachedruidSegmentAddedBytes
	metricApachedruidSegmentAssignSkippedCount                   metricApachedruidSegmentAssignSkippedCount
	metricApachedruidSegmentAssignedCount                        metricApachedruidSegmentAssignedCount
	metricApachedruidSegmentCompactedBytes                       metricApachedruidSegmentCompactedBytes
	metricApachedruidSegmentCompactedCount                       metricApachedruidSegmentCompactedCount
	metricApachedruidSegmentCount                                metricApachedruidSegmentCount
	metricApachedruidSegmentDeletedCount                         metricApachedruidSegmentDeletedCount
	metricApachedruidSegmentDropQueueCount                       metricApachedruidSegmentDropQueueCount
	metricApachedruidSegmentDropSkippedCount                     metricApachedruidSegmentDropSkippedCount
	metricApachedruidSegmentDroppedCount                         metricApachedruidSegmentDroppedCount
	metricApachedruidSegmentLoadQueueAssigned                    metricApachedruidSegmentLoadQueueAssigned
	metricApachedruidSegmentLoadQueueCancelled                   metricApachedruidSegmentLoadQueueCancelled
	metricApachedruidSegmentLoadQueueCount                       metricApachedruidSegmentLoadQueueCount
	metricApachedruidSegmentLoadQueueFailed                      metricApachedruidSegmentLoadQueueFailed
	metricApachedruidSegmentLoadQueueSize                        metricApachedruidSegmentLoadQueueSize
	metricApachedruidSegmentLoadQueueSuccess                     metricApachedruidSegmentLoadQueueSuccess
	metricApachedruidSegmentMax                                  metricApachedruidSegmentMax
	metricApachedruidSegmentMoveSkippedCount                     metricApachedruidSegmentMoveSkippedCount
	metricApachedruidSegmentMovedBytes                           metricApachedruidSegmentMovedBytes
	metricApachedruidSegmentMovedCount                           metricApachedruidSegmentMovedCount
	metricApachedruidSegmentNukedBytes                           metricApachedruidSegmentNukedBytes
	metricApachedruidSegmentOverShadowedCount                    metricApachedruidSegmentOverShadowedCount
	metricApachedruidSegmentPendingDelete                        metricApachedruidSegmentPendingDelete
	metricApachedruidSegmentRowCountAvg                          metricApachedruidSegmentRowCountAvg
	metricApachedruidSegmentRowCountRangeCount                   metricApachedruidSegmentRowCountRangeCount
	metricApachedruidSegmentScanActive                           metricApachedruidSegmentScanActive
	metricApachedruidSegmentScanPending                          metricApachedruidSegmentScanPending
	metricApachedruidSegmentSize                                 metricApachedruidSegmentSize
	metricApachedruidSegmentSkipCompactBytes                     metricApachedruidSegmentSkipCompactBytes
	metricApachedruidSegmentSkipCompactCount                     metricApachedruidSegmentSkipCompactCount
	metricApachedruidSegmentUnavailableCount                     metricApachedruidSegmentUnavailableCount
	metricApachedruidSegmentUnderReplicatedCount                 metricApachedruidSegmentUnderReplicatedCount
	metricApachedruidSegmentUnneededCount                        metricApachedruidSegmentUnneededCount
	metricApachedruidSegmentUsed                                 metricApachedruidSegmentUsed
	metricApachedruidSegmentUsedPercent                          metricApachedruidSegmentUsedPercent
	metricApachedruidSegmentWaitCompactBytes                     metricApachedruidSegmentWaitCompactBytes
	metricApachedruidSegmentWaitCompactCount                     metricApachedruidSegmentWaitCompactCount
	metricApachedruidServerviewInitTime                          metricApachedruidServerviewInitTime
	metricApachedruidServerviewSyncHealthy                       metricApachedruidServerviewSyncHealthy
	metricApachedruidServerviewSyncUnstableTime                  metricApachedruidServerviewSyncUnstableTime
	metricApachedruidSQLQueryBytes                               metricApachedruidSQLQueryBytes
	metricApachedruidSQLQueryPlanningTimeMs                      metricApachedruidSQLQueryPlanningTimeMs
	metricApachedruidSQLQueryTime                                metricApachedruidSQLQueryTime
	metricApachedruidSubqueryByteLimitCount                      metricApachedruidSubqueryByteLimitCount
	metricApachedruidSubqueryFallbackCount                       metricApachedruidSubqueryFallbackCount
	metricApachedruidSubqueryFallbackInsufficientTypeCount       metricApachedruidSubqueryFallbackInsufficientTypeCount
	metricApachedruidSubqueryFallbackUnknownReasonCount          metricApachedruidSubqueryFallbackUnknownReasonCount
	metricApachedruidSubqueryRowLimitCount                       metricApachedruidSubqueryRowLimitCount
	metricApachedruidSysCPU                                      metricApachedruidSysCPU
	metricApachedruidSysDiskQueue                                metricApachedruidSysDiskQueue
	metricApachedruidSysDiskReadCount                            metricApachedruidSysDiskReadCount
	metricApachedruidSysDiskReadSize                             metricApachedruidSysDiskReadSize
	metricApachedruidSysDiskTransferTime                         metricApachedruidSysDiskTransferTime
	metricApachedruidSysDiskWriteCount                           metricApachedruidSysDiskWriteCount
	metricApachedruidSysDiskWriteSize                            metricApachedruidSysDiskWriteSize
	metricApachedruidSysFsFilesCount                             metricApachedruidSysFsFilesCount
	metricApachedruidSysFsFilesFree                              metricApachedruidSysFsFilesFree
	metricApachedruidSysFsMax                                    metricApachedruidSysFsMax
	metricApachedruidSysFsUsed                                   metricApachedruidSysFsUsed
	metricApachedruidSysLa1                                      metricApachedruidSysLa1
	metricApachedruidSysLa15                                     metricApachedruidSysLa15
	metricApachedruidSysLa5                                      metricApachedruidSysLa5
	metricApachedruidSysMemFree                                  metricApachedruidSysMemFree
	metricApachedruidSysMemMax                                   metricApachedruidSysMemMax
	metricApachedruidSysMemUsed                                  metricApachedruidSysMemUsed
	metricApachedruidSysNetReadDropped                           metricApachedruidSysNetReadDropped
	metricApachedruidSysNetReadErrors                            metricApachedruidSysNetReadErrors
	metricApachedruidSysNetReadPackets                           metricApachedruidSysNetReadPackets
	metricApachedruidSysNetReadSize                              metricApachedruidSysNetReadSize
	metricApachedruidSysNetWriteCollisions                       metricApachedruidSysNetWriteCollisions
	metricApachedruidSysNetWriteErrors                           metricApachedruidSysNetWriteErrors
	metricApachedruidSysNetWritePackets                          metricApachedruidSysNetWritePackets
	metricApachedruidSysNetWriteSize                             metricApachedruidSysNetWriteSize
	metricApachedruidSysStorageUsed                              metricApachedruidSysStorageUsed
	metricApachedruidSysSwapFree                                 metricApachedruidSysSwapFree
	metricApachedruidSysSwapMax                                  metricApachedruidSysSwapMax
	metricApachedruidSysSwapPageIn                               metricApachedruidSysSwapPageIn
	metricApachedruidSysSwapPageOut                              metricApachedruidSysSwapPageOut
	metricApachedruidSysTcpv4ActiveOpens                         metricApachedruidSysTcpv4ActiveOpens
	metricApachedruidSysTcpv4AttemptFails                        metricApachedruidSysTcpv4AttemptFails
	metricApachedruidSysTcpv4EstabResets                         metricApachedruidSysTcpv4EstabResets
	metricApachedruidSysTcpv4InErrs                              metricApachedruidSysTcpv4InErrs
	metricApachedruidSysTcpv4InSegs                              metricApachedruidSysTcpv4InSegs
	metricApachedruidSysTcpv4OutRsts                             metricApachedruidSysTcpv4OutRsts
	metricApachedruidSysTcpv4OutSegs                             metricApachedruidSysTcpv4OutSegs
	metricApachedruidSysTcpv4PassiveOpens                        metricApachedruidSysTcpv4PassiveOpens
	metricApachedruidSysTcpv4RetransSegs                         metricApachedruidSysTcpv4RetransSegs
	metricApachedruidSysUptime                                   metricApachedruidSysUptime
	metricApachedruidTaskActionBatchAttempts                     metricApachedruidTaskActionBatchAttempts
	metricApachedruidTaskActionBatchQueueTime                    metricApachedruidTaskActionBatchQueueTime
	metricApachedruidTaskActionBatchRunTime                      metricApachedruidTaskActionBatchRunTime
	metricApachedruidTaskActionBatchSize                         metricApachedruidTaskActionBatchSize
	metricApachedruidTaskActionFailedCount                       metricApachedruidTaskActionFailedCount
	metricApachedruidTaskActionLogTime                           metricApachedruidTaskActionLogTime
	metricApachedruidTaskActionRunTime                           metricApachedruidTaskActionRunTime
	metricApachedruidTaskActionSuccessCount                      metricApachedruidTaskActionSuccessCount
	metricApachedruidTaskFailedCount                             metricApachedruidTaskFailedCount
	metricApachedruidTaskPendingCount                            metricApachedruidTaskPendingCount
	metricApachedruidTaskPendingTime                             metricApachedruidTaskPendingTime
	metricApachedruidTaskRunTime                                 metricApachedruidTaskRunTime
	metricApachedruidTaskRunningCount                            metricApachedruidTaskRunningCount
	metricApachedruidTaskSegmentAvailabilityWaitTime             metricApachedruidTaskSegmentAvailabilityWaitTime
	metricApachedruidTaskSuccessCount                            metricApachedruidTaskSuccessCount
	metricApachedruidTaskWaitingCount                            metricApachedruidTaskWaitingCount
	metricApachedruidTaskSlotBlacklistedCount                    metricApachedruidTaskSlotBlacklistedCount
	metricApachedruidTaskSlotIdleCount                           metricApachedruidTaskSlotIdleCount
	metricApachedruidTaskSlotLazyCount                           metricApachedruidTaskSlotLazyCount
	metricApachedruidTaskSlotTotalCount                          metricApachedruidTaskSlotTotalCount
	metricApachedruidTaskSlotUsedCount                           metricApachedruidTaskSlotUsedCount
	metricApachedruidTierHistoricalCount                         metricApachedruidTierHistoricalCount
	metricApachedruidTierReplicationFactor                       metricApachedruidTierReplicationFactor
	metricApachedruidTierRequiredCapacity                        metricApachedruidTierRequiredCapacity
	metricApachedruidTierTotalCapacity                           metricApachedruidTierTotalCapacity
	metricApachedruidWorkerTaskFailedCount                       metricApachedruidWorkerTaskFailedCount
	metricApachedruidWorkerTaskSuccessCount                      metricApachedruidWorkerTaskSuccessCount
	metricApachedruidWorkerTaskSlotIdleCount                     metricApachedruidWorkerTaskSlotIdleCount
	metricApachedruidWorkerTaskSlotTotalCount                    metricApachedruidWorkerTaskSlotTotalCount
	metricApachedruidWorkerTaskSlotUsedCount                     metricApachedruidWorkerTaskSlotUsedCount
	metricApachedruidZkConnected                                 metricApachedruidZkConnected
	metricApachedruidZkReconnectTime                             metricApachedruidZkReconnectTime
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func NewMetricsBuilder(mbc MetricsBuilderConfig, settings receiver.CreateSettings, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		config:        mbc,
		startTime:     pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer: pmetric.NewMetrics(),
		buildInfo:     settings.BuildInfo,
		metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis: newMetricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis(mbc.Metrics.ApachedruidCompactSegmentAnalyzerFetchAndProcessMillis),
		metricApachedruidCompactTaskCount:                            newMetricApachedruidCompactTaskCount(mbc.Metrics.ApachedruidCompactTaskCount),
		metricApachedruidCompactTaskAvailableSlotCount:               newMetricApachedruidCompactTaskAvailableSlotCount(mbc.Metrics.ApachedruidCompactTaskAvailableSlotCount),
		metricApachedruidCompactTaskMaxSlotCount:                     newMetricApachedruidCompactTaskMaxSlotCount(mbc.Metrics.ApachedruidCompactTaskMaxSlotCount),
		metricApachedruidCoordinatorGlobalTime:                       newMetricApachedruidCoordinatorGlobalTime(mbc.Metrics.ApachedruidCoordinatorGlobalTime),
		metricApachedruidCoordinatorTime:                             newMetricApachedruidCoordinatorTime(mbc.Metrics.ApachedruidCoordinatorTime),
		metricApachedruidIngestBytesReceived:                         newMetricApachedruidIngestBytesReceived(mbc.Metrics.ApachedruidIngestBytesReceived),
		metricApachedruidIngestCount:                                 newMetricApachedruidIngestCount(mbc.Metrics.ApachedruidIngestCount),
		metricApachedruidIngestEventsBuffered:                        newMetricApachedruidIngestEventsBuffered(mbc.Metrics.ApachedruidIngestEventsBuffered),
		metricApachedruidIngestEventsDuplicate:                       newMetricApachedruidIngestEventsDuplicate(mbc.Metrics.ApachedruidIngestEventsDuplicate),
		metricApachedruidIngestEventsMessageGap:                      newMetricApachedruidIngestEventsMessageGap(mbc.Metrics.ApachedruidIngestEventsMessageGap),
		metricApachedruidIngestEventsProcessed:                       newMetricApachedruidIngestEventsProcessed(mbc.Metrics.ApachedruidIngestEventsProcessed),
		metricApachedruidIngestEventsProcessedWithError:              newMetricApachedruidIngestEventsProcessedWithError(mbc.Metrics.ApachedruidIngestEventsProcessedWithError),
		metricApachedruidIngestEventsThrownAway:                      newMetricApachedruidIngestEventsThrownAway(mbc.Metrics.ApachedruidIngestEventsThrownAway),
		metricApachedruidIngestEventsUnparseable:                     newMetricApachedruidIngestEventsUnparseable(mbc.Metrics.ApachedruidIngestEventsUnparseable),
		metricApachedruidIngestHandoffCount:                          newMetricApachedruidIngestHandoffCount(mbc.Metrics.ApachedruidIngestHandoffCount),
		metricApachedruidIngestHandoffFailed:                         newMetricApachedruidIngestHandoffFailed(mbc.Metrics.ApachedruidIngestHandoffFailed),
		metricApachedruidIngestHandoffTime:                           newMetricApachedruidIngestHandoffTime(mbc.Metrics.ApachedruidIngestHandoffTime),
		metricApachedruidIngestInputBytes:                            newMetricApachedruidIngestInputBytes(mbc.Metrics.ApachedruidIngestInputBytes),
		metricApachedruidIngestKafkaAvgLag:                           newMetricApachedruidIngestKafkaAvgLag(mbc.Metrics.ApachedruidIngestKafkaAvgLag),
		metricApachedruidIngestKafkaLag:                              newMetricApachedruidIngestKafkaLag(mbc.Metrics.ApachedruidIngestKafkaLag),
		metricApachedruidIngestKafkaMaxLag:                           newMetricApachedruidIngestKafkaMaxLag(mbc.Metrics.ApachedruidIngestKafkaMaxLag),
		metricApachedruidIngestKafkaPartitionLag:                     newMetricApachedruidIngestKafkaPartitionLag(mbc.Metrics.ApachedruidIngestKafkaPartitionLag),
		metricApachedruidIngestKinesisAvgLagTime:                     newMetricApachedruidIngestKinesisAvgLagTime(mbc.Metrics.ApachedruidIngestKinesisAvgLagTime),
		metricApachedruidIngestKinesisLagTime:                        newMetricApachedruidIngestKinesisLagTime(mbc.Metrics.ApachedruidIngestKinesisLagTime),
		metricApachedruidIngestKinesisMaxLagTime:                     newMetricApachedruidIngestKinesisMaxLagTime(mbc.Metrics.ApachedruidIngestKinesisMaxLagTime),
		metricApachedruidIngestKinesisPartitionLagTime:               newMetricApachedruidIngestKinesisPartitionLagTime(mbc.Metrics.ApachedruidIngestKinesisPartitionLagTime),
		metricApachedruidIngestMergeCPU:                              newMetricApachedruidIngestMergeCPU(mbc.Metrics.ApachedruidIngestMergeCPU),
		metricApachedruidIngestMergeTime:                             newMetricApachedruidIngestMergeTime(mbc.Metrics.ApachedruidIngestMergeTime),
		metricApachedruidIngestNoticesQueueSize:                      newMetricApachedruidIngestNoticesQueueSize(mbc.Metrics.ApachedruidIngestNoticesQueueSize),
		metricApachedruidIngestNoticesTime:                           newMetricApachedruidIngestNoticesTime(mbc.Metrics.ApachedruidIngestNoticesTime),
		metricApachedruidIngestPauseTime:                             newMetricApachedruidIngestPauseTime(mbc.Metrics.ApachedruidIngestPauseTime),
		metricApachedruidIngestPersistsBackPressure:                  newMetricApachedruidIngestPersistsBackPressure(mbc.Metrics.ApachedruidIngestPersistsBackPressure),
		metricApachedruidIngestPersistsCount:                         newMetricApachedruidIngestPersistsCount(mbc.Metrics.ApachedruidIngestPersistsCount),
		metricApachedruidIngestPersistsCPU:                           newMetricApachedruidIngestPersistsCPU(mbc.Metrics.ApachedruidIngestPersistsCPU),
		metricApachedruidIngestPersistsFailed:                        newMetricApachedruidIngestPersistsFailed(mbc.Metrics.ApachedruidIngestPersistsFailed),
		metricApachedruidIngestPersistsTime:                          newMetricApachedruidIngestPersistsTime(mbc.Metrics.ApachedruidIngestPersistsTime),
		metricApachedruidIngestRowsOutput:                            newMetricApachedruidIngestRowsOutput(mbc.Metrics.ApachedruidIngestRowsOutput),
		metricApachedruidIngestSegmentsCount:                         newMetricApachedruidIngestSegmentsCount(mbc.Metrics.ApachedruidIngestSegmentsCount),
		metricApachedruidIngestShuffleBytes:                          newMetricApachedruidIngestShuffleBytes(mbc.Metrics.ApachedruidIngestShuffleBytes),
		metricApachedruidIngestShuffleRequests:                       newMetricApachedruidIngestShuffleRequests(mbc.Metrics.ApachedruidIngestShuffleRequests),
		metricApachedruidIngestSinkCount:                             newMetricApachedruidIngestSinkCount(mbc.Metrics.ApachedruidIngestSinkCount),
		metricApachedruidIngestTombstonesCount:                       newMetricApachedruidIngestTombstonesCount(mbc.Metrics.ApachedruidIngestTombstonesCount),
		metricApachedruidIntervalCompactedCount:                      newMetricApachedruidIntervalCompactedCount(mbc.Metrics.ApachedruidIntervalCompactedCount),
		metricApachedruidIntervalSkipCompactCount:                    newMetricApachedruidIntervalSkipCompactCount(mbc.Metrics.ApachedruidIntervalSkipCompactCount),
		metricApachedruidIntervalWaitCompactCount:                    newMetricApachedruidIntervalWaitCompactCount(mbc.Metrics.ApachedruidIntervalWaitCompactCount),
		metricApachedruidJettyNumOpenConnections:                     newMetricApachedruidJettyNumOpenConnections(mbc.Metrics.ApachedruidJettyNumOpenConnections),
		metricApachedruidJettyThreadPoolBusy:                         newMetricApachedruidJettyThreadPoolBusy(mbc.Metrics.ApachedruidJettyThreadPoolBusy),
		metricApachedruidJettyThreadPoolIdle:                         newMetricApachedruidJettyThreadPoolIdle(mbc.Metrics.ApachedruidJettyThreadPoolIdle),
		metricApachedruidJettyThreadPoolIsLowOnThreads:               newMetricApachedruidJettyThreadPoolIsLowOnThreads(mbc.Metrics.ApachedruidJettyThreadPoolIsLowOnThreads),
		metricApachedruidJettyThreadPoolMax:                          newMetricApachedruidJettyThreadPoolMax(mbc.Metrics.ApachedruidJettyThreadPoolMax),
		metricApachedruidJettyThreadPoolMin:                          newMetricApachedruidJettyThreadPoolMin(mbc.Metrics.ApachedruidJettyThreadPoolMin),
		metricApachedruidJettyThreadPoolQueueSize:                    newMetricApachedruidJettyThreadPoolQueueSize(mbc.Metrics.ApachedruidJettyThreadPoolQueueSize),
		metricApachedruidJettyThreadPoolTotal:                        newMetricApachedruidJettyThreadPoolTotal(mbc.Metrics.ApachedruidJettyThreadPoolTotal),
		metricApachedruidJvmBufferpoolCapacity:                       newMetricApachedruidJvmBufferpoolCapacity(mbc.Metrics.ApachedruidJvmBufferpoolCapacity),
		metricApachedruidJvmBufferpoolCount:                          newMetricApachedruidJvmBufferpoolCount(mbc.Metrics.ApachedruidJvmBufferpoolCount),
		metricApachedruidJvmBufferpoolUsed:                           newMetricApachedruidJvmBufferpoolUsed(mbc.Metrics.ApachedruidJvmBufferpoolUsed),
		metricApachedruidJvmGcCount:                                  newMetricApachedruidJvmGcCount(mbc.Metrics.ApachedruidJvmGcCount),
		metricApachedruidJvmGcCPU:                                    newMetricApachedruidJvmGcCPU(mbc.Metrics.ApachedruidJvmGcCPU),
		metricApachedruidJvmMemCommitted:                             newMetricApachedruidJvmMemCommitted(mbc.Metrics.ApachedruidJvmMemCommitted),
		metricApachedruidJvmMemInit:                                  newMetricApachedruidJvmMemInit(mbc.Metrics.ApachedruidJvmMemInit),
		metricApachedruidJvmMemMax:                                   newMetricApachedruidJvmMemMax(mbc.Metrics.ApachedruidJvmMemMax),
		metricApachedruidJvmMemUsed:                                  newMetricApachedruidJvmMemUsed(mbc.Metrics.ApachedruidJvmMemUsed),
		metricApachedruidJvmPoolCommitted:                            newMetricApachedruidJvmPoolCommitted(mbc.Metrics.ApachedruidJvmPoolCommitted),
		metricApachedruidJvmPoolInit:                                 newMetricApachedruidJvmPoolInit(mbc.Metrics.ApachedruidJvmPoolInit),
		metricApachedruidJvmPoolMax:                                  newMetricApachedruidJvmPoolMax(mbc.Metrics.ApachedruidJvmPoolMax),
		metricApachedruidJvmPoolUsed:                                 newMetricApachedruidJvmPoolUsed(mbc.Metrics.ApachedruidJvmPoolUsed),
		metricApachedruidKillPendingSegmentsCount:                    newMetricApachedruidKillPendingSegmentsCount(mbc.Metrics.ApachedruidKillPendingSegmentsCount),
		metricApachedruidKillTaskCount:                               newMetricApachedruidKillTaskCount(mbc.Metrics.ApachedruidKillTaskCount),
		metricApachedruidKillTaskAvailableSlotCount:                  newMetricApachedruidKillTaskAvailableSlotCount(mbc.Metrics.ApachedruidKillTaskAvailableSlotCount),
		metricApachedruidKillTaskMaxSlotCount:                        newMetricApachedruidKillTaskMaxSlotCount(mbc.Metrics.ApachedruidKillTaskMaxSlotCount),
		metricApachedruidMergeBufferPendingRequests:                  newMetricApachedruidMergeBufferPendingRequests(mbc.Metrics.ApachedruidMergeBufferPendingRequests),
		metricApachedruidMetadataKillAuditCount:                      newMetricApachedruidMetadataKillAuditCount(mbc.Metrics.ApachedruidMetadataKillAuditCount),
		metricApachedruidMetadataKillCompactionCount:                 newMetricApachedruidMetadataKillCompactionCount(mbc.Metrics.ApachedruidMetadataKillCompactionCount),
		metricApachedruidMetadataKillDatasourceCount:                 newMetricApachedruidMetadataKillDatasourceCount(mbc.Metrics.ApachedruidMetadataKillDatasourceCount),
		metricApachedruidMetadataKillRuleCount:                       newMetricApachedruidMetadataKillRuleCount(mbc.Metrics.ApachedruidMetadataKillRuleCount),
		metricApachedruidMetadataKillSupervisorCount:                 newMetricApachedruidMetadataKillSupervisorCount(mbc.Metrics.ApachedruidMetadataKillSupervisorCount),
		metricApachedruidMetadatacacheInitTime:                       newMetricApachedruidMetadatacacheInitTime(mbc.Metrics.ApachedruidMetadatacacheInitTime),
		metricApachedruidMetadatacacheRefreshCount:                   newMetricApachedruidMetadatacacheRefreshCount(mbc.Metrics.ApachedruidMetadatacacheRefreshCount),
		metricApachedruidMetadatacacheRefreshTime:                    newMetricApachedruidMetadatacacheRefreshTime(mbc.Metrics.ApachedruidMetadatacacheRefreshTime),
		metricApachedruidQueryByteLimitExceededCount:                 newMetricApachedruidQueryByteLimitExceededCount(mbc.Metrics.ApachedruidQueryByteLimitExceededCount),
		metricApachedruidQueryBytes:                                  newMetricApachedruidQueryBytes(mbc.Metrics.ApachedruidQueryBytes),
		metricApachedruidQueryCacheDeltaAverageBytes:                 newMetricApachedruidQueryCacheDeltaAverageBytes(mbc.Metrics.ApachedruidQueryCacheDeltaAverageBytes),
		metricApachedruidQueryCacheDeltaErrors:                       newMetricApachedruidQueryCacheDeltaErrors(mbc.Metrics.ApachedruidQueryCacheDeltaErrors),
		metricApachedruidQueryCacheDeltaEvictions:                    newMetricApachedruidQueryCacheDeltaEvictions(mbc.Metrics.ApachedruidQueryCacheDeltaEvictions),
		metricApachedruidQueryCacheDeltaHitRate:                      newMetricApachedruidQueryCacheDeltaHitRate(mbc.Metrics.ApachedruidQueryCacheDeltaHitRate),
		metricApachedruidQueryCacheDeltaHits:                         newMetricApachedruidQueryCacheDeltaHits(mbc.Metrics.ApachedruidQueryCacheDeltaHits),
		metricApachedruidQueryCacheDeltaMisses:                       newMetricApachedruidQueryCacheDeltaMisses(mbc.Metrics.ApachedruidQueryCacheDeltaMisses),
		metricApachedruidQueryCacheDeltaNumEntries:                   newMetricApachedruidQueryCacheDeltaNumEntries(mbc.Metrics.ApachedruidQueryCacheDeltaNumEntries),
		metricApachedruidQueryCacheDeltaPutError:                     newMetricApachedruidQueryCacheDeltaPutError(mbc.Metrics.ApachedruidQueryCacheDeltaPutError),
		metricApachedruidQueryCacheDeltaPutOk:                        newMetricApachedruidQueryCacheDeltaPutOk(mbc.Metrics.ApachedruidQueryCacheDeltaPutOk),
		metricApachedruidQueryCacheDeltaPutOversized:                 newMetricApachedruidQueryCacheDeltaPutOversized(mbc.Metrics.ApachedruidQueryCacheDeltaPutOversized),
		metricApachedruidQueryCacheDeltaSizeBytes:                    newMetricApachedruidQueryCacheDeltaSizeBytes(mbc.Metrics.ApachedruidQueryCacheDeltaSizeBytes),
		metricApachedruidQueryCacheDeltaTimeouts:                     newMetricApachedruidQueryCacheDeltaTimeouts(mbc.Metrics.ApachedruidQueryCacheDeltaTimeouts),
		metricApachedruidQueryCacheMemcachedDelta:                    newMetricApachedruidQueryCacheMemcachedDelta(mbc.Metrics.ApachedruidQueryCacheMemcachedDelta),
		metricApachedruidQueryCacheMemcachedTotal:                    newMetricApachedruidQueryCacheMemcachedTotal(mbc.Metrics.ApachedruidQueryCacheMemcachedTotal),
		metricApachedruidQueryCacheTotalAverageBytes:                 newMetricApachedruidQueryCacheTotalAverageBytes(mbc.Metrics.ApachedruidQueryCacheTotalAverageBytes),
		metricApachedruidQueryCacheTotalErrors:                       newMetricApachedruidQueryCacheTotalErrors(mbc.Metrics.ApachedruidQueryCacheTotalErrors),
		metricApachedruidQueryCacheTotalEvictions:                    newMetricApachedruidQueryCacheTotalEvictions(mbc.Metrics.ApachedruidQueryCacheTotalEvictions),
		metricApachedruidQueryCacheTotalHitRate:                      newMetricApachedruidQueryCacheTotalHitRate(mbc.Metrics.ApachedruidQueryCacheTotalHitRate),
		metricApachedruidQueryCacheTotalHits:                         newMetricApachedruidQueryCacheTotalHits(mbc.Metrics.ApachedruidQueryCacheTotalHits),
		metricApachedruidQueryCacheTotalMisses:                       newMetricApachedruidQueryCacheTotalMisses(mbc.Metrics.ApachedruidQueryCacheTotalMisses),
		metricApachedruidQueryCacheTotalNumEntries:                   newMetricApachedruidQueryCacheTotalNumEntries(mbc.Metrics.ApachedruidQueryCacheTotalNumEntries),
		metricApachedruidQueryCacheTotalPutError:                     newMetricApachedruidQueryCacheTotalPutError(mbc.Metrics.ApachedruidQueryCacheTotalPutError),
		metricApachedruidQueryCacheTotalPutOk:                        newMetricApachedruidQueryCacheTotalPutOk(mbc.Metrics.ApachedruidQueryCacheTotalPutOk),
		metricApachedruidQueryCacheTotalPutOversized:                 newMetricApachedruidQueryCacheTotalPutOversized(mbc.Metrics.ApachedruidQueryCacheTotalPutOversized),
		metricApachedruidQueryCacheTotalSizeBytes:                    newMetricApachedruidQueryCacheTotalSizeBytes(mbc.Metrics.ApachedruidQueryCacheTotalSizeBytes),
		metricApachedruidQueryCacheTotalTimeouts:                     newMetricApachedruidQueryCacheTotalTimeouts(mbc.Metrics.ApachedruidQueryCacheTotalTimeouts),
		metricApachedruidQueryCount:                                  newMetricApachedruidQueryCount(mbc.Metrics.ApachedruidQueryCount),
		metricApachedruidQueryCPUTime:                                newMetricApachedruidQueryCPUTime(mbc.Metrics.ApachedruidQueryCPUTime),
		metricApachedruidQueryFailedCount:                            newMetricApachedruidQueryFailedCount(mbc.Metrics.ApachedruidQueryFailedCount),
		metricApachedruidQueryInterruptedCount:                       newMetricApachedruidQueryInterruptedCount(mbc.Metrics.ApachedruidQueryInterruptedCount),
		metricApachedruidQueryNodeBackpressure:                       newMetricApachedruidQueryNodeBackpressure(mbc.Metrics.ApachedruidQueryNodeBackpressure),
		metricApachedruidQueryNodeBytes:                              newMetricApachedruidQueryNodeBytes(mbc.Metrics.ApachedruidQueryNodeBytes),
		metricApachedruidQueryNodeTime:                               newMetricApachedruidQueryNodeTime(mbc.Metrics.ApachedruidQueryNodeTime),
		metricApachedruidQueryNodeTtfb:                               newMetricApachedruidQueryNodeTtfb(mbc.Metrics.ApachedruidQueryNodeTtfb),
		metricApachedruidQueryPriority:                               newMetricApachedruidQueryPriority(mbc.Metrics.ApachedruidQueryPriority),
		metricApachedruidQueryRowLimitExceededCount:                  newMetricApachedruidQueryRowLimitExceededCount(mbc.Metrics.ApachedruidQueryRowLimitExceededCount),
		metricApachedruidQuerySegmentTime:                            newMetricApachedruidQuerySegmentTime(mbc.Metrics.ApachedruidQuerySegmentTime),
		metricApachedruidQuerySegmentAndCacheTime:                    newMetricApachedruidQuerySegmentAndCacheTime(mbc.Metrics.ApachedruidQuerySegmentAndCacheTime),
		metricApachedruidQuerySegmentsCount:                          newMetricApachedruidQuerySegmentsCount(mbc.Metrics.ApachedruidQuerySegmentsCount),
		metricApachedruidQuerySuccessCount:                           newMetricApachedruidQuerySuccessCount(mbc.Metrics.ApachedruidQuerySuccessCount),
		metricApachedruidQueryTime:                                   newMetricApachedruidQueryTime(mbc.Metrics.ApachedruidQueryTime),
		metricApachedruidQueryTimeoutCount:                           newMetricApachedruidQueryTimeoutCount(mbc.Metrics.ApachedruidQueryTimeoutCount),
		metricApachedruidQueryWaitTime:                               newMetricApachedruidQueryWaitTime(mbc.Metrics.ApachedruidQueryWaitTime),
		metricApachedruidSegmentAddedBytes:                           newMetricApachedruidSegmentAddedBytes(mbc.Metrics.ApachedruidSegmentAddedBytes),
		metricApachedruidSegmentAssignSkippedCount:                   newMetricApachedruidSegmentAssignSkippedCount(mbc.Metrics.ApachedruidSegmentAssignSkippedCount),
		metricApachedruidSegmentAssignedCount:                        newMetricApachedruidSegmentAssignedCount(mbc.Metrics.ApachedruidSegmentAssignedCount),
		metricApachedruidSegmentCompactedBytes:                       newMetricApachedruidSegmentCompactedBytes(mbc.Metrics.ApachedruidSegmentCompactedBytes),
		metricApachedruidSegmentCompactedCount:                       newMetricApachedruidSegmentCompactedCount(mbc.Metrics.ApachedruidSegmentCompactedCount),
		metricApachedruidSegmentCount:                                newMetricApachedruidSegmentCount(mbc.Metrics.ApachedruidSegmentCount),
		metricApachedruidSegmentDeletedCount:                         newMetricApachedruidSegmentDeletedCount(mbc.Metrics.ApachedruidSegmentDeletedCount),
		metricApachedruidSegmentDropQueueCount:                       newMetricApachedruidSegmentDropQueueCount(mbc.Metrics.ApachedruidSegmentDropQueueCount),
		metricApachedruidSegmentDropSkippedCount:                     newMetricApachedruidSegmentDropSkippedCount(mbc.Metrics.ApachedruidSegmentDropSkippedCount),
		metricApachedruidSegmentDroppedCount:                         newMetricApachedruidSegmentDroppedCount(mbc.Metrics.ApachedruidSegmentDroppedCount),
		metricApachedruidSegmentLoadQueueAssigned:                    newMetricApachedruidSegmentLoadQueueAssigned(mbc.Metrics.ApachedruidSegmentLoadQueueAssigned),
		metricApachedruidSegmentLoadQueueCancelled:                   newMetricApachedruidSegmentLoadQueueCancelled(mbc.Metrics.ApachedruidSegmentLoadQueueCancelled),
		metricApachedruidSegmentLoadQueueCount:                       newMetricApachedruidSegmentLoadQueueCount(mbc.Metrics.ApachedruidSegmentLoadQueueCount),
		metricApachedruidSegmentLoadQueueFailed:                      newMetricApachedruidSegmentLoadQueueFailed(mbc.Metrics.ApachedruidSegmentLoadQueueFailed),
		metricApachedruidSegmentLoadQueueSize:                        newMetricApachedruidSegmentLoadQueueSize(mbc.Metrics.ApachedruidSegmentLoadQueueSize),
		metricApachedruidSegmentLoadQueueSuccess:                     newMetricApachedruidSegmentLoadQueueSuccess(mbc.Metrics.ApachedruidSegmentLoadQueueSuccess),
		metricApachedruidSegmentMax:                                  newMetricApachedruidSegmentMax(mbc.Metrics.ApachedruidSegmentMax),
		metricApachedruidSegmentMoveSkippedCount:                     newMetricApachedruidSegmentMoveSkippedCount(mbc.Metrics.ApachedruidSegmentMoveSkippedCount),
		metricApachedruidSegmentMovedBytes:                           newMetricApachedruidSegmentMovedBytes(mbc.Metrics.ApachedruidSegmentMovedBytes),
		metricApachedruidSegmentMovedCount:                           newMetricApachedruidSegmentMovedCount(mbc.Metrics.ApachedruidSegmentMovedCount),
		metricApachedruidSegmentNukedBytes:                           newMetricApachedruidSegmentNukedBytes(mbc.Metrics.ApachedruidSegmentNukedBytes),
		metricApachedruidSegmentOverShadowedCount:                    newMetricApachedruidSegmentOverShadowedCount(mbc.Metrics.ApachedruidSegmentOverShadowedCount),
		metricApachedruidSegmentPendingDelete:                        newMetricApachedruidSegmentPendingDelete(mbc.Metrics.ApachedruidSegmentPendingDelete),
		metricApachedruidSegmentRowCountAvg:                          newMetricApachedruidSegmentRowCountAvg(mbc.Metrics.ApachedruidSegmentRowCountAvg),
		metricApachedruidSegmentRowCountRangeCount:                   newMetricApachedruidSegmentRowCountRangeCount(mbc.Metrics.ApachedruidSegmentRowCountRangeCount),
		metricApachedruidSegmentScanActive:                           newMetricApachedruidSegmentScanActive(mbc.Metrics.ApachedruidSegmentScanActive),
		metricApachedruidSegmentScanPending:                          newMetricApachedruidSegmentScanPending(mbc.Metrics.ApachedruidSegmentScanPending),
		metricApachedruidSegmentSize:                                 newMetricApachedruidSegmentSize(mbc.Metrics.ApachedruidSegmentSize),
		metricApachedruidSegmentSkipCompactBytes:                     newMetricApachedruidSegmentSkipCompactBytes(mbc.Metrics.ApachedruidSegmentSkipCompactBytes),
		metricApachedruidSegmentSkipCompactCount:                     newMetricApachedruidSegmentSkipCompactCount(mbc.Metrics.ApachedruidSegmentSkipCompactCount),
		metricApachedruidSegmentUnavailableCount:                     newMetricApachedruidSegmentUnavailableCount(mbc.Metrics.ApachedruidSegmentUnavailableCount),
		metricApachedruidSegmentUnderReplicatedCount:                 newMetricApachedruidSegmentUnderReplicatedCount(mbc.Metrics.ApachedruidSegmentUnderReplicatedCount),
		metricApachedruidSegmentUnneededCount:                        newMetricApachedruidSegmentUnneededCount(mbc.Metrics.ApachedruidSegmentUnneededCount),
		metricApachedruidSegmentUsed:                                 newMetricApachedruidSegmentUsed(mbc.Metrics.ApachedruidSegmentUsed),
		metricApachedruidSegmentUsedPercent:                          newMetricApachedruidSegmentUsedPercent(mbc.Metrics.ApachedruidSegmentUsedPercent),
		metricApachedruidSegmentWaitCompactBytes:                     newMetricApachedruidSegmentWaitCompactBytes(mbc.Metrics.ApachedruidSegmentWaitCompactBytes),
		metricApachedruidSegmentWaitCompactCount:                     newMetricApachedruidSegmentWaitCompactCount(mbc.Metrics.ApachedruidSegmentWaitCompactCount),
		metricApachedruidServerviewInitTime:                          newMetricApachedruidServerviewInitTime(mbc.Metrics.ApachedruidServerviewInitTime),
		metricApachedruidServerviewSyncHealthy:                       newMetricApachedruidServerviewSyncHealthy(mbc.Metrics.ApachedruidServerviewSyncHealthy),
		metricApachedruidServerviewSyncUnstableTime:                  newMetricApachedruidServerviewSyncUnstableTime(mbc.Metrics.ApachedruidServerviewSyncUnstableTime),
		metricApachedruidSQLQueryBytes:                               newMetricApachedruidSQLQueryBytes(mbc.Metrics.ApachedruidSQLQueryBytes),
		metricApachedruidSQLQueryPlanningTimeMs:                      newMetricApachedruidSQLQueryPlanningTimeMs(mbc.Metrics.ApachedruidSQLQueryPlanningTimeMs),
		metricApachedruidSQLQueryTime:                                newMetricApachedruidSQLQueryTime(mbc.Metrics.ApachedruidSQLQueryTime),
		metricApachedruidSubqueryByteLimitCount:                      newMetricApachedruidSubqueryByteLimitCount(mbc.Metrics.ApachedruidSubqueryByteLimitCount),
		metricApachedruidSubqueryFallbackCount:                       newMetricApachedruidSubqueryFallbackCount(mbc.Metrics.ApachedruidSubqueryFallbackCount),
		metricApachedruidSubqueryFallbackInsufficientTypeCount:       newMetricApachedruidSubqueryFallbackInsufficientTypeCount(mbc.Metrics.ApachedruidSubqueryFallbackInsufficientTypeCount),
		metricApachedruidSubqueryFallbackUnknownReasonCount:          newMetricApachedruidSubqueryFallbackUnknownReasonCount(mbc.Metrics.ApachedruidSubqueryFallbackUnknownReasonCount),
		metricApachedruidSubqueryRowLimitCount:                       newMetricApachedruidSubqueryRowLimitCount(mbc.Metrics.ApachedruidSubqueryRowLimitCount),
		metricApachedruidSysCPU:                                      newMetricApachedruidSysCPU(mbc.Metrics.ApachedruidSysCPU),
		metricApachedruidSysDiskQueue:                                newMetricApachedruidSysDiskQueue(mbc.Metrics.ApachedruidSysDiskQueue),
		metricApachedruidSysDiskReadCount:                            newMetricApachedruidSysDiskReadCount(mbc.Metrics.ApachedruidSysDiskReadCount),
		metricApachedruidSysDiskReadSize:                             newMetricApachedruidSysDiskReadSize(mbc.Metrics.ApachedruidSysDiskReadSize),
		metricApachedruidSysDiskTransferTime:                         newMetricApachedruidSysDiskTransferTime(mbc.Metrics.ApachedruidSysDiskTransferTime),
		metricApachedruidSysDiskWriteCount:                           newMetricApachedruidSysDiskWriteCount(mbc.Metrics.ApachedruidSysDiskWriteCount),
		metricApachedruidSysDiskWriteSize:                            newMetricApachedruidSysDiskWriteSize(mbc.Metrics.ApachedruidSysDiskWriteSize),
		metricApachedruidSysFsFilesCount:                             newMetricApachedruidSysFsFilesCount(mbc.Metrics.ApachedruidSysFsFilesCount),
		metricApachedruidSysFsFilesFree:                              newMetricApachedruidSysFsFilesFree(mbc.Metrics.ApachedruidSysFsFilesFree),
		metricApachedruidSysFsMax:                                    newMetricApachedruidSysFsMax(mbc.Metrics.ApachedruidSysFsMax),
		metricApachedruidSysFsUsed:                                   newMetricApachedruidSysFsUsed(mbc.Metrics.ApachedruidSysFsUsed),
		metricApachedruidSysLa1:                                      newMetricApachedruidSysLa1(mbc.Metrics.ApachedruidSysLa1),
		metricApachedruidSysLa15:                                     newMetricApachedruidSysLa15(mbc.Metrics.ApachedruidSysLa15),
		metricApachedruidSysLa5:                                      newMetricApachedruidSysLa5(mbc.Metrics.ApachedruidSysLa5),
		metricApachedruidSysMemFree:                                  newMetricApachedruidSysMemFree(mbc.Metrics.ApachedruidSysMemFree),
		metricApachedruidSysMemMax:                                   newMetricApachedruidSysMemMax(mbc.Metrics.ApachedruidSysMemMax),
		metricApachedruidSysMemUsed:                                  newMetricApachedruidSysMemUsed(mbc.Metrics.ApachedruidSysMemUsed),
		metricApachedruidSysNetReadDropped:                           newMetricApachedruidSysNetReadDropped(mbc.Metrics.ApachedruidSysNetReadDropped),
		metricApachedruidSysNetReadErrors:                            newMetricApachedruidSysNetReadErrors(mbc.Metrics.ApachedruidSysNetReadErrors),
		metricApachedruidSysNetReadPackets:                           newMetricApachedruidSysNetReadPackets(mbc.Metrics.ApachedruidSysNetReadPackets),
		metricApachedruidSysNetReadSize:                              newMetricApachedruidSysNetReadSize(mbc.Metrics.ApachedruidSysNetReadSize),
		metricApachedruidSysNetWriteCollisions:                       newMetricApachedruidSysNetWriteCollisions(mbc.Metrics.ApachedruidSysNetWriteCollisions),
		metricApachedruidSysNetWriteErrors:                           newMetricApachedruidSysNetWriteErrors(mbc.Metrics.ApachedruidSysNetWriteErrors),
		metricApachedruidSysNetWritePackets:                          newMetricApachedruidSysNetWritePackets(mbc.Metrics.ApachedruidSysNetWritePackets),
		metricApachedruidSysNetWriteSize:                             newMetricApachedruidSysNetWriteSize(mbc.Metrics.ApachedruidSysNetWriteSize),
		metricApachedruidSysStorageUsed:                              newMetricApachedruidSysStorageUsed(mbc.Metrics.ApachedruidSysStorageUsed),
		metricApachedruidSysSwapFree:                                 newMetricApachedruidSysSwapFree(mbc.Metrics.ApachedruidSysSwapFree),
		metricApachedruidSysSwapMax:                                  newMetricApachedruidSysSwapMax(mbc.Metrics.ApachedruidSysSwapMax),
		metricApachedruidSysSwapPageIn:                               newMetricApachedruidSysSwapPageIn(mbc.Metrics.ApachedruidSysSwapPageIn),
		metricApachedruidSysSwapPageOut:                              newMetricApachedruidSysSwapPageOut(mbc.Metrics.ApachedruidSysSwapPageOut),
		metricApachedruidSysTcpv4ActiveOpens:                         newMetricApachedruidSysTcpv4ActiveOpens(mbc.Metrics.ApachedruidSysTcpv4ActiveOpens),
		metricApachedruidSysTcpv4AttemptFails:                        newMetricApachedruidSysTcpv4AttemptFails(mbc.Metrics.ApachedruidSysTcpv4AttemptFails),
		metricApachedruidSysTcpv4EstabResets:                         newMetricApachedruidSysTcpv4EstabResets(mbc.Metrics.ApachedruidSysTcpv4EstabResets),
		metricApachedruidSysTcpv4InErrs:                              newMetricApachedruidSysTcpv4InErrs(mbc.Metrics.ApachedruidSysTcpv4InErrs),
		metricApachedruidSysTcpv4InSegs:                              newMetricApachedruidSysTcpv4InSegs(mbc.Metrics.ApachedruidSysTcpv4InSegs),
		metricApachedruidSysTcpv4OutRsts:                             newMetricApachedruidSysTcpv4OutRsts(mbc.Metrics.ApachedruidSysTcpv4OutRsts),
		metricApachedruidSysTcpv4OutSegs:                             newMetricApachedruidSysTcpv4OutSegs(mbc.Metrics.ApachedruidSysTcpv4OutSegs),
		metricApachedruidSysTcpv4PassiveOpens:                        newMetricApachedruidSysTcpv4PassiveOpens(mbc.Metrics.ApachedruidSysTcpv4PassiveOpens),
		metricApachedruidSysTcpv4RetransSegs:                         newMetricApachedruidSysTcpv4RetransSegs(mbc.Metrics.ApachedruidSysTcpv4RetransSegs),
		metricApachedruidSysUptime:                                   newMetricApachedruidSysUptime(mbc.Metrics.ApachedruidSysUptime),
		metricApachedruidTaskActionBatchAttempts:                     newMetricApachedruidTaskActionBatchAttempts(mbc.Metrics.ApachedruidTaskActionBatchAttempts),
		metricApachedruidTaskActionBatchQueueTime:                    newMetricApachedruidTaskActionBatchQueueTime(mbc.Metrics.ApachedruidTaskActionBatchQueueTime),
		metricApachedruidTaskActionBatchRunTime:                      newMetricApachedruidTaskActionBatchRunTime(mbc.Metrics.ApachedruidTaskActionBatchRunTime),
		metricApachedruidTaskActionBatchSize:                         newMetricApachedruidTaskActionBatchSize(mbc.Metrics.ApachedruidTaskActionBatchSize),
		metricApachedruidTaskActionFailedCount:                       newMetricApachedruidTaskActionFailedCount(mbc.Metrics.ApachedruidTaskActionFailedCount),
		metricApachedruidTaskActionLogTime:                           newMetricApachedruidTaskActionLogTime(mbc.Metrics.ApachedruidTaskActionLogTime),
		metricApachedruidTaskActionRunTime:                           newMetricApachedruidTaskActionRunTime(mbc.Metrics.ApachedruidTaskActionRunTime),
		metricApachedruidTaskActionSuccessCount:                      newMetricApachedruidTaskActionSuccessCount(mbc.Metrics.ApachedruidTaskActionSuccessCount),
		metricApachedruidTaskFailedCount:                             newMetricApachedruidTaskFailedCount(mbc.Metrics.ApachedruidTaskFailedCount),
		metricApachedruidTaskPendingCount:                            newMetricApachedruidTaskPendingCount(mbc.Metrics.ApachedruidTaskPendingCount),
		metricApachedruidTaskPendingTime:                             newMetricApachedruidTaskPendingTime(mbc.Metrics.ApachedruidTaskPendingTime),
		metricApachedruidTaskRunTime:                                 newMetricApachedruidTaskRunTime(mbc.Metrics.ApachedruidTaskRunTime),
		metricApachedruidTaskRunningCount:                            newMetricApachedruidTaskRunningCount(mbc.Metrics.ApachedruidTaskRunningCount),
		metricApachedruidTaskSegmentAvailabilityWaitTime:             newMetricApachedruidTaskSegmentAvailabilityWaitTime(mbc.Metrics.ApachedruidTaskSegmentAvailabilityWaitTime),
		metricApachedruidTaskSuccessCount:                            newMetricApachedruidTaskSuccessCount(mbc.Metrics.ApachedruidTaskSuccessCount),
		metricApachedruidTaskWaitingCount:                            newMetricApachedruidTaskWaitingCount(mbc.Metrics.ApachedruidTaskWaitingCount),
		metricApachedruidTaskSlotBlacklistedCount:                    newMetricApachedruidTaskSlotBlacklistedCount(mbc.Metrics.ApachedruidTaskSlotBlacklistedCount),
		metricApachedruidTaskSlotIdleCount:                           newMetricApachedruidTaskSlotIdleCount(mbc.Metrics.ApachedruidTaskSlotIdleCount),
		metricApachedruidTaskSlotLazyCount:                           newMetricApachedruidTaskSlotLazyCount(mbc.Metrics.ApachedruidTaskSlotLazyCount),
		metricApachedruidTaskSlotTotalCount:                          newMetricApachedruidTaskSlotTotalCount(mbc.Metrics.ApachedruidTaskSlotTotalCount),
		metricApachedruidTaskSlotUsedCount:                           newMetricApachedruidTaskSlotUsedCount(mbc.Metrics.ApachedruidTaskSlotUsedCount),
		metricApachedruidTierHistoricalCount:                         newMetricApachedruidTierHistoricalCount(mbc.Metrics.ApachedruidTierHistoricalCount),
		metricApachedruidTierReplicationFactor:                       newMetricApachedruidTierReplicationFactor(mbc.Metrics.ApachedruidTierReplicationFactor),
		metricApachedruidTierRequiredCapacity:                        newMetricApachedruidTierRequiredCapacity(mbc.Metrics.ApachedruidTierRequiredCapacity),
		metricApachedruidTierTotalCapacity:                           newMetricApachedruidTierTotalCapacity(mbc.Metrics.ApachedruidTierTotalCapacity),
		metricApachedruidWorkerTaskFailedCount:                       newMetricApachedruidWorkerTaskFailedCount(mbc.Metrics.ApachedruidWorkerTaskFailedCount),
		metricApachedruidWorkerTaskSuccessCount:                      newMetricApachedruidWorkerTaskSuccessCount(mbc.Metrics.ApachedruidWorkerTaskSuccessCount),
		metricApachedruidWorkerTaskSlotIdleCount:                     newMetricApachedruidWorkerTaskSlotIdleCount(mbc.Metrics.ApachedruidWorkerTaskSlotIdleCount),
		metricApachedruidWorkerTaskSlotTotalCount:                    newMetricApachedruidWorkerTaskSlotTotalCount(mbc.Metrics.ApachedruidWorkerTaskSlotTotalCount),
		metricApachedruidWorkerTaskSlotUsedCount:                     newMetricApachedruidWorkerTaskSlotUsedCount(mbc.Metrics.ApachedruidWorkerTaskSlotUsedCount),
		metricApachedruidZkConnected:                                 newMetricApachedruidZkConnected(mbc.Metrics.ApachedruidZkConnected),
		metricApachedruidZkReconnectTime:                             newMetricApachedruidZkReconnectTime(mbc.Metrics.ApachedruidZkReconnectTime),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// NewResourceBuilder returns a new resource builder that should be used to build a resource associated with for the emitted metrics.
func (mb *MetricsBuilder) NewResourceBuilder() *ResourceBuilder {
	return NewResourceBuilder(mb.config.ResourceAttributes)
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption func(pmetric.ResourceMetrics)

// WithResource sets the provided resource on the emitted ResourceMetrics.
// It's recommended to use ResourceBuilder to create the resource.
func WithResource(res pcommon.Resource) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		res.CopyTo(rm.Resource())
	}
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	}
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(rmo ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName("github.com/open-telemetry/opentelemetry-collector-contrib/receiver/apachedruidreceiver")
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis.emit(ils.Metrics())
	mb.metricApachedruidCompactTaskCount.emit(ils.Metrics())
	mb.metricApachedruidCompactTaskAvailableSlotCount.emit(ils.Metrics())
	mb.metricApachedruidCompactTaskMaxSlotCount.emit(ils.Metrics())
	mb.metricApachedruidCoordinatorGlobalTime.emit(ils.Metrics())
	mb.metricApachedruidCoordinatorTime.emit(ils.Metrics())
	mb.metricApachedruidIngestBytesReceived.emit(ils.Metrics())
	mb.metricApachedruidIngestCount.emit(ils.Metrics())
	mb.metricApachedruidIngestEventsBuffered.emit(ils.Metrics())
	mb.metricApachedruidIngestEventsDuplicate.emit(ils.Metrics())
	mb.metricApachedruidIngestEventsMessageGap.emit(ils.Metrics())
	mb.metricApachedruidIngestEventsProcessed.emit(ils.Metrics())
	mb.metricApachedruidIngestEventsProcessedWithError.emit(ils.Metrics())
	mb.metricApachedruidIngestEventsThrownAway.emit(ils.Metrics())
	mb.metricApachedruidIngestEventsUnparseable.emit(ils.Metrics())
	mb.metricApachedruidIngestHandoffCount.emit(ils.Metrics())
	mb.metricApachedruidIngestHandoffFailed.emit(ils.Metrics())
	mb.metricApachedruidIngestHandoffTime.emit(ils.Metrics())
	mb.metricApachedruidIngestInputBytes.emit(ils.Metrics())
	mb.metricApachedruidIngestKafkaAvgLag.emit(ils.Metrics())
	mb.metricApachedruidIngestKafkaLag.emit(ils.Metrics())
	mb.metricApachedruidIngestKafkaMaxLag.emit(ils.Metrics())
	mb.metricApachedruidIngestKafkaPartitionLag.emit(ils.Metrics())
	mb.metricApachedruidIngestKinesisAvgLagTime.emit(ils.Metrics())
	mb.metricApachedruidIngestKinesisLagTime.emit(ils.Metrics())
	mb.metricApachedruidIngestKinesisMaxLagTime.emit(ils.Metrics())
	mb.metricApachedruidIngestKinesisPartitionLagTime.emit(ils.Metrics())
	mb.metricApachedruidIngestMergeCPU.emit(ils.Metrics())
	mb.metricApachedruidIngestMergeTime.emit(ils.Metrics())
	mb.metricApachedruidIngestNoticesQueueSize.emit(ils.Metrics())
	mb.metricApachedruidIngestNoticesTime.emit(ils.Metrics())
	mb.metricApachedruidIngestPauseTime.emit(ils.Metrics())
	mb.metricApachedruidIngestPersistsBackPressure.emit(ils.Metrics())
	mb.metricApachedruidIngestPersistsCount.emit(ils.Metrics())
	mb.metricApachedruidIngestPersistsCPU.emit(ils.Metrics())
	mb.metricApachedruidIngestPersistsFailed.emit(ils.Metrics())
	mb.metricApachedruidIngestPersistsTime.emit(ils.Metrics())
	mb.metricApachedruidIngestRowsOutput.emit(ils.Metrics())
	mb.metricApachedruidIngestSegmentsCount.emit(ils.Metrics())
	mb.metricApachedruidIngestShuffleBytes.emit(ils.Metrics())
	mb.metricApachedruidIngestShuffleRequests.emit(ils.Metrics())
	mb.metricApachedruidIngestSinkCount.emit(ils.Metrics())
	mb.metricApachedruidIngestTombstonesCount.emit(ils.Metrics())
	mb.metricApachedruidIntervalCompactedCount.emit(ils.Metrics())
	mb.metricApachedruidIntervalSkipCompactCount.emit(ils.Metrics())
	mb.metricApachedruidIntervalWaitCompactCount.emit(ils.Metrics())
	mb.metricApachedruidJettyNumOpenConnections.emit(ils.Metrics())
	mb.metricApachedruidJettyThreadPoolBusy.emit(ils.Metrics())
	mb.metricApachedruidJettyThreadPoolIdle.emit(ils.Metrics())
	mb.metricApachedruidJettyThreadPoolIsLowOnThreads.emit(ils.Metrics())
	mb.metricApachedruidJettyThreadPoolMax.emit(ils.Metrics())
	mb.metricApachedruidJettyThreadPoolMin.emit(ils.Metrics())
	mb.metricApachedruidJettyThreadPoolQueueSize.emit(ils.Metrics())
	mb.metricApachedruidJettyThreadPoolTotal.emit(ils.Metrics())
	mb.metricApachedruidJvmBufferpoolCapacity.emit(ils.Metrics())
	mb.metricApachedruidJvmBufferpoolCount.emit(ils.Metrics())
	mb.metricApachedruidJvmBufferpoolUsed.emit(ils.Metrics())
	mb.metricApachedruidJvmGcCount.emit(ils.Metrics())
	mb.metricApachedruidJvmGcCPU.emit(ils.Metrics())
	mb.metricApachedruidJvmMemCommitted.emit(ils.Metrics())
	mb.metricApachedruidJvmMemInit.emit(ils.Metrics())
	mb.metricApachedruidJvmMemMax.emit(ils.Metrics())
	mb.metricApachedruidJvmMemUsed.emit(ils.Metrics())
	mb.metricApachedruidJvmPoolCommitted.emit(ils.Metrics())
	mb.metricApachedruidJvmPoolInit.emit(ils.Metrics())
	mb.metricApachedruidJvmPoolMax.emit(ils.Metrics())
	mb.metricApachedruidJvmPoolUsed.emit(ils.Metrics())
	mb.metricApachedruidKillPendingSegmentsCount.emit(ils.Metrics())
	mb.metricApachedruidKillTaskCount.emit(ils.Metrics())
	mb.metricApachedruidKillTaskAvailableSlotCount.emit(ils.Metrics())
	mb.metricApachedruidKillTaskMaxSlotCount.emit(ils.Metrics())
	mb.metricApachedruidMergeBufferPendingRequests.emit(ils.Metrics())
	mb.metricApachedruidMetadataKillAuditCount.emit(ils.Metrics())
	mb.metricApachedruidMetadataKillCompactionCount.emit(ils.Metrics())
	mb.metricApachedruidMetadataKillDatasourceCount.emit(ils.Metrics())
	mb.metricApachedruidMetadataKillRuleCount.emit(ils.Metrics())
	mb.metricApachedruidMetadataKillSupervisorCount.emit(ils.Metrics())
	mb.metricApachedruidMetadatacacheInitTime.emit(ils.Metrics())
	mb.metricApachedruidMetadatacacheRefreshCount.emit(ils.Metrics())
	mb.metricApachedruidMetadatacacheRefreshTime.emit(ils.Metrics())
	mb.metricApachedruidQueryByteLimitExceededCount.emit(ils.Metrics())
	mb.metricApachedruidQueryBytes.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaAverageBytes.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaErrors.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaEvictions.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaHitRate.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaHits.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaMisses.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaNumEntries.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaPutError.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaPutOk.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaPutOversized.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaSizeBytes.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheDeltaTimeouts.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheMemcachedDelta.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheMemcachedTotal.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalAverageBytes.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalErrors.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalEvictions.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalHitRate.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalHits.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalMisses.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalNumEntries.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalPutError.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalPutOk.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalPutOversized.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalSizeBytes.emit(ils.Metrics())
	mb.metricApachedruidQueryCacheTotalTimeouts.emit(ils.Metrics())
	mb.metricApachedruidQueryCount.emit(ils.Metrics())
	mb.metricApachedruidQueryCPUTime.emit(ils.Metrics())
	mb.metricApachedruidQueryFailedCount.emit(ils.Metrics())
	mb.metricApachedruidQueryInterruptedCount.emit(ils.Metrics())
	mb.metricApachedruidQueryNodeBackpressure.emit(ils.Metrics())
	mb.metricApachedruidQueryNodeBytes.emit(ils.Metrics())
	mb.metricApachedruidQueryNodeTime.emit(ils.Metrics())
	mb.metricApachedruidQueryNodeTtfb.emit(ils.Metrics())
	mb.metricApachedruidQueryPriority.emit(ils.Metrics())
	mb.metricApachedruidQueryRowLimitExceededCount.emit(ils.Metrics())
	mb.metricApachedruidQuerySegmentTime.emit(ils.Metrics())
	mb.metricApachedruidQuerySegmentAndCacheTime.emit(ils.Metrics())
	mb.metricApachedruidQuerySegmentsCount.emit(ils.Metrics())
	mb.metricApachedruidQuerySuccessCount.emit(ils.Metrics())
	mb.metricApachedruidQueryTime.emit(ils.Metrics())
	mb.metricApachedruidQueryTimeoutCount.emit(ils.Metrics())
	mb.metricApachedruidQueryWaitTime.emit(ils.Metrics())
	mb.metricApachedruidSegmentAddedBytes.emit(ils.Metrics())
	mb.metricApachedruidSegmentAssignSkippedCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentAssignedCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentCompactedBytes.emit(ils.Metrics())
	mb.metricApachedruidSegmentCompactedCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentDeletedCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentDropQueueCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentDropSkippedCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentDroppedCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentLoadQueueAssigned.emit(ils.Metrics())
	mb.metricApachedruidSegmentLoadQueueCancelled.emit(ils.Metrics())
	mb.metricApachedruidSegmentLoadQueueCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentLoadQueueFailed.emit(ils.Metrics())
	mb.metricApachedruidSegmentLoadQueueSize.emit(ils.Metrics())
	mb.metricApachedruidSegmentLoadQueueSuccess.emit(ils.Metrics())
	mb.metricApachedruidSegmentMax.emit(ils.Metrics())
	mb.metricApachedruidSegmentMoveSkippedCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentMovedBytes.emit(ils.Metrics())
	mb.metricApachedruidSegmentMovedCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentNukedBytes.emit(ils.Metrics())
	mb.metricApachedruidSegmentOverShadowedCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentPendingDelete.emit(ils.Metrics())
	mb.metricApachedruidSegmentRowCountAvg.emit(ils.Metrics())
	mb.metricApachedruidSegmentRowCountRangeCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentScanActive.emit(ils.Metrics())
	mb.metricApachedruidSegmentScanPending.emit(ils.Metrics())
	mb.metricApachedruidSegmentSize.emit(ils.Metrics())
	mb.metricApachedruidSegmentSkipCompactBytes.emit(ils.Metrics())
	mb.metricApachedruidSegmentSkipCompactCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentUnavailableCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentUnderReplicatedCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentUnneededCount.emit(ils.Metrics())
	mb.metricApachedruidSegmentUsed.emit(ils.Metrics())
	mb.metricApachedruidSegmentUsedPercent.emit(ils.Metrics())
	mb.metricApachedruidSegmentWaitCompactBytes.emit(ils.Metrics())
	mb.metricApachedruidSegmentWaitCompactCount.emit(ils.Metrics())
	mb.metricApachedruidServerviewInitTime.emit(ils.Metrics())
	mb.metricApachedruidServerviewSyncHealthy.emit(ils.Metrics())
	mb.metricApachedruidServerviewSyncUnstableTime.emit(ils.Metrics())
	mb.metricApachedruidSQLQueryBytes.emit(ils.Metrics())
	mb.metricApachedruidSQLQueryPlanningTimeMs.emit(ils.Metrics())
	mb.metricApachedruidSQLQueryTime.emit(ils.Metrics())
	mb.metricApachedruidSubqueryByteLimitCount.emit(ils.Metrics())
	mb.metricApachedruidSubqueryFallbackCount.emit(ils.Metrics())
	mb.metricApachedruidSubqueryFallbackInsufficientTypeCount.emit(ils.Metrics())
	mb.metricApachedruidSubqueryFallbackUnknownReasonCount.emit(ils.Metrics())
	mb.metricApachedruidSubqueryRowLimitCount.emit(ils.Metrics())
	mb.metricApachedruidSysCPU.emit(ils.Metrics())
	mb.metricApachedruidSysDiskQueue.emit(ils.Metrics())
	mb.metricApachedruidSysDiskReadCount.emit(ils.Metrics())
	mb.metricApachedruidSysDiskReadSize.emit(ils.Metrics())
	mb.metricApachedruidSysDiskTransferTime.emit(ils.Metrics())
	mb.metricApachedruidSysDiskWriteCount.emit(ils.Metrics())
	mb.metricApachedruidSysDiskWriteSize.emit(ils.Metrics())
	mb.metricApachedruidSysFsFilesCount.emit(ils.Metrics())
	mb.metricApachedruidSysFsFilesFree.emit(ils.Metrics())
	mb.metricApachedruidSysFsMax.emit(ils.Metrics())
	mb.metricApachedruidSysFsUsed.emit(ils.Metrics())
	mb.metricApachedruidSysLa1.emit(ils.Metrics())
	mb.metricApachedruidSysLa15.emit(ils.Metrics())
	mb.metricApachedruidSysLa5.emit(ils.Metrics())
	mb.metricApachedruidSysMemFree.emit(ils.Metrics())
	mb.metricApachedruidSysMemMax.emit(ils.Metrics())
	mb.metricApachedruidSysMemUsed.emit(ils.Metrics())
	mb.metricApachedruidSysNetReadDropped.emit(ils.Metrics())
	mb.metricApachedruidSysNetReadErrors.emit(ils.Metrics())
	mb.metricApachedruidSysNetReadPackets.emit(ils.Metrics())
	mb.metricApachedruidSysNetReadSize.emit(ils.Metrics())
	mb.metricApachedruidSysNetWriteCollisions.emit(ils.Metrics())
	mb.metricApachedruidSysNetWriteErrors.emit(ils.Metrics())
	mb.metricApachedruidSysNetWritePackets.emit(ils.Metrics())
	mb.metricApachedruidSysNetWriteSize.emit(ils.Metrics())
	mb.metricApachedruidSysStorageUsed.emit(ils.Metrics())
	mb.metricApachedruidSysSwapFree.emit(ils.Metrics())
	mb.metricApachedruidSysSwapMax.emit(ils.Metrics())
	mb.metricApachedruidSysSwapPageIn.emit(ils.Metrics())
	mb.metricApachedruidSysSwapPageOut.emit(ils.Metrics())
	mb.metricApachedruidSysTcpv4ActiveOpens.emit(ils.Metrics())
	mb.metricApachedruidSysTcpv4AttemptFails.emit(ils.Metrics())
	mb.metricApachedruidSysTcpv4EstabResets.emit(ils.Metrics())
	mb.metricApachedruidSysTcpv4InErrs.emit(ils.Metrics())
	mb.metricApachedruidSysTcpv4InSegs.emit(ils.Metrics())
	mb.metricApachedruidSysTcpv4OutRsts.emit(ils.Metrics())
	mb.metricApachedruidSysTcpv4OutSegs.emit(ils.Metrics())
	mb.metricApachedruidSysTcpv4PassiveOpens.emit(ils.Metrics())
	mb.metricApachedruidSysTcpv4RetransSegs.emit(ils.Metrics())
	mb.metricApachedruidSysUptime.emit(ils.Metrics())
	mb.metricApachedruidTaskActionBatchAttempts.emit(ils.Metrics())
	mb.metricApachedruidTaskActionBatchQueueTime.emit(ils.Metrics())
	mb.metricApachedruidTaskActionBatchRunTime.emit(ils.Metrics())
	mb.metricApachedruidTaskActionBatchSize.emit(ils.Metrics())
	mb.metricApachedruidTaskActionFailedCount.emit(ils.Metrics())
	mb.metricApachedruidTaskActionLogTime.emit(ils.Metrics())
	mb.metricApachedruidTaskActionRunTime.emit(ils.Metrics())
	mb.metricApachedruidTaskActionSuccessCount.emit(ils.Metrics())
	mb.metricApachedruidTaskFailedCount.emit(ils.Metrics())
	mb.metricApachedruidTaskPendingCount.emit(ils.Metrics())
	mb.metricApachedruidTaskPendingTime.emit(ils.Metrics())
	mb.metricApachedruidTaskRunTime.emit(ils.Metrics())
	mb.metricApachedruidTaskRunningCount.emit(ils.Metrics())
	mb.metricApachedruidTaskSegmentAvailabilityWaitTime.emit(ils.Metrics())
	mb.metricApachedruidTaskSuccessCount.emit(ils.Metrics())
	mb.metricApachedruidTaskWaitingCount.emit(ils.Metrics())
	mb.metricApachedruidTaskSlotBlacklistedCount.emit(ils.Metrics())
	mb.metricApachedruidTaskSlotIdleCount.emit(ils.Metrics())
	mb.metricApachedruidTaskSlotLazyCount.emit(ils.Metrics())
	mb.metricApachedruidTaskSlotTotalCount.emit(ils.Metrics())
	mb.metricApachedruidTaskSlotUsedCount.emit(ils.Metrics())
	mb.metricApachedruidTierHistoricalCount.emit(ils.Metrics())
	mb.metricApachedruidTierReplicationFactor.emit(ils.Metrics())
	mb.metricApachedruidTierRequiredCapacity.emit(ils.Metrics())
	mb.metricApachedruidTierTotalCapacity.emit(ils.Metrics())
	mb.metricApachedruidWorkerTaskFailedCount.emit(ils.Metrics())
	mb.metricApachedruidWorkerTaskSuccessCount.emit(ils.Metrics())
	mb.metricApachedruidWorkerTaskSlotIdleCount.emit(ils.Metrics())
	mb.metricApachedruidWorkerTaskSlotTotalCount.emit(ils.Metrics())
	mb.metricApachedruidWorkerTaskSlotUsedCount.emit(ils.Metrics())
	mb.metricApachedruidZkConnected.emit(ils.Metrics())
	mb.metricApachedruidZkReconnectTime.emit(ils.Metrics())

	for _, op := range rmo {
		op(rm)
	}
	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user config, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(rmo ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(rmo...)
	metrics := mb.metricsBuffer
	mb.metricsBuffer = pmetric.NewMetrics()
	return metrics
}

// RecordApachedruidCompactSegmentAnalyzerFetchAndProcessMillisDataPoint adds a data point to apachedruid.compact.segment_analyzer.fetch_and_process_millis metric.
func (mb *MetricsBuilder) RecordApachedruidCompactSegmentAnalyzerFetchAndProcessMillisDataPoint(ts pcommon.Timestamp, val int64, compactTaskTypeAttributeValue string, compactDataSourceAttributeValue string, compactGroupIDAttributeValue string, compactTagsAttributeValue string, compactTaskIDAttributeValue string) {
	mb.metricApachedruidCompactSegmentAnalyzerFetchAndProcessMillis.recordDataPoint(mb.startTime, ts, val, compactTaskTypeAttributeValue, compactDataSourceAttributeValue, compactGroupIDAttributeValue, compactTagsAttributeValue, compactTaskIDAttributeValue)
}

// RecordApachedruidCompactTaskCountDataPoint adds a data point to apachedruid.compact.task.count metric.
func (mb *MetricsBuilder) RecordApachedruidCompactTaskCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidCompactTaskCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidCompactTaskAvailableSlotCountDataPoint adds a data point to apachedruid.compact_task.available_slot.count metric.
func (mb *MetricsBuilder) RecordApachedruidCompactTaskAvailableSlotCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidCompactTaskAvailableSlotCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidCompactTaskMaxSlotCountDataPoint adds a data point to apachedruid.compact_task.max_slot.count metric.
func (mb *MetricsBuilder) RecordApachedruidCompactTaskMaxSlotCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidCompactTaskMaxSlotCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidCoordinatorGlobalTimeDataPoint adds a data point to apachedruid.coordinator.global.time metric.
func (mb *MetricsBuilder) RecordApachedruidCoordinatorGlobalTimeDataPoint(ts pcommon.Timestamp, val int64, coordinatorDutyGroupAttributeValue string) {
	mb.metricApachedruidCoordinatorGlobalTime.recordDataPoint(mb.startTime, ts, val, coordinatorDutyGroupAttributeValue)
}

// RecordApachedruidCoordinatorTimeDataPoint adds a data point to apachedruid.coordinator.time metric.
func (mb *MetricsBuilder) RecordApachedruidCoordinatorTimeDataPoint(ts pcommon.Timestamp, val int64, coordinatorDutyAttributeValue string) {
	mb.metricApachedruidCoordinatorTime.recordDataPoint(mb.startTime, ts, val, coordinatorDutyAttributeValue)
}

// RecordApachedruidIngestBytesReceivedDataPoint adds a data point to apachedruid.ingest.bytes.received metric.
func (mb *MetricsBuilder) RecordApachedruidIngestBytesReceivedDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestTaskIDAttributeValue string, ingestDataSourceAttributeValue string, ingestServiceNameAttributeValue string) {
	mb.metricApachedruidIngestBytesReceived.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestTaskIDAttributeValue, ingestDataSourceAttributeValue, ingestServiceNameAttributeValue)
}

// RecordApachedruidIngestCountDataPoint adds a data point to apachedruid.ingest.count metric.
func (mb *MetricsBuilder) RecordApachedruidIngestCountDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string, ingestTaskIngestionModeAttributeValue string) {
	mb.metricApachedruidIngestCount.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue, ingestTaskIngestionModeAttributeValue)
}

// RecordApachedruidIngestEventsBufferedDataPoint adds a data point to apachedruid.ingest.events.buffered metric.
func (mb *MetricsBuilder) RecordApachedruidIngestEventsBufferedDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestServiceNameAttributeValue string, ingestBufferCapacityAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestEventsBuffered.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestServiceNameAttributeValue, ingestBufferCapacityAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestEventsDuplicateDataPoint adds a data point to apachedruid.ingest.events.duplicate metric.
func (mb *MetricsBuilder) RecordApachedruidIngestEventsDuplicateDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestEventsDuplicate.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestEventsMessageGapDataPoint adds a data point to apachedruid.ingest.events.message_gap metric.
func (mb *MetricsBuilder) RecordApachedruidIngestEventsMessageGapDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestEventsMessageGap.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestEventsProcessedDataPoint adds a data point to apachedruid.ingest.events.processed metric.
func (mb *MetricsBuilder) RecordApachedruidIngestEventsProcessedDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestEventsProcessed.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestEventsProcessedWithErrorDataPoint adds a data point to apachedruid.ingest.events.processed_with_error metric.
func (mb *MetricsBuilder) RecordApachedruidIngestEventsProcessedWithErrorDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestEventsProcessedWithError.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestEventsThrownAwayDataPoint adds a data point to apachedruid.ingest.events.thrown_away metric.
func (mb *MetricsBuilder) RecordApachedruidIngestEventsThrownAwayDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestEventsThrownAway.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestEventsUnparseableDataPoint adds a data point to apachedruid.ingest.events.unparseable metric.
func (mb *MetricsBuilder) RecordApachedruidIngestEventsUnparseableDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestEventsUnparseable.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestHandoffCountDataPoint adds a data point to apachedruid.ingest.handoff.count metric.
func (mb *MetricsBuilder) RecordApachedruidIngestHandoffCountDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestHandoffCount.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestHandoffFailedDataPoint adds a data point to apachedruid.ingest.handoff.failed metric.
func (mb *MetricsBuilder) RecordApachedruidIngestHandoffFailedDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestHandoffFailed.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestHandoffTimeDataPoint adds a data point to apachedruid.ingest.handoff.time metric.
func (mb *MetricsBuilder) RecordApachedruidIngestHandoffTimeDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestHandoffTime.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestInputBytesDataPoint adds a data point to apachedruid.ingest.input.bytes metric.
func (mb *MetricsBuilder) RecordApachedruidIngestInputBytesDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestInputBytes.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestKafkaAvgLagDataPoint adds a data point to apachedruid.ingest.kafka.avg_lag metric.
func (mb *MetricsBuilder) RecordApachedruidIngestKafkaAvgLagDataPoint(ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	mb.metricApachedruidIngestKafkaAvgLag.recordDataPoint(mb.startTime, ts, val, ingestTagsAttributeValue, ingestStreamAttributeValue, ingestDataSourceAttributeValue)
}

// RecordApachedruidIngestKafkaLagDataPoint adds a data point to apachedruid.ingest.kafka.lag metric.
func (mb *MetricsBuilder) RecordApachedruidIngestKafkaLagDataPoint(ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	mb.metricApachedruidIngestKafkaLag.recordDataPoint(mb.startTime, ts, val, ingestTagsAttributeValue, ingestStreamAttributeValue, ingestDataSourceAttributeValue)
}

// RecordApachedruidIngestKafkaMaxLagDataPoint adds a data point to apachedruid.ingest.kafka.max_lag metric.
func (mb *MetricsBuilder) RecordApachedruidIngestKafkaMaxLagDataPoint(ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	mb.metricApachedruidIngestKafkaMaxLag.recordDataPoint(mb.startTime, ts, val, ingestTagsAttributeValue, ingestStreamAttributeValue, ingestDataSourceAttributeValue)
}

// RecordApachedruidIngestKafkaPartitionLagDataPoint adds a data point to apachedruid.ingest.kafka.partition_lag metric.
func (mb *MetricsBuilder) RecordApachedruidIngestKafkaPartitionLagDataPoint(ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestPartitionAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	mb.metricApachedruidIngestKafkaPartitionLag.recordDataPoint(mb.startTime, ts, val, ingestTagsAttributeValue, ingestPartitionAttributeValue, ingestStreamAttributeValue, ingestDataSourceAttributeValue)
}

// RecordApachedruidIngestKinesisAvgLagTimeDataPoint adds a data point to apachedruid.ingest.kinesis.avg_lag.time metric.
func (mb *MetricsBuilder) RecordApachedruidIngestKinesisAvgLagTimeDataPoint(ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	mb.metricApachedruidIngestKinesisAvgLagTime.recordDataPoint(mb.startTime, ts, val, ingestTagsAttributeValue, ingestStreamAttributeValue, ingestDataSourceAttributeValue)
}

// RecordApachedruidIngestKinesisLagTimeDataPoint adds a data point to apachedruid.ingest.kinesis.lag.time metric.
func (mb *MetricsBuilder) RecordApachedruidIngestKinesisLagTimeDataPoint(ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	mb.metricApachedruidIngestKinesisLagTime.recordDataPoint(mb.startTime, ts, val, ingestTagsAttributeValue, ingestStreamAttributeValue, ingestDataSourceAttributeValue)
}

// RecordApachedruidIngestKinesisMaxLagTimeDataPoint adds a data point to apachedruid.ingest.kinesis.max_lag.time metric.
func (mb *MetricsBuilder) RecordApachedruidIngestKinesisMaxLagTimeDataPoint(ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	mb.metricApachedruidIngestKinesisMaxLagTime.recordDataPoint(mb.startTime, ts, val, ingestTagsAttributeValue, ingestStreamAttributeValue, ingestDataSourceAttributeValue)
}

// RecordApachedruidIngestKinesisPartitionLagTimeDataPoint adds a data point to apachedruid.ingest.kinesis.partition_lag.time metric.
func (mb *MetricsBuilder) RecordApachedruidIngestKinesisPartitionLagTimeDataPoint(ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestPartitionAttributeValue string, ingestStreamAttributeValue string, ingestDataSourceAttributeValue string) {
	mb.metricApachedruidIngestKinesisPartitionLagTime.recordDataPoint(mb.startTime, ts, val, ingestTagsAttributeValue, ingestPartitionAttributeValue, ingestStreamAttributeValue, ingestDataSourceAttributeValue)
}

// RecordApachedruidIngestMergeCPUDataPoint adds a data point to apachedruid.ingest.merge.cpu metric.
func (mb *MetricsBuilder) RecordApachedruidIngestMergeCPUDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestMergeCPU.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestMergeTimeDataPoint adds a data point to apachedruid.ingest.merge.time metric.
func (mb *MetricsBuilder) RecordApachedruidIngestMergeTimeDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestMergeTime.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestNoticesQueueSizeDataPoint adds a data point to apachedruid.ingest.notices.queue_size metric.
func (mb *MetricsBuilder) RecordApachedruidIngestNoticesQueueSizeDataPoint(ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestDataSourceAttributeValue string) {
	mb.metricApachedruidIngestNoticesQueueSize.recordDataPoint(mb.startTime, ts, val, ingestTagsAttributeValue, ingestDataSourceAttributeValue)
}

// RecordApachedruidIngestNoticesTimeDataPoint adds a data point to apachedruid.ingest.notices.time metric.
func (mb *MetricsBuilder) RecordApachedruidIngestNoticesTimeDataPoint(ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestDataSourceAttributeValue string) {
	mb.metricApachedruidIngestNoticesTime.recordDataPoint(mb.startTime, ts, val, ingestTagsAttributeValue, ingestDataSourceAttributeValue)
}

// RecordApachedruidIngestPauseTimeDataPoint adds a data point to apachedruid.ingest.pause.time metric.
func (mb *MetricsBuilder) RecordApachedruidIngestPauseTimeDataPoint(ts pcommon.Timestamp, val int64, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string, ingestDataSourceAttributeValue string) {
	mb.metricApachedruidIngestPauseTime.recordDataPoint(mb.startTime, ts, val, ingestTagsAttributeValue, ingestTaskIDAttributeValue, ingestDataSourceAttributeValue)
}

// RecordApachedruidIngestPersistsBackPressureDataPoint adds a data point to apachedruid.ingest.persists.back_pressure metric.
func (mb *MetricsBuilder) RecordApachedruidIngestPersistsBackPressureDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestPersistsBackPressure.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestPersistsCountDataPoint adds a data point to apachedruid.ingest.persists.count metric.
func (mb *MetricsBuilder) RecordApachedruidIngestPersistsCountDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestPersistsCount.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestPersistsCPUDataPoint adds a data point to apachedruid.ingest.persists.cpu metric.
func (mb *MetricsBuilder) RecordApachedruidIngestPersistsCPUDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestPersistsCPU.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestPersistsFailedDataPoint adds a data point to apachedruid.ingest.persists.failed metric.
func (mb *MetricsBuilder) RecordApachedruidIngestPersistsFailedDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestPersistsFailed.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestPersistsTimeDataPoint adds a data point to apachedruid.ingest.persists.time metric.
func (mb *MetricsBuilder) RecordApachedruidIngestPersistsTimeDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestPersistsTime.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestRowsOutputDataPoint adds a data point to apachedruid.ingest.rows.output metric.
func (mb *MetricsBuilder) RecordApachedruidIngestRowsOutputDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestTaskIDAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string) {
	mb.metricApachedruidIngestRowsOutput.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestTaskIDAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue)
}

// RecordApachedruidIngestSegmentsCountDataPoint adds a data point to apachedruid.ingest.segments.count metric.
func (mb *MetricsBuilder) RecordApachedruidIngestSegmentsCountDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string, ingestTaskIngestionModeAttributeValue string) {
	mb.metricApachedruidIngestSegmentsCount.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue, ingestTaskIngestionModeAttributeValue)
}

// RecordApachedruidIngestShuffleBytesDataPoint adds a data point to apachedruid.ingest.shuffle.bytes metric.
func (mb *MetricsBuilder) RecordApachedruidIngestShuffleBytesDataPoint(ts pcommon.Timestamp, val int64, ingestSupervisorTaskIDAttributeValue string) {
	mb.metricApachedruidIngestShuffleBytes.recordDataPoint(mb.startTime, ts, val, ingestSupervisorTaskIDAttributeValue)
}

// RecordApachedruidIngestShuffleRequestsDataPoint adds a data point to apachedruid.ingest.shuffle.requests metric.
func (mb *MetricsBuilder) RecordApachedruidIngestShuffleRequestsDataPoint(ts pcommon.Timestamp, val int64, ingestSupervisorTaskIDAttributeValue string) {
	mb.metricApachedruidIngestShuffleRequests.recordDataPoint(mb.startTime, ts, val, ingestSupervisorTaskIDAttributeValue)
}

// RecordApachedruidIngestSinkCountDataPoint adds a data point to apachedruid.ingest.sink.count metric.
func (mb *MetricsBuilder) RecordApachedruidIngestSinkCountDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string) {
	mb.metricApachedruidIngestSinkCount.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue)
}

// RecordApachedruidIngestTombstonesCountDataPoint adds a data point to apachedruid.ingest.tombstones.count metric.
func (mb *MetricsBuilder) RecordApachedruidIngestTombstonesCountDataPoint(ts pcommon.Timestamp, val int64, ingestTaskTypeAttributeValue string, ingestDataSourceAttributeValue string, ingestGroupIDAttributeValue string, ingestTagsAttributeValue string, ingestTaskIDAttributeValue string, ingestTaskIngestionModeAttributeValue string) {
	mb.metricApachedruidIngestTombstonesCount.recordDataPoint(mb.startTime, ts, val, ingestTaskTypeAttributeValue, ingestDataSourceAttributeValue, ingestGroupIDAttributeValue, ingestTagsAttributeValue, ingestTaskIDAttributeValue, ingestTaskIngestionModeAttributeValue)
}

// RecordApachedruidIntervalCompactedCountDataPoint adds a data point to apachedruid.interval.compacted.count metric.
func (mb *MetricsBuilder) RecordApachedruidIntervalCompactedCountDataPoint(ts pcommon.Timestamp, val int64, intervalDataSourceAttributeValue string) {
	mb.metricApachedruidIntervalCompactedCount.recordDataPoint(mb.startTime, ts, val, intervalDataSourceAttributeValue)
}

// RecordApachedruidIntervalSkipCompactCountDataPoint adds a data point to apachedruid.interval.skip_compact.count metric.
func (mb *MetricsBuilder) RecordApachedruidIntervalSkipCompactCountDataPoint(ts pcommon.Timestamp, val int64, intervalDataSourceAttributeValue string) {
	mb.metricApachedruidIntervalSkipCompactCount.recordDataPoint(mb.startTime, ts, val, intervalDataSourceAttributeValue)
}

// RecordApachedruidIntervalWaitCompactCountDataPoint adds a data point to apachedruid.interval.wait_compact.count metric.
func (mb *MetricsBuilder) RecordApachedruidIntervalWaitCompactCountDataPoint(ts pcommon.Timestamp, val int64, intervalDataSourceAttributeValue string) {
	mb.metricApachedruidIntervalWaitCompactCount.recordDataPoint(mb.startTime, ts, val, intervalDataSourceAttributeValue)
}

// RecordApachedruidJettyNumOpenConnectionsDataPoint adds a data point to apachedruid.jetty.num_open_connections metric.
func (mb *MetricsBuilder) RecordApachedruidJettyNumOpenConnectionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidJettyNumOpenConnections.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidJettyThreadPoolBusyDataPoint adds a data point to apachedruid.jetty.thread_pool.busy metric.
func (mb *MetricsBuilder) RecordApachedruidJettyThreadPoolBusyDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidJettyThreadPoolBusy.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidJettyThreadPoolIdleDataPoint adds a data point to apachedruid.jetty.thread_pool.idle metric.
func (mb *MetricsBuilder) RecordApachedruidJettyThreadPoolIdleDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidJettyThreadPoolIdle.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidJettyThreadPoolIsLowOnThreadsDataPoint adds a data point to apachedruid.jetty.thread_pool.is_low_on_threads metric.
func (mb *MetricsBuilder) RecordApachedruidJettyThreadPoolIsLowOnThreadsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidJettyThreadPoolIsLowOnThreads.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidJettyThreadPoolMaxDataPoint adds a data point to apachedruid.jetty.thread_pool.max metric.
func (mb *MetricsBuilder) RecordApachedruidJettyThreadPoolMaxDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidJettyThreadPoolMax.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidJettyThreadPoolMinDataPoint adds a data point to apachedruid.jetty.thread_pool.min metric.
func (mb *MetricsBuilder) RecordApachedruidJettyThreadPoolMinDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidJettyThreadPoolMin.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidJettyThreadPoolQueueSizeDataPoint adds a data point to apachedruid.jetty.thread_pool.queue_size metric.
func (mb *MetricsBuilder) RecordApachedruidJettyThreadPoolQueueSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidJettyThreadPoolQueueSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidJettyThreadPoolTotalDataPoint adds a data point to apachedruid.jetty.thread_pool.total metric.
func (mb *MetricsBuilder) RecordApachedruidJettyThreadPoolTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidJettyThreadPoolTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidJvmBufferpoolCapacityDataPoint adds a data point to apachedruid.jvm.bufferpool.capacity metric.
func (mb *MetricsBuilder) RecordApachedruidJvmBufferpoolCapacityDataPoint(ts pcommon.Timestamp, val int64, jvmBufferpoolNameAttributeValue string) {
	mb.metricApachedruidJvmBufferpoolCapacity.recordDataPoint(mb.startTime, ts, val, jvmBufferpoolNameAttributeValue)
}

// RecordApachedruidJvmBufferpoolCountDataPoint adds a data point to apachedruid.jvm.bufferpool.count metric.
func (mb *MetricsBuilder) RecordApachedruidJvmBufferpoolCountDataPoint(ts pcommon.Timestamp, val int64, jvmBufferpoolNameAttributeValue string) {
	mb.metricApachedruidJvmBufferpoolCount.recordDataPoint(mb.startTime, ts, val, jvmBufferpoolNameAttributeValue)
}

// RecordApachedruidJvmBufferpoolUsedDataPoint adds a data point to apachedruid.jvm.bufferpool.used metric.
func (mb *MetricsBuilder) RecordApachedruidJvmBufferpoolUsedDataPoint(ts pcommon.Timestamp, val int64, jvmBufferpoolNameAttributeValue string) {
	mb.metricApachedruidJvmBufferpoolUsed.recordDataPoint(mb.startTime, ts, val, jvmBufferpoolNameAttributeValue)
}

// RecordApachedruidJvmGcCountDataPoint adds a data point to apachedruid.jvm.gc.count metric.
func (mb *MetricsBuilder) RecordApachedruidJvmGcCountDataPoint(ts pcommon.Timestamp, val int64, jvmGcGenAttributeValue string, jvmGcNameAttributeValue string) {
	mb.metricApachedruidJvmGcCount.recordDataPoint(mb.startTime, ts, val, jvmGcGenAttributeValue, jvmGcNameAttributeValue)
}

// RecordApachedruidJvmGcCPUDataPoint adds a data point to apachedruid.jvm.gc.cpu metric.
func (mb *MetricsBuilder) RecordApachedruidJvmGcCPUDataPoint(ts pcommon.Timestamp, val int64, jvmGcGenAttributeValue string, jvmGcNameAttributeValue string) {
	mb.metricApachedruidJvmGcCPU.recordDataPoint(mb.startTime, ts, val, jvmGcGenAttributeValue, jvmGcNameAttributeValue)
}

// RecordApachedruidJvmMemCommittedDataPoint adds a data point to apachedruid.jvm.mem.committed metric.
func (mb *MetricsBuilder) RecordApachedruidJvmMemCommittedDataPoint(ts pcommon.Timestamp, val int64, jvmMemKindAttributeValue string) {
	mb.metricApachedruidJvmMemCommitted.recordDataPoint(mb.startTime, ts, val, jvmMemKindAttributeValue)
}

// RecordApachedruidJvmMemInitDataPoint adds a data point to apachedruid.jvm.mem.init metric.
func (mb *MetricsBuilder) RecordApachedruidJvmMemInitDataPoint(ts pcommon.Timestamp, val int64, jvmMemKindAttributeValue string) {
	mb.metricApachedruidJvmMemInit.recordDataPoint(mb.startTime, ts, val, jvmMemKindAttributeValue)
}

// RecordApachedruidJvmMemMaxDataPoint adds a data point to apachedruid.jvm.mem.max metric.
func (mb *MetricsBuilder) RecordApachedruidJvmMemMaxDataPoint(ts pcommon.Timestamp, val int64, jvmMemKindAttributeValue string) {
	mb.metricApachedruidJvmMemMax.recordDataPoint(mb.startTime, ts, val, jvmMemKindAttributeValue)
}

// RecordApachedruidJvmMemUsedDataPoint adds a data point to apachedruid.jvm.mem.used metric.
func (mb *MetricsBuilder) RecordApachedruidJvmMemUsedDataPoint(ts pcommon.Timestamp, val int64, jvmMemKindAttributeValue string) {
	mb.metricApachedruidJvmMemUsed.recordDataPoint(mb.startTime, ts, val, jvmMemKindAttributeValue)
}

// RecordApachedruidJvmPoolCommittedDataPoint adds a data point to apachedruid.jvm.pool.committed metric.
func (mb *MetricsBuilder) RecordApachedruidJvmPoolCommittedDataPoint(ts pcommon.Timestamp, val int64, jvmPoolNameAttributeValue string, jvmPoolKindAttributeValue string) {
	mb.metricApachedruidJvmPoolCommitted.recordDataPoint(mb.startTime, ts, val, jvmPoolNameAttributeValue, jvmPoolKindAttributeValue)
}

// RecordApachedruidJvmPoolInitDataPoint adds a data point to apachedruid.jvm.pool.init metric.
func (mb *MetricsBuilder) RecordApachedruidJvmPoolInitDataPoint(ts pcommon.Timestamp, val int64, jvmPoolNameAttributeValue string, jvmPoolKindAttributeValue string) {
	mb.metricApachedruidJvmPoolInit.recordDataPoint(mb.startTime, ts, val, jvmPoolNameAttributeValue, jvmPoolKindAttributeValue)
}

// RecordApachedruidJvmPoolMaxDataPoint adds a data point to apachedruid.jvm.pool.max metric.
func (mb *MetricsBuilder) RecordApachedruidJvmPoolMaxDataPoint(ts pcommon.Timestamp, val int64, jvmPoolNameAttributeValue string, jvmPoolKindAttributeValue string) {
	mb.metricApachedruidJvmPoolMax.recordDataPoint(mb.startTime, ts, val, jvmPoolNameAttributeValue, jvmPoolKindAttributeValue)
}

// RecordApachedruidJvmPoolUsedDataPoint adds a data point to apachedruid.jvm.pool.used metric.
func (mb *MetricsBuilder) RecordApachedruidJvmPoolUsedDataPoint(ts pcommon.Timestamp, val int64, jvmPoolNameAttributeValue string, jvmPoolKindAttributeValue string) {
	mb.metricApachedruidJvmPoolUsed.recordDataPoint(mb.startTime, ts, val, jvmPoolNameAttributeValue, jvmPoolKindAttributeValue)
}

// RecordApachedruidKillPendingSegmentsCountDataPoint adds a data point to apachedruid.kill.pending_segments.count metric.
func (mb *MetricsBuilder) RecordApachedruidKillPendingSegmentsCountDataPoint(ts pcommon.Timestamp, val int64, killDataSourceAttributeValue string) {
	mb.metricApachedruidKillPendingSegmentsCount.recordDataPoint(mb.startTime, ts, val, killDataSourceAttributeValue)
}

// RecordApachedruidKillTaskCountDataPoint adds a data point to apachedruid.kill.task.count metric.
func (mb *MetricsBuilder) RecordApachedruidKillTaskCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidKillTaskCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidKillTaskAvailableSlotCountDataPoint adds a data point to apachedruid.kill_task.available_slot.count metric.
func (mb *MetricsBuilder) RecordApachedruidKillTaskAvailableSlotCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidKillTaskAvailableSlotCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidKillTaskMaxSlotCountDataPoint adds a data point to apachedruid.kill_task.max_slot.count metric.
func (mb *MetricsBuilder) RecordApachedruidKillTaskMaxSlotCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidKillTaskMaxSlotCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidMergeBufferPendingRequestsDataPoint adds a data point to apachedruid.merge_buffer.pending_requests metric.
func (mb *MetricsBuilder) RecordApachedruidMergeBufferPendingRequestsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidMergeBufferPendingRequests.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidMetadataKillAuditCountDataPoint adds a data point to apachedruid.metadata.kill.audit.count metric.
func (mb *MetricsBuilder) RecordApachedruidMetadataKillAuditCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidMetadataKillAuditCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidMetadataKillCompactionCountDataPoint adds a data point to apachedruid.metadata.kill.compaction.count metric.
func (mb *MetricsBuilder) RecordApachedruidMetadataKillCompactionCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidMetadataKillCompactionCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidMetadataKillDatasourceCountDataPoint adds a data point to apachedruid.metadata.kill.datasource.count metric.
func (mb *MetricsBuilder) RecordApachedruidMetadataKillDatasourceCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidMetadataKillDatasourceCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidMetadataKillRuleCountDataPoint adds a data point to apachedruid.metadata.kill.rule.count metric.
func (mb *MetricsBuilder) RecordApachedruidMetadataKillRuleCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidMetadataKillRuleCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidMetadataKillSupervisorCountDataPoint adds a data point to apachedruid.metadata.kill.supervisor.count metric.
func (mb *MetricsBuilder) RecordApachedruidMetadataKillSupervisorCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidMetadataKillSupervisorCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidMetadatacacheInitTimeDataPoint adds a data point to apachedruid.metadatacache.init.time metric.
func (mb *MetricsBuilder) RecordApachedruidMetadatacacheInitTimeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidMetadatacacheInitTime.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidMetadatacacheRefreshCountDataPoint adds a data point to apachedruid.metadatacache.refresh.count metric.
func (mb *MetricsBuilder) RecordApachedruidMetadatacacheRefreshCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidMetadatacacheRefreshCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidMetadatacacheRefreshTimeDataPoint adds a data point to apachedruid.metadatacache.refresh.time metric.
func (mb *MetricsBuilder) RecordApachedruidMetadatacacheRefreshTimeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidMetadatacacheRefreshTime.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryByteLimitExceededCountDataPoint adds a data point to apachedruid.query.byte_limit.exceeded.count metric.
func (mb *MetricsBuilder) RecordApachedruidQueryByteLimitExceededCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryByteLimitExceededCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryBytesDataPoint adds a data point to apachedruid.query.bytes metric.
func (mb *MetricsBuilder) RecordApachedruidQueryBytesDataPoint(ts pcommon.Timestamp, val int64, queryDataSourceAttributeValue string, queryNumMetricsAttributeValue string, queryDimensionAttributeValue string, queryHasFiltersAttributeValue string, queryThresholdAttributeValue int64, queryNumComplexMetricsAttributeValue int64, queryTypeAttributeValue string, queryRemoteAddressAttributeValue string, queryIDAttributeValue string, queryContextAttributeValue string, queryNumDimensionsAttributeValue string, queryIntervalAttributeValue string, queryDurationAttributeValue string) {
	mb.metricApachedruidQueryBytes.recordDataPoint(mb.startTime, ts, val, queryDataSourceAttributeValue, queryNumMetricsAttributeValue, queryDimensionAttributeValue, queryHasFiltersAttributeValue, queryThresholdAttributeValue, queryNumComplexMetricsAttributeValue, queryTypeAttributeValue, queryRemoteAddressAttributeValue, queryIDAttributeValue, queryContextAttributeValue, queryNumDimensionsAttributeValue, queryIntervalAttributeValue, queryDurationAttributeValue)
}

// RecordApachedruidQueryCacheDeltaAverageBytesDataPoint adds a data point to apachedruid.query.cache.delta.average_bytes metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaAverageBytesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheDeltaAverageBytes.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheDeltaErrorsDataPoint adds a data point to apachedruid.query.cache.delta.errors metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaErrorsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheDeltaErrors.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheDeltaEvictionsDataPoint adds a data point to apachedruid.query.cache.delta.evictions metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaEvictionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheDeltaEvictions.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheDeltaHitRateDataPoint adds a data point to apachedruid.query.cache.delta.hit_rate metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaHitRateDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricApachedruidQueryCacheDeltaHitRate.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheDeltaHitsDataPoint adds a data point to apachedruid.query.cache.delta.hits metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaHitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheDeltaHits.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheDeltaMissesDataPoint adds a data point to apachedruid.query.cache.delta.misses metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaMissesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheDeltaMisses.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheDeltaNumEntriesDataPoint adds a data point to apachedruid.query.cache.delta.num_entries metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaNumEntriesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheDeltaNumEntries.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheDeltaPutErrorDataPoint adds a data point to apachedruid.query.cache.delta.put.error metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaPutErrorDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheDeltaPutError.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheDeltaPutOkDataPoint adds a data point to apachedruid.query.cache.delta.put.ok metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaPutOkDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheDeltaPutOk.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheDeltaPutOversizedDataPoint adds a data point to apachedruid.query.cache.delta.put.oversized metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaPutOversizedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheDeltaPutOversized.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheDeltaSizeBytesDataPoint adds a data point to apachedruid.query.cache.delta.size_bytes metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaSizeBytesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheDeltaSizeBytes.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheDeltaTimeoutsDataPoint adds a data point to apachedruid.query.cache.delta.timeouts metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheDeltaTimeoutsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheDeltaTimeouts.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheMemcachedDeltaDataPoint adds a data point to apachedruid.query.cache.memcached.delta metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheMemcachedDeltaDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheMemcachedDelta.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheMemcachedTotalDataPoint adds a data point to apachedruid.query.cache.memcached.total metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheMemcachedTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheMemcachedTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalAverageBytesDataPoint adds a data point to apachedruid.query.cache.total.average_bytes metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalAverageBytesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheTotalAverageBytes.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalErrorsDataPoint adds a data point to apachedruid.query.cache.total.errors metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalErrorsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheTotalErrors.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalEvictionsDataPoint adds a data point to apachedruid.query.cache.total.evictions metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalEvictionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheTotalEvictions.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalHitRateDataPoint adds a data point to apachedruid.query.cache.total.hit_rate metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalHitRateDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricApachedruidQueryCacheTotalHitRate.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalHitsDataPoint adds a data point to apachedruid.query.cache.total.hits metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalHitsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheTotalHits.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalMissesDataPoint adds a data point to apachedruid.query.cache.total.misses metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalMissesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheTotalMisses.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalNumEntriesDataPoint adds a data point to apachedruid.query.cache.total.num_entries metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalNumEntriesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheTotalNumEntries.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalPutErrorDataPoint adds a data point to apachedruid.query.cache.total.put.error metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalPutErrorDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheTotalPutError.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalPutOkDataPoint adds a data point to apachedruid.query.cache.total.put.ok metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalPutOkDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheTotalPutOk.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalPutOversizedDataPoint adds a data point to apachedruid.query.cache.total.put.oversized metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalPutOversizedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheTotalPutOversized.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalSizeBytesDataPoint adds a data point to apachedruid.query.cache.total.size_bytes metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalSizeBytesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheTotalSizeBytes.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCacheTotalTimeoutsDataPoint adds a data point to apachedruid.query.cache.total.timeouts metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCacheTotalTimeoutsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCacheTotalTimeouts.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCountDataPoint adds a data point to apachedruid.query.count metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCPUTimeDataPoint adds a data point to apachedruid.query.cpu.time metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCPUTimeDataPoint(ts pcommon.Timestamp, val int64, queryDataSourceAttributeValue string, queryNumMetricsAttributeValue string, queryDimensionAttributeValue string, queryHasFiltersAttributeValue string, queryThresholdAttributeValue int64, queryNumComplexMetricsAttributeValue int64, queryTypeAttributeValue string, queryRemoteAddressAttributeValue string, queryIDAttributeValue string, queryContextAttributeValue string, queryNumDimensionsAttributeValue string, queryIntervalAttributeValue string, queryDurationAttributeValue string) {
	mb.metricApachedruidQueryCPUTime.recordDataPoint(mb.startTime, ts, val, queryDataSourceAttributeValue, queryNumMetricsAttributeValue, queryDimensionAttributeValue, queryHasFiltersAttributeValue, queryThresholdAttributeValue, queryNumComplexMetricsAttributeValue, queryTypeAttributeValue, queryRemoteAddressAttributeValue, queryIDAttributeValue, queryContextAttributeValue, queryNumDimensionsAttributeValue, queryIntervalAttributeValue, queryDurationAttributeValue)
}

// RecordApachedruidQueryFailedCountDataPoint adds a data point to apachedruid.query.failed.count metric.
func (mb *MetricsBuilder) RecordApachedruidQueryFailedCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryFailedCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryInterruptedCountDataPoint adds a data point to apachedruid.query.interrupted.count metric.
func (mb *MetricsBuilder) RecordApachedruidQueryInterruptedCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryInterruptedCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryNodeBackpressureDataPoint adds a data point to apachedruid.query.node.backpressure metric.
func (mb *MetricsBuilder) RecordApachedruidQueryNodeBackpressureDataPoint(ts pcommon.Timestamp, val int64, queryStatusAttributeValue string, queryServerAttributeValue string, queryIDAttributeValue string) {
	mb.metricApachedruidQueryNodeBackpressure.recordDataPoint(mb.startTime, ts, val, queryStatusAttributeValue, queryServerAttributeValue, queryIDAttributeValue)
}

// RecordApachedruidQueryNodeBytesDataPoint adds a data point to apachedruid.query.node.bytes metric.
func (mb *MetricsBuilder) RecordApachedruidQueryNodeBytesDataPoint(ts pcommon.Timestamp, val int64, queryStatusAttributeValue string, queryServerAttributeValue string, queryIDAttributeValue string) {
	mb.metricApachedruidQueryNodeBytes.recordDataPoint(mb.startTime, ts, val, queryStatusAttributeValue, queryServerAttributeValue, queryIDAttributeValue)
}

// RecordApachedruidQueryNodeTimeDataPoint adds a data point to apachedruid.query.node.time metric.
func (mb *MetricsBuilder) RecordApachedruidQueryNodeTimeDataPoint(ts pcommon.Timestamp, val int64, queryStatusAttributeValue string, queryServerAttributeValue string, queryIDAttributeValue string) {
	mb.metricApachedruidQueryNodeTime.recordDataPoint(mb.startTime, ts, val, queryStatusAttributeValue, queryServerAttributeValue, queryIDAttributeValue)
}

// RecordApachedruidQueryNodeTtfbDataPoint adds a data point to apachedruid.query.node.ttfb metric.
func (mb *MetricsBuilder) RecordApachedruidQueryNodeTtfbDataPoint(ts pcommon.Timestamp, val int64, queryStatusAttributeValue string, queryServerAttributeValue string, queryIDAttributeValue string) {
	mb.metricApachedruidQueryNodeTtfb.recordDataPoint(mb.startTime, ts, val, queryStatusAttributeValue, queryServerAttributeValue, queryIDAttributeValue)
}

// RecordApachedruidQueryPriorityDataPoint adds a data point to apachedruid.query.priority metric.
func (mb *MetricsBuilder) RecordApachedruidQueryPriorityDataPoint(ts pcommon.Timestamp, val int64, queryTypeAttributeValue string, queryDataSourceAttributeValue string, queryLaneAttributeValue string) {
	mb.metricApachedruidQueryPriority.recordDataPoint(mb.startTime, ts, val, queryTypeAttributeValue, queryDataSourceAttributeValue, queryLaneAttributeValue)
}

// RecordApachedruidQueryRowLimitExceededCountDataPoint adds a data point to apachedruid.query.row_limit.exceeded.count metric.
func (mb *MetricsBuilder) RecordApachedruidQueryRowLimitExceededCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryRowLimitExceededCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQuerySegmentTimeDataPoint adds a data point to apachedruid.query.segment.time metric.
func (mb *MetricsBuilder) RecordApachedruidQuerySegmentTimeDataPoint(ts pcommon.Timestamp, val int64, queryStatusAttributeValue string, querySegmentAttributeValue string, queryIDAttributeValue string, queryVectorizedAttributeValue string) {
	mb.metricApachedruidQuerySegmentTime.recordDataPoint(mb.startTime, ts, val, queryStatusAttributeValue, querySegmentAttributeValue, queryIDAttributeValue, queryVectorizedAttributeValue)
}

// RecordApachedruidQuerySegmentAndCacheTimeDataPoint adds a data point to apachedruid.query.segment_and_cache.time metric.
func (mb *MetricsBuilder) RecordApachedruidQuerySegmentAndCacheTimeDataPoint(ts pcommon.Timestamp, val int64, querySegmentAttributeValue string, queryIDAttributeValue string) {
	mb.metricApachedruidQuerySegmentAndCacheTime.recordDataPoint(mb.startTime, ts, val, querySegmentAttributeValue, queryIDAttributeValue)
}

// RecordApachedruidQuerySegmentsCountDataPoint adds a data point to apachedruid.query.segments.count metric.
func (mb *MetricsBuilder) RecordApachedruidQuerySegmentsCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQuerySegmentsCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQuerySuccessCountDataPoint adds a data point to apachedruid.query.success.count metric.
func (mb *MetricsBuilder) RecordApachedruidQuerySuccessCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQuerySuccessCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryTimeDataPoint adds a data point to apachedruid.query.time metric.
func (mb *MetricsBuilder) RecordApachedruidQueryTimeDataPoint(ts pcommon.Timestamp, val int64, queryDataSourceAttributeValue string, queryNumMetricsAttributeValue string, queryDimensionAttributeValue string, queryHasFiltersAttributeValue string, queryThresholdAttributeValue int64, queryNumComplexMetricsAttributeValue int64, queryTypeAttributeValue string, queryRemoteAddressAttributeValue string, queryIDAttributeValue string, queryContextAttributeValue string, queryNumDimensionsAttributeValue string, queryIntervalAttributeValue string, queryDurationAttributeValue string) {
	mb.metricApachedruidQueryTime.recordDataPoint(mb.startTime, ts, val, queryDataSourceAttributeValue, queryNumMetricsAttributeValue, queryDimensionAttributeValue, queryHasFiltersAttributeValue, queryThresholdAttributeValue, queryNumComplexMetricsAttributeValue, queryTypeAttributeValue, queryRemoteAddressAttributeValue, queryIDAttributeValue, queryContextAttributeValue, queryNumDimensionsAttributeValue, queryIntervalAttributeValue, queryDurationAttributeValue)
}

// RecordApachedruidQueryTimeoutCountDataPoint adds a data point to apachedruid.query.timeout.count metric.
func (mb *MetricsBuilder) RecordApachedruidQueryTimeoutCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryTimeoutCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryWaitTimeDataPoint adds a data point to apachedruid.query.wait.time metric.
func (mb *MetricsBuilder) RecordApachedruidQueryWaitTimeDataPoint(ts pcommon.Timestamp, val int64, querySegmentAttributeValue string, queryIDAttributeValue string) {
	mb.metricApachedruidQueryWaitTime.recordDataPoint(mb.startTime, ts, val, querySegmentAttributeValue, queryIDAttributeValue)
}

// RecordApachedruidSegmentAddedBytesDataPoint adds a data point to apachedruid.segment.added.bytes metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentAddedBytesDataPoint(ts pcommon.Timestamp, val int64, segmentTaskTypeAttributeValue string, segmentDataSourceAttributeValue string, segmentGroupIDAttributeValue string, segmentTagsAttributeValue string, segmentTaskIDAttributeValue string, segmentIntervalAttributeValue string) {
	mb.metricApachedruidSegmentAddedBytes.recordDataPoint(mb.startTime, ts, val, segmentTaskTypeAttributeValue, segmentDataSourceAttributeValue, segmentGroupIDAttributeValue, segmentTagsAttributeValue, segmentTaskIDAttributeValue, segmentIntervalAttributeValue)
}

// RecordApachedruidSegmentAssignSkippedCountDataPoint adds a data point to apachedruid.segment.assign_skipped.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentAssignSkippedCountDataPoint(ts pcommon.Timestamp, val int64, segmentDescriptionAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentAssignSkippedCount.recordDataPoint(mb.startTime, ts, val, segmentDescriptionAttributeValue, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentAssignedCountDataPoint adds a data point to apachedruid.segment.assigned.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentAssignedCountDataPoint(ts pcommon.Timestamp, val int64, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentAssignedCount.recordDataPoint(mb.startTime, ts, val, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentCompactedBytesDataPoint adds a data point to apachedruid.segment.compacted.bytes metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentCompactedBytesDataPoint(ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentCompactedBytes.recordDataPoint(mb.startTime, ts, val, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentCompactedCountDataPoint adds a data point to apachedruid.segment.compacted.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentCompactedCountDataPoint(ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentCompactedCount.recordDataPoint(mb.startTime, ts, val, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentCountDataPoint adds a data point to apachedruid.segment.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentCountDataPoint(ts pcommon.Timestamp, val int64, segmentPriorityAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentCount.recordDataPoint(mb.startTime, ts, val, segmentPriorityAttributeValue, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentDeletedCountDataPoint adds a data point to apachedruid.segment.deleted.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentDeletedCountDataPoint(ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentDeletedCount.recordDataPoint(mb.startTime, ts, val, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentDropQueueCountDataPoint adds a data point to apachedruid.segment.drop_queue.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentDropQueueCountDataPoint(ts pcommon.Timestamp, val int64, segmentServerAttributeValue string) {
	mb.metricApachedruidSegmentDropQueueCount.recordDataPoint(mb.startTime, ts, val, segmentServerAttributeValue)
}

// RecordApachedruidSegmentDropSkippedCountDataPoint adds a data point to apachedruid.segment.drop_skipped.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentDropSkippedCountDataPoint(ts pcommon.Timestamp, val int64, segmentDescriptionAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentDropSkippedCount.recordDataPoint(mb.startTime, ts, val, segmentDescriptionAttributeValue, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentDroppedCountDataPoint adds a data point to apachedruid.segment.dropped.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentDroppedCountDataPoint(ts pcommon.Timestamp, val int64, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentDroppedCount.recordDataPoint(mb.startTime, ts, val, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentLoadQueueAssignedDataPoint adds a data point to apachedruid.segment.load_queue.assigned metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentLoadQueueAssignedDataPoint(ts pcommon.Timestamp, val int64, segmentServerAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentLoadQueueAssigned.recordDataPoint(mb.startTime, ts, val, segmentServerAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentLoadQueueCancelledDataPoint adds a data point to apachedruid.segment.load_queue.cancelled metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentLoadQueueCancelledDataPoint(ts pcommon.Timestamp, val int64, segmentServerAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentLoadQueueCancelled.recordDataPoint(mb.startTime, ts, val, segmentServerAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentLoadQueueCountDataPoint adds a data point to apachedruid.segment.load_queue.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentLoadQueueCountDataPoint(ts pcommon.Timestamp, val int64, segmentServerAttributeValue string) {
	mb.metricApachedruidSegmentLoadQueueCount.recordDataPoint(mb.startTime, ts, val, segmentServerAttributeValue)
}

// RecordApachedruidSegmentLoadQueueFailedDataPoint adds a data point to apachedruid.segment.load_queue.failed metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentLoadQueueFailedDataPoint(ts pcommon.Timestamp, val int64, segmentServerAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentLoadQueueFailed.recordDataPoint(mb.startTime, ts, val, segmentServerAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentLoadQueueSizeDataPoint adds a data point to apachedruid.segment.load_queue.size metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentLoadQueueSizeDataPoint(ts pcommon.Timestamp, val int64, segmentServerAttributeValue string) {
	mb.metricApachedruidSegmentLoadQueueSize.recordDataPoint(mb.startTime, ts, val, segmentServerAttributeValue)
}

// RecordApachedruidSegmentLoadQueueSuccessDataPoint adds a data point to apachedruid.segment.load_queue.success metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentLoadQueueSuccessDataPoint(ts pcommon.Timestamp, val int64, segmentServerAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentLoadQueueSuccess.recordDataPoint(mb.startTime, ts, val, segmentServerAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentMaxDataPoint adds a data point to apachedruid.segment.max metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentMaxDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSegmentMax.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSegmentMoveSkippedCountDataPoint adds a data point to apachedruid.segment.move_skipped.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentMoveSkippedCountDataPoint(ts pcommon.Timestamp, val int64, segmentDescriptionAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentMoveSkippedCount.recordDataPoint(mb.startTime, ts, val, segmentDescriptionAttributeValue, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentMovedBytesDataPoint adds a data point to apachedruid.segment.moved.bytes metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentMovedBytesDataPoint(ts pcommon.Timestamp, val int64, segmentTaskTypeAttributeValue string, segmentDataSourceAttributeValue string, segmentGroupIDAttributeValue string, segmentTagsAttributeValue string, segmentTaskIDAttributeValue string, segmentIntervalAttributeValue string) {
	mb.metricApachedruidSegmentMovedBytes.recordDataPoint(mb.startTime, ts, val, segmentTaskTypeAttributeValue, segmentDataSourceAttributeValue, segmentGroupIDAttributeValue, segmentTagsAttributeValue, segmentTaskIDAttributeValue, segmentIntervalAttributeValue)
}

// RecordApachedruidSegmentMovedCountDataPoint adds a data point to apachedruid.segment.moved.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentMovedCountDataPoint(ts pcommon.Timestamp, val int64, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentMovedCount.recordDataPoint(mb.startTime, ts, val, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentNukedBytesDataPoint adds a data point to apachedruid.segment.nuked.bytes metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentNukedBytesDataPoint(ts pcommon.Timestamp, val int64, segmentTaskTypeAttributeValue string, segmentDataSourceAttributeValue string, segmentGroupIDAttributeValue string, segmentTagsAttributeValue string, segmentTaskIDAttributeValue string, segmentIntervalAttributeValue string) {
	mb.metricApachedruidSegmentNukedBytes.recordDataPoint(mb.startTime, ts, val, segmentTaskTypeAttributeValue, segmentDataSourceAttributeValue, segmentGroupIDAttributeValue, segmentTagsAttributeValue, segmentTaskIDAttributeValue, segmentIntervalAttributeValue)
}

// RecordApachedruidSegmentOverShadowedCountDataPoint adds a data point to apachedruid.segment.over_shadowed.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentOverShadowedCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSegmentOverShadowedCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSegmentPendingDeleteDataPoint adds a data point to apachedruid.segment.pending_delete metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentPendingDeleteDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSegmentPendingDelete.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSegmentRowCountAvgDataPoint adds a data point to apachedruid.segment.row_count.avg metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentRowCountAvgDataPoint(ts pcommon.Timestamp, val int64, segmentPriorityAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentRowCountAvg.recordDataPoint(mb.startTime, ts, val, segmentPriorityAttributeValue, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentRowCountRangeCountDataPoint adds a data point to apachedruid.segment.row_count.range.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentRowCountRangeCountDataPoint(ts pcommon.Timestamp, val int64, segmentPriorityAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string, segmentRangeAttributeValue string) {
	mb.metricApachedruidSegmentRowCountRangeCount.recordDataPoint(mb.startTime, ts, val, segmentPriorityAttributeValue, segmentTierAttributeValue, segmentDataSourceAttributeValue, segmentRangeAttributeValue)
}

// RecordApachedruidSegmentScanActiveDataPoint adds a data point to apachedruid.segment.scan.active metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentScanActiveDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSegmentScanActive.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSegmentScanPendingDataPoint adds a data point to apachedruid.segment.scan.pending metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentScanPendingDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSegmentScanPending.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSegmentSizeDataPoint adds a data point to apachedruid.segment.size metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentSizeDataPoint(ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentSize.recordDataPoint(mb.startTime, ts, val, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentSkipCompactBytesDataPoint adds a data point to apachedruid.segment.skip_compact.bytes metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentSkipCompactBytesDataPoint(ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentSkipCompactBytes.recordDataPoint(mb.startTime, ts, val, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentSkipCompactCountDataPoint adds a data point to apachedruid.segment.skip_compact.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentSkipCompactCountDataPoint(ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentSkipCompactCount.recordDataPoint(mb.startTime, ts, val, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentUnavailableCountDataPoint adds a data point to apachedruid.segment.unavailable.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentUnavailableCountDataPoint(ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentUnavailableCount.recordDataPoint(mb.startTime, ts, val, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentUnderReplicatedCountDataPoint adds a data point to apachedruid.segment.under_replicated.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentUnderReplicatedCountDataPoint(ts pcommon.Timestamp, val int64, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentUnderReplicatedCount.recordDataPoint(mb.startTime, ts, val, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentUnneededCountDataPoint adds a data point to apachedruid.segment.unneeded.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentUnneededCountDataPoint(ts pcommon.Timestamp, val int64, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentUnneededCount.recordDataPoint(mb.startTime, ts, val, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentUsedDataPoint adds a data point to apachedruid.segment.used metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentUsedDataPoint(ts pcommon.Timestamp, val int64, segmentPriorityAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentUsed.recordDataPoint(mb.startTime, ts, val, segmentPriorityAttributeValue, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentUsedPercentDataPoint adds a data point to apachedruid.segment.used_percent metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentUsedPercentDataPoint(ts pcommon.Timestamp, val float64, segmentPriorityAttributeValue string, segmentTierAttributeValue string, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentUsedPercent.recordDataPoint(mb.startTime, ts, val, segmentPriorityAttributeValue, segmentTierAttributeValue, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentWaitCompactBytesDataPoint adds a data point to apachedruid.segment.wait_compact.bytes metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentWaitCompactBytesDataPoint(ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentWaitCompactBytes.recordDataPoint(mb.startTime, ts, val, segmentDataSourceAttributeValue)
}

// RecordApachedruidSegmentWaitCompactCountDataPoint adds a data point to apachedruid.segment.wait_compact.count metric.
func (mb *MetricsBuilder) RecordApachedruidSegmentWaitCompactCountDataPoint(ts pcommon.Timestamp, val int64, segmentDataSourceAttributeValue string) {
	mb.metricApachedruidSegmentWaitCompactCount.recordDataPoint(mb.startTime, ts, val, segmentDataSourceAttributeValue)
}

// RecordApachedruidServerviewInitTimeDataPoint adds a data point to apachedruid.serverview.init.time metric.
func (mb *MetricsBuilder) RecordApachedruidServerviewInitTimeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidServerviewInitTime.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidServerviewSyncHealthyDataPoint adds a data point to apachedruid.serverview.sync.healthy metric.
func (mb *MetricsBuilder) RecordApachedruidServerviewSyncHealthyDataPoint(ts pcommon.Timestamp, val int64, serverviewTierAttributeValue string, serverviewServerAttributeValue string) {
	mb.metricApachedruidServerviewSyncHealthy.recordDataPoint(mb.startTime, ts, val, serverviewTierAttributeValue, serverviewServerAttributeValue)
}

// RecordApachedruidServerviewSyncUnstableTimeDataPoint adds a data point to apachedruid.serverview.sync.unstable_time metric.
func (mb *MetricsBuilder) RecordApachedruidServerviewSyncUnstableTimeDataPoint(ts pcommon.Timestamp, val int64, serverviewTierAttributeValue string, serverviewServerAttributeValue string) {
	mb.metricApachedruidServerviewSyncUnstableTime.recordDataPoint(mb.startTime, ts, val, serverviewTierAttributeValue, serverviewServerAttributeValue)
}

// RecordApachedruidSQLQueryBytesDataPoint adds a data point to apachedruid.sql_query.bytes metric.
func (mb *MetricsBuilder) RecordApachedruidSQLQueryBytesDataPoint(ts pcommon.Timestamp, val int64, sqlQueryDataSourceAttributeValue string, sqlQueryNativeQueryIdsAttributeValue string, sqlQueryEngineAttributeValue string, sqlQueryRemoteAddressAttributeValue string, sqlQueryIDAttributeValue string, sqlQuerySuccessAttributeValue string) {
	mb.metricApachedruidSQLQueryBytes.recordDataPoint(mb.startTime, ts, val, sqlQueryDataSourceAttributeValue, sqlQueryNativeQueryIdsAttributeValue, sqlQueryEngineAttributeValue, sqlQueryRemoteAddressAttributeValue, sqlQueryIDAttributeValue, sqlQuerySuccessAttributeValue)
}

// RecordApachedruidSQLQueryPlanningTimeMsDataPoint adds a data point to apachedruid.sql_query.planning_time_ms metric.
func (mb *MetricsBuilder) RecordApachedruidSQLQueryPlanningTimeMsDataPoint(ts pcommon.Timestamp, val int64, sqlQueryDataSourceAttributeValue string, sqlQueryNativeQueryIdsAttributeValue string, sqlQueryEngineAttributeValue string, sqlQueryRemoteAddressAttributeValue string, sqlQueryIDAttributeValue string, sqlQuerySuccessAttributeValue string) {
	mb.metricApachedruidSQLQueryPlanningTimeMs.recordDataPoint(mb.startTime, ts, val, sqlQueryDataSourceAttributeValue, sqlQueryNativeQueryIdsAttributeValue, sqlQueryEngineAttributeValue, sqlQueryRemoteAddressAttributeValue, sqlQueryIDAttributeValue, sqlQuerySuccessAttributeValue)
}

// RecordApachedruidSQLQueryTimeDataPoint adds a data point to apachedruid.sql_query.time metric.
func (mb *MetricsBuilder) RecordApachedruidSQLQueryTimeDataPoint(ts pcommon.Timestamp, val int64, sqlQueryDataSourceAttributeValue string, sqlQueryNativeQueryIdsAttributeValue string, sqlQueryEngineAttributeValue string, sqlQueryRemoteAddressAttributeValue string, sqlQueryIDAttributeValue string, sqlQuerySuccessAttributeValue string) {
	mb.metricApachedruidSQLQueryTime.recordDataPoint(mb.startTime, ts, val, sqlQueryDataSourceAttributeValue, sqlQueryNativeQueryIdsAttributeValue, sqlQueryEngineAttributeValue, sqlQueryRemoteAddressAttributeValue, sqlQueryIDAttributeValue, sqlQuerySuccessAttributeValue)
}

// RecordApachedruidSubqueryByteLimitCountDataPoint adds a data point to apachedruid.subquery.byte_limit.count metric.
func (mb *MetricsBuilder) RecordApachedruidSubqueryByteLimitCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSubqueryByteLimitCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSubqueryFallbackCountDataPoint adds a data point to apachedruid.subquery.fallback.count metric.
func (mb *MetricsBuilder) RecordApachedruidSubqueryFallbackCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSubqueryFallbackCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSubqueryFallbackInsufficientTypeCountDataPoint adds a data point to apachedruid.subquery.fallback.insufficient_type.count metric.
func (mb *MetricsBuilder) RecordApachedruidSubqueryFallbackInsufficientTypeCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSubqueryFallbackInsufficientTypeCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSubqueryFallbackUnknownReasonCountDataPoint adds a data point to apachedruid.subquery.fallback.unknown_reason.count metric.
func (mb *MetricsBuilder) RecordApachedruidSubqueryFallbackUnknownReasonCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSubqueryFallbackUnknownReasonCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSubqueryRowLimitCountDataPoint adds a data point to apachedruid.subquery.row_limit.count metric.
func (mb *MetricsBuilder) RecordApachedruidSubqueryRowLimitCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSubqueryRowLimitCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysCPUDataPoint adds a data point to apachedruid.sys.cpu metric.
func (mb *MetricsBuilder) RecordApachedruidSysCPUDataPoint(ts pcommon.Timestamp, val int64, sysCPUTimeAttributeValue string, sysCPUNameAttributeValue string) {
	mb.metricApachedruidSysCPU.recordDataPoint(mb.startTime, ts, val, sysCPUTimeAttributeValue, sysCPUNameAttributeValue)
}

// RecordApachedruidSysDiskQueueDataPoint adds a data point to apachedruid.sys.disk.queue metric.
func (mb *MetricsBuilder) RecordApachedruidSysDiskQueueDataPoint(ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	mb.metricApachedruidSysDiskQueue.recordDataPoint(mb.startTime, ts, val, sysDiskNameAttributeValue)
}

// RecordApachedruidSysDiskReadCountDataPoint adds a data point to apachedruid.sys.disk.read.count metric.
func (mb *MetricsBuilder) RecordApachedruidSysDiskReadCountDataPoint(ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	mb.metricApachedruidSysDiskReadCount.recordDataPoint(mb.startTime, ts, val, sysDiskNameAttributeValue)
}

// RecordApachedruidSysDiskReadSizeDataPoint adds a data point to apachedruid.sys.disk.read.size metric.
func (mb *MetricsBuilder) RecordApachedruidSysDiskReadSizeDataPoint(ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	mb.metricApachedruidSysDiskReadSize.recordDataPoint(mb.startTime, ts, val, sysDiskNameAttributeValue)
}

// RecordApachedruidSysDiskTransferTimeDataPoint adds a data point to apachedruid.sys.disk.transfer_time metric.
func (mb *MetricsBuilder) RecordApachedruidSysDiskTransferTimeDataPoint(ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	mb.metricApachedruidSysDiskTransferTime.recordDataPoint(mb.startTime, ts, val, sysDiskNameAttributeValue)
}

// RecordApachedruidSysDiskWriteCountDataPoint adds a data point to apachedruid.sys.disk.write.count metric.
func (mb *MetricsBuilder) RecordApachedruidSysDiskWriteCountDataPoint(ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	mb.metricApachedruidSysDiskWriteCount.recordDataPoint(mb.startTime, ts, val, sysDiskNameAttributeValue)
}

// RecordApachedruidSysDiskWriteSizeDataPoint adds a data point to apachedruid.sys.disk.write.size metric.
func (mb *MetricsBuilder) RecordApachedruidSysDiskWriteSizeDataPoint(ts pcommon.Timestamp, val int64, sysDiskNameAttributeValue string) {
	mb.metricApachedruidSysDiskWriteSize.recordDataPoint(mb.startTime, ts, val, sysDiskNameAttributeValue)
}

// RecordApachedruidSysFsFilesCountDataPoint adds a data point to apachedruid.sys.fs.files.count metric.
func (mb *MetricsBuilder) RecordApachedruidSysFsFilesCountDataPoint(ts pcommon.Timestamp, val int64, sysFsDirNameAttributeValue string, sysFsDevNameAttributeValue string) {
	mb.metricApachedruidSysFsFilesCount.recordDataPoint(mb.startTime, ts, val, sysFsDirNameAttributeValue, sysFsDevNameAttributeValue)
}

// RecordApachedruidSysFsFilesFreeDataPoint adds a data point to apachedruid.sys.fs.files.free metric.
func (mb *MetricsBuilder) RecordApachedruidSysFsFilesFreeDataPoint(ts pcommon.Timestamp, val int64, sysFsDirNameAttributeValue string, sysFsDevNameAttributeValue string) {
	mb.metricApachedruidSysFsFilesFree.recordDataPoint(mb.startTime, ts, val, sysFsDirNameAttributeValue, sysFsDevNameAttributeValue)
}

// RecordApachedruidSysFsMaxDataPoint adds a data point to apachedruid.sys.fs.max metric.
func (mb *MetricsBuilder) RecordApachedruidSysFsMaxDataPoint(ts pcommon.Timestamp, val int64, sysFsDirNameAttributeValue string, sysFsDevNameAttributeValue string) {
	mb.metricApachedruidSysFsMax.recordDataPoint(mb.startTime, ts, val, sysFsDirNameAttributeValue, sysFsDevNameAttributeValue)
}

// RecordApachedruidSysFsUsedDataPoint adds a data point to apachedruid.sys.fs.used metric.
func (mb *MetricsBuilder) RecordApachedruidSysFsUsedDataPoint(ts pcommon.Timestamp, val int64, sysFsDirNameAttributeValue string, sysFsDevNameAttributeValue string) {
	mb.metricApachedruidSysFsUsed.recordDataPoint(mb.startTime, ts, val, sysFsDirNameAttributeValue, sysFsDevNameAttributeValue)
}

// RecordApachedruidSysLa1DataPoint adds a data point to apachedruid.sys.la.1 metric.
func (mb *MetricsBuilder) RecordApachedruidSysLa1DataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysLa1.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysLa15DataPoint adds a data point to apachedruid.sys.la.15 metric.
func (mb *MetricsBuilder) RecordApachedruidSysLa15DataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysLa15.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysLa5DataPoint adds a data point to apachedruid.sys.la.5 metric.
func (mb *MetricsBuilder) RecordApachedruidSysLa5DataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysLa5.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysMemFreeDataPoint adds a data point to apachedruid.sys.mem.free metric.
func (mb *MetricsBuilder) RecordApachedruidSysMemFreeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysMemFree.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysMemMaxDataPoint adds a data point to apachedruid.sys.mem.max metric.
func (mb *MetricsBuilder) RecordApachedruidSysMemMaxDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysMemMax.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysMemUsedDataPoint adds a data point to apachedruid.sys.mem.used metric.
func (mb *MetricsBuilder) RecordApachedruidSysMemUsedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysMemUsed.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysNetReadDroppedDataPoint adds a data point to apachedruid.sys.net.read.dropped metric.
func (mb *MetricsBuilder) RecordApachedruidSysNetReadDroppedDataPoint(ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	mb.metricApachedruidSysNetReadDropped.recordDataPoint(mb.startTime, ts, val, sysNetHwaddrAttributeValue, sysNetNameAttributeValue, sysNetAddressAttributeValue)
}

// RecordApachedruidSysNetReadErrorsDataPoint adds a data point to apachedruid.sys.net.read.errors metric.
func (mb *MetricsBuilder) RecordApachedruidSysNetReadErrorsDataPoint(ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	mb.metricApachedruidSysNetReadErrors.recordDataPoint(mb.startTime, ts, val, sysNetHwaddrAttributeValue, sysNetNameAttributeValue, sysNetAddressAttributeValue)
}

// RecordApachedruidSysNetReadPacketsDataPoint adds a data point to apachedruid.sys.net.read.packets metric.
func (mb *MetricsBuilder) RecordApachedruidSysNetReadPacketsDataPoint(ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	mb.metricApachedruidSysNetReadPackets.recordDataPoint(mb.startTime, ts, val, sysNetHwaddrAttributeValue, sysNetNameAttributeValue, sysNetAddressAttributeValue)
}

// RecordApachedruidSysNetReadSizeDataPoint adds a data point to apachedruid.sys.net.read.size metric.
func (mb *MetricsBuilder) RecordApachedruidSysNetReadSizeDataPoint(ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	mb.metricApachedruidSysNetReadSize.recordDataPoint(mb.startTime, ts, val, sysNetHwaddrAttributeValue, sysNetNameAttributeValue, sysNetAddressAttributeValue)
}

// RecordApachedruidSysNetWriteCollisionsDataPoint adds a data point to apachedruid.sys.net.write.collisions metric.
func (mb *MetricsBuilder) RecordApachedruidSysNetWriteCollisionsDataPoint(ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	mb.metricApachedruidSysNetWriteCollisions.recordDataPoint(mb.startTime, ts, val, sysNetHwaddrAttributeValue, sysNetNameAttributeValue, sysNetAddressAttributeValue)
}

// RecordApachedruidSysNetWriteErrorsDataPoint adds a data point to apachedruid.sys.net.write.errors metric.
func (mb *MetricsBuilder) RecordApachedruidSysNetWriteErrorsDataPoint(ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	mb.metricApachedruidSysNetWriteErrors.recordDataPoint(mb.startTime, ts, val, sysNetHwaddrAttributeValue, sysNetNameAttributeValue, sysNetAddressAttributeValue)
}

// RecordApachedruidSysNetWritePacketsDataPoint adds a data point to apachedruid.sys.net.write.packets metric.
func (mb *MetricsBuilder) RecordApachedruidSysNetWritePacketsDataPoint(ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	mb.metricApachedruidSysNetWritePackets.recordDataPoint(mb.startTime, ts, val, sysNetHwaddrAttributeValue, sysNetNameAttributeValue, sysNetAddressAttributeValue)
}

// RecordApachedruidSysNetWriteSizeDataPoint adds a data point to apachedruid.sys.net.write.size metric.
func (mb *MetricsBuilder) RecordApachedruidSysNetWriteSizeDataPoint(ts pcommon.Timestamp, val int64, sysNetHwaddrAttributeValue string, sysNetNameAttributeValue string, sysNetAddressAttributeValue string) {
	mb.metricApachedruidSysNetWriteSize.recordDataPoint(mb.startTime, ts, val, sysNetHwaddrAttributeValue, sysNetNameAttributeValue, sysNetAddressAttributeValue)
}

// RecordApachedruidSysStorageUsedDataPoint adds a data point to apachedruid.sys.storage.used metric.
func (mb *MetricsBuilder) RecordApachedruidSysStorageUsedDataPoint(ts pcommon.Timestamp, val int64, sysFsDirNameAttributeValue string) {
	mb.metricApachedruidSysStorageUsed.recordDataPoint(mb.startTime, ts, val, sysFsDirNameAttributeValue)
}

// RecordApachedruidSysSwapFreeDataPoint adds a data point to apachedruid.sys.swap.free metric.
func (mb *MetricsBuilder) RecordApachedruidSysSwapFreeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysSwapFree.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysSwapMaxDataPoint adds a data point to apachedruid.sys.swap.max metric.
func (mb *MetricsBuilder) RecordApachedruidSysSwapMaxDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysSwapMax.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysSwapPageInDataPoint adds a data point to apachedruid.sys.swap.page_in metric.
func (mb *MetricsBuilder) RecordApachedruidSysSwapPageInDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysSwapPageIn.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysSwapPageOutDataPoint adds a data point to apachedruid.sys.swap.page_out metric.
func (mb *MetricsBuilder) RecordApachedruidSysSwapPageOutDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysSwapPageOut.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysTcpv4ActiveOpensDataPoint adds a data point to apachedruid.sys.tcpv4.active_opens metric.
func (mb *MetricsBuilder) RecordApachedruidSysTcpv4ActiveOpensDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysTcpv4ActiveOpens.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysTcpv4AttemptFailsDataPoint adds a data point to apachedruid.sys.tcpv4.attempt_fails metric.
func (mb *MetricsBuilder) RecordApachedruidSysTcpv4AttemptFailsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysTcpv4AttemptFails.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysTcpv4EstabResetsDataPoint adds a data point to apachedruid.sys.tcpv4.estab_resets metric.
func (mb *MetricsBuilder) RecordApachedruidSysTcpv4EstabResetsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysTcpv4EstabResets.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysTcpv4InErrsDataPoint adds a data point to apachedruid.sys.tcpv4.in.errs metric.
func (mb *MetricsBuilder) RecordApachedruidSysTcpv4InErrsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysTcpv4InErrs.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysTcpv4InSegsDataPoint adds a data point to apachedruid.sys.tcpv4.in.segs metric.
func (mb *MetricsBuilder) RecordApachedruidSysTcpv4InSegsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysTcpv4InSegs.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysTcpv4OutRstsDataPoint adds a data point to apachedruid.sys.tcpv4.out.rsts metric.
func (mb *MetricsBuilder) RecordApachedruidSysTcpv4OutRstsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysTcpv4OutRsts.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysTcpv4OutSegsDataPoint adds a data point to apachedruid.sys.tcpv4.out.segs metric.
func (mb *MetricsBuilder) RecordApachedruidSysTcpv4OutSegsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysTcpv4OutSegs.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysTcpv4PassiveOpensDataPoint adds a data point to apachedruid.sys.tcpv4.passive_opens metric.
func (mb *MetricsBuilder) RecordApachedruidSysTcpv4PassiveOpensDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysTcpv4PassiveOpens.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysTcpv4RetransSegsDataPoint adds a data point to apachedruid.sys.tcpv4.retrans.segs metric.
func (mb *MetricsBuilder) RecordApachedruidSysTcpv4RetransSegsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysTcpv4RetransSegs.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSysUptimeDataPoint adds a data point to apachedruid.sys.uptime metric.
func (mb *MetricsBuilder) RecordApachedruidSysUptimeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSysUptime.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidTaskActionBatchAttemptsDataPoint adds a data point to apachedruid.task.action.batch.attempts metric.
func (mb *MetricsBuilder) RecordApachedruidTaskActionBatchAttemptsDataPoint(ts pcommon.Timestamp, val int64, taskIntervalAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string) {
	mb.metricApachedruidTaskActionBatchAttempts.recordDataPoint(mb.startTime, ts, val, taskIntervalAttributeValue, taskDataSourceAttributeValue, taskActionTypeAttributeValue)
}

// RecordApachedruidTaskActionBatchQueueTimeDataPoint adds a data point to apachedruid.task.action.batch.queue_time metric.
func (mb *MetricsBuilder) RecordApachedruidTaskActionBatchQueueTimeDataPoint(ts pcommon.Timestamp, val int64, taskIntervalAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string) {
	mb.metricApachedruidTaskActionBatchQueueTime.recordDataPoint(mb.startTime, ts, val, taskIntervalAttributeValue, taskDataSourceAttributeValue, taskActionTypeAttributeValue)
}

// RecordApachedruidTaskActionBatchRunTimeDataPoint adds a data point to apachedruid.task.action.batch.run_time metric.
func (mb *MetricsBuilder) RecordApachedruidTaskActionBatchRunTimeDataPoint(ts pcommon.Timestamp, val int64, taskIntervalAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string) {
	mb.metricApachedruidTaskActionBatchRunTime.recordDataPoint(mb.startTime, ts, val, taskIntervalAttributeValue, taskDataSourceAttributeValue, taskActionTypeAttributeValue)
}

// RecordApachedruidTaskActionBatchSizeDataPoint adds a data point to apachedruid.task.action.batch.size metric.
func (mb *MetricsBuilder) RecordApachedruidTaskActionBatchSizeDataPoint(ts pcommon.Timestamp, val int64, taskIntervalAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string) {
	mb.metricApachedruidTaskActionBatchSize.recordDataPoint(mb.startTime, ts, val, taskIntervalAttributeValue, taskDataSourceAttributeValue, taskActionTypeAttributeValue)
}

// RecordApachedruidTaskActionFailedCountDataPoint adds a data point to apachedruid.task.action.failed.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskActionFailedCountDataPoint(ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string, taskGroupIDAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	mb.metricApachedruidTaskActionFailedCount.recordDataPoint(mb.startTime, ts, val, taskTypeAttributeValue, taskDataSourceAttributeValue, taskActionTypeAttributeValue, taskGroupIDAttributeValue, taskTagsAttributeValue, taskIDAttributeValue)
}

// RecordApachedruidTaskActionLogTimeDataPoint adds a data point to apachedruid.task.action.log.time metric.
func (mb *MetricsBuilder) RecordApachedruidTaskActionLogTimeDataPoint(ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string, taskGroupIDAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	mb.metricApachedruidTaskActionLogTime.recordDataPoint(mb.startTime, ts, val, taskTypeAttributeValue, taskDataSourceAttributeValue, taskActionTypeAttributeValue, taskGroupIDAttributeValue, taskTagsAttributeValue, taskIDAttributeValue)
}

// RecordApachedruidTaskActionRunTimeDataPoint adds a data point to apachedruid.task.action.run.time metric.
func (mb *MetricsBuilder) RecordApachedruidTaskActionRunTimeDataPoint(ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string, taskGroupIDAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	mb.metricApachedruidTaskActionRunTime.recordDataPoint(mb.startTime, ts, val, taskTypeAttributeValue, taskDataSourceAttributeValue, taskActionTypeAttributeValue, taskGroupIDAttributeValue, taskTagsAttributeValue, taskIDAttributeValue)
}

// RecordApachedruidTaskActionSuccessCountDataPoint adds a data point to apachedruid.task.action.success.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskActionSuccessCountDataPoint(ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskActionTypeAttributeValue string, taskGroupIDAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	mb.metricApachedruidTaskActionSuccessCount.recordDataPoint(mb.startTime, ts, val, taskTypeAttributeValue, taskDataSourceAttributeValue, taskActionTypeAttributeValue, taskGroupIDAttributeValue, taskTagsAttributeValue, taskIDAttributeValue)
}

// RecordApachedruidTaskFailedCountDataPoint adds a data point to apachedruid.task.failed.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskFailedCountDataPoint(ts pcommon.Timestamp, val int64, taskDataSourceAttributeValue string) {
	mb.metricApachedruidTaskFailedCount.recordDataPoint(mb.startTime, ts, val, taskDataSourceAttributeValue)
}

// RecordApachedruidTaskPendingCountDataPoint adds a data point to apachedruid.task.pending.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskPendingCountDataPoint(ts pcommon.Timestamp, val int64, taskDataSourceAttributeValue string) {
	mb.metricApachedruidTaskPendingCount.recordDataPoint(mb.startTime, ts, val, taskDataSourceAttributeValue)
}

// RecordApachedruidTaskPendingTimeDataPoint adds a data point to apachedruid.task.pending.time metric.
func (mb *MetricsBuilder) RecordApachedruidTaskPendingTimeDataPoint(ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskGroupIDAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	mb.metricApachedruidTaskPendingTime.recordDataPoint(mb.startTime, ts, val, taskTypeAttributeValue, taskDataSourceAttributeValue, taskGroupIDAttributeValue, taskTagsAttributeValue, taskIDAttributeValue)
}

// RecordApachedruidTaskRunTimeDataPoint adds a data point to apachedruid.task.run.time metric.
func (mb *MetricsBuilder) RecordApachedruidTaskRunTimeDataPoint(ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskGroupIDAttributeValue string, taskStatusAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	mb.metricApachedruidTaskRunTime.recordDataPoint(mb.startTime, ts, val, taskTypeAttributeValue, taskDataSourceAttributeValue, taskGroupIDAttributeValue, taskStatusAttributeValue, taskTagsAttributeValue, taskIDAttributeValue)
}

// RecordApachedruidTaskRunningCountDataPoint adds a data point to apachedruid.task.running.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskRunningCountDataPoint(ts pcommon.Timestamp, val int64, taskDataSourceAttributeValue string) {
	mb.metricApachedruidTaskRunningCount.recordDataPoint(mb.startTime, ts, val, taskDataSourceAttributeValue)
}

// RecordApachedruidTaskSegmentAvailabilityWaitTimeDataPoint adds a data point to apachedruid.task.segment_availability.wait.time metric.
func (mb *MetricsBuilder) RecordApachedruidTaskSegmentAvailabilityWaitTimeDataPoint(ts pcommon.Timestamp, val int64, taskTypeAttributeValue string, taskDataSourceAttributeValue string, taskGroupIDAttributeValue string, taskSegmentAvailabilityConfirmedAttributeValue string, taskTagsAttributeValue string, taskIDAttributeValue string) {
	mb.metricApachedruidTaskSegmentAvailabilityWaitTime.recordDataPoint(mb.startTime, ts, val, taskTypeAttributeValue, taskDataSourceAttributeValue, taskGroupIDAttributeValue, taskSegmentAvailabilityConfirmedAttributeValue, taskTagsAttributeValue, taskIDAttributeValue)
}

// RecordApachedruidTaskSuccessCountDataPoint adds a data point to apachedruid.task.success.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskSuccessCountDataPoint(ts pcommon.Timestamp, val int64, taskDataSourceAttributeValue string) {
	mb.metricApachedruidTaskSuccessCount.recordDataPoint(mb.startTime, ts, val, taskDataSourceAttributeValue)
}

// RecordApachedruidTaskWaitingCountDataPoint adds a data point to apachedruid.task.waiting.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskWaitingCountDataPoint(ts pcommon.Timestamp, val int64, taskDataSourceAttributeValue string) {
	mb.metricApachedruidTaskWaitingCount.recordDataPoint(mb.startTime, ts, val, taskDataSourceAttributeValue)
}

// RecordApachedruidTaskSlotBlacklistedCountDataPoint adds a data point to apachedruid.task_slot.blacklisted.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskSlotBlacklistedCountDataPoint(ts pcommon.Timestamp, val int64, taskSlotCategoryAttributeValue string) {
	mb.metricApachedruidTaskSlotBlacklistedCount.recordDataPoint(mb.startTime, ts, val, taskSlotCategoryAttributeValue)
}

// RecordApachedruidTaskSlotIdleCountDataPoint adds a data point to apachedruid.task_slot.idle.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskSlotIdleCountDataPoint(ts pcommon.Timestamp, val int64, taskSlotCategoryAttributeValue string) {
	mb.metricApachedruidTaskSlotIdleCount.recordDataPoint(mb.startTime, ts, val, taskSlotCategoryAttributeValue)
}

// RecordApachedruidTaskSlotLazyCountDataPoint adds a data point to apachedruid.task_slot.lazy.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskSlotLazyCountDataPoint(ts pcommon.Timestamp, val int64, taskSlotCategoryAttributeValue string) {
	mb.metricApachedruidTaskSlotLazyCount.recordDataPoint(mb.startTime, ts, val, taskSlotCategoryAttributeValue)
}

// RecordApachedruidTaskSlotTotalCountDataPoint adds a data point to apachedruid.task_slot.total.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskSlotTotalCountDataPoint(ts pcommon.Timestamp, val int64, taskSlotCategoryAttributeValue string) {
	mb.metricApachedruidTaskSlotTotalCount.recordDataPoint(mb.startTime, ts, val, taskSlotCategoryAttributeValue)
}

// RecordApachedruidTaskSlotUsedCountDataPoint adds a data point to apachedruid.task_slot.used.count metric.
func (mb *MetricsBuilder) RecordApachedruidTaskSlotUsedCountDataPoint(ts pcommon.Timestamp, val int64, taskSlotCategoryAttributeValue string) {
	mb.metricApachedruidTaskSlotUsedCount.recordDataPoint(mb.startTime, ts, val, taskSlotCategoryAttributeValue)
}

// RecordApachedruidTierHistoricalCountDataPoint adds a data point to apachedruid.tier.historical.count metric.
func (mb *MetricsBuilder) RecordApachedruidTierHistoricalCountDataPoint(ts pcommon.Timestamp, val int64, tierAttributeValue string) {
	mb.metricApachedruidTierHistoricalCount.recordDataPoint(mb.startTime, ts, val, tierAttributeValue)
}

// RecordApachedruidTierReplicationFactorDataPoint adds a data point to apachedruid.tier.replication.factor metric.
func (mb *MetricsBuilder) RecordApachedruidTierReplicationFactorDataPoint(ts pcommon.Timestamp, val int64, tierAttributeValue string) {
	mb.metricApachedruidTierReplicationFactor.recordDataPoint(mb.startTime, ts, val, tierAttributeValue)
}

// RecordApachedruidTierRequiredCapacityDataPoint adds a data point to apachedruid.tier.required.capacity metric.
func (mb *MetricsBuilder) RecordApachedruidTierRequiredCapacityDataPoint(ts pcommon.Timestamp, val int64, tierAttributeValue string) {
	mb.metricApachedruidTierRequiredCapacity.recordDataPoint(mb.startTime, ts, val, tierAttributeValue)
}

// RecordApachedruidTierTotalCapacityDataPoint adds a data point to apachedruid.tier.total.capacity metric.
func (mb *MetricsBuilder) RecordApachedruidTierTotalCapacityDataPoint(ts pcommon.Timestamp, val int64, tierAttributeValue string) {
	mb.metricApachedruidTierTotalCapacity.recordDataPoint(mb.startTime, ts, val, tierAttributeValue)
}

// RecordApachedruidWorkerTaskFailedCountDataPoint adds a data point to apachedruid.worker.task.failed.count metric.
func (mb *MetricsBuilder) RecordApachedruidWorkerTaskFailedCountDataPoint(ts pcommon.Timestamp, val int64, workerCategoryAttributeValue string, workerVersionAttributeValue string) {
	mb.metricApachedruidWorkerTaskFailedCount.recordDataPoint(mb.startTime, ts, val, workerCategoryAttributeValue, workerVersionAttributeValue)
}

// RecordApachedruidWorkerTaskSuccessCountDataPoint adds a data point to apachedruid.worker.task.success.count metric.
func (mb *MetricsBuilder) RecordApachedruidWorkerTaskSuccessCountDataPoint(ts pcommon.Timestamp, val int64, workerCategoryAttributeValue string, workerVersionAttributeValue string) {
	mb.metricApachedruidWorkerTaskSuccessCount.recordDataPoint(mb.startTime, ts, val, workerCategoryAttributeValue, workerVersionAttributeValue)
}

// RecordApachedruidWorkerTaskSlotIdleCountDataPoint adds a data point to apachedruid.worker.task_slot.idle.count metric.
func (mb *MetricsBuilder) RecordApachedruidWorkerTaskSlotIdleCountDataPoint(ts pcommon.Timestamp, val int64, workerCategoryAttributeValue string, workerVersionAttributeValue string) {
	mb.metricApachedruidWorkerTaskSlotIdleCount.recordDataPoint(mb.startTime, ts, val, workerCategoryAttributeValue, workerVersionAttributeValue)
}

// RecordApachedruidWorkerTaskSlotTotalCountDataPoint adds a data point to apachedruid.worker.task_slot.total.count metric.
func (mb *MetricsBuilder) RecordApachedruidWorkerTaskSlotTotalCountDataPoint(ts pcommon.Timestamp, val int64, workerCategoryAttributeValue string, workerVersionAttributeValue string) {
	mb.metricApachedruidWorkerTaskSlotTotalCount.recordDataPoint(mb.startTime, ts, val, workerCategoryAttributeValue, workerVersionAttributeValue)
}

// RecordApachedruidWorkerTaskSlotUsedCountDataPoint adds a data point to apachedruid.worker.task_slot.used.count metric.
func (mb *MetricsBuilder) RecordApachedruidWorkerTaskSlotUsedCountDataPoint(ts pcommon.Timestamp, val int64, workerCategoryAttributeValue string, workerVersionAttributeValue string) {
	mb.metricApachedruidWorkerTaskSlotUsedCount.recordDataPoint(mb.startTime, ts, val, workerCategoryAttributeValue, workerVersionAttributeValue)
}

// RecordApachedruidZkConnectedDataPoint adds a data point to apachedruid.zk.connected metric.
func (mb *MetricsBuilder) RecordApachedruidZkConnectedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidZkConnected.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidZkReconnectTimeDataPoint adds a data point to apachedruid.zk.reconnect.time metric.
func (mb *MetricsBuilder) RecordApachedruidZkReconnectTimeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidZkReconnectTime.recordDataPoint(mb.startTime, ts, val)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}
