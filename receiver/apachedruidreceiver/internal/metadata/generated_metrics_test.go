// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"testing"

	"github.com/stretchr/testify/assert"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver/receivertest"
	"go.uber.org/zap"
	"go.uber.org/zap/zaptest/observer"
)

type testConfigCollection int

const (
	testSetDefault testConfigCollection = iota
	testSetAll
	testSetNone
)

func TestMetricsBuilder(t *testing.T) {
	tests := []struct {
		name      string
		configSet testConfigCollection
	}{
		{
			name:      "default",
			configSet: testSetDefault,
		},
		{
			name:      "all_set",
			configSet: testSetAll,
		},
		{
			name:      "none_set",
			configSet: testSetNone,
		},
	}
	for _, test := range tests {
		t.Run(test.name, func(t *testing.T) {
			start := pcommon.Timestamp(1_000_000_000)
			ts := pcommon.Timestamp(1_000_001_000)
			observedZapCore, observedLogs := observer.New(zap.WarnLevel)
			settings := receivertest.NewNopCreateSettings()
			settings.Logger = zap.New(observedZapCore)
			mb := NewMetricsBuilder(loadMetricsBuilderConfig(t, test.name), settings, WithStartTime(start))

			expectedWarnings := 0

			assert.Equal(t, expectedWarnings, observedLogs.Len())

			defaultMetricsCount := 0
			allMetricsCount := 0

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidCompactSegmentAnalyzerFetchAndProcessMillisDataPoint(ts, 1, "compact_task_type-val", "compact_data_source-val", "compact_group_id-val", "compact_tags-val", "compact_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidCompactTaskCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidCompactTaskAvailableSlotCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidCompactTaskMaxSlotCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidCoordinatorGlobalTimeDataPoint(ts, 1, "coordinator_duty_group-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidCoordinatorTimeDataPoint(ts, 1, "coordinator_duty-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestBytesReceivedDataPoint(ts, 1, "ingest_task_type-val", "ingest_task_id-val", "ingest_data_source-val", "ingest_service_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestCountDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val", "ingest_task_ingestion_mode-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestEventsBufferedDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_service_name-val", "ingest_buffer_capacity-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestEventsDuplicateDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestEventsMessageGapDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestEventsProcessedDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestEventsProcessedWithErrorDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestEventsThrownAwayDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestEventsUnparseableDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestHandoffCountDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestHandoffFailedDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestHandoffTimeDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestInputBytesDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestKafkaAvgLagDataPoint(ts, 1, "ingest_tags-val", "ingest_stream-val", "ingest_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestKafkaLagDataPoint(ts, 1, "ingest_tags-val", "ingest_stream-val", "ingest_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestKafkaMaxLagDataPoint(ts, 1, "ingest_tags-val", "ingest_stream-val", "ingest_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestKafkaPartitionLagDataPoint(ts, 1, "ingest_tags-val", "ingest_partition-val", "ingest_stream-val", "ingest_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestKinesisAvgLagTimeDataPoint(ts, 1, "ingest_tags-val", "ingest_stream-val", "ingest_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestKinesisLagTimeDataPoint(ts, 1, "ingest_tags-val", "ingest_stream-val", "ingest_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestKinesisMaxLagTimeDataPoint(ts, 1, "ingest_tags-val", "ingest_stream-val", "ingest_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestKinesisPartitionLagTimeDataPoint(ts, 1, "ingest_tags-val", "ingest_partition-val", "ingest_stream-val", "ingest_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestMergeCPUDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestMergeTimeDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestNoticesQueueSizeDataPoint(ts, 1, "ingest_tags-val", "ingest_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestNoticesTimeDataPoint(ts, 1, "ingest_tags-val", "ingest_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestPauseTimeDataPoint(ts, 1, "ingest_tags-val", "ingest_task_id-val", "ingest_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestPersistsBackPressureDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestPersistsCountDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestPersistsCPUDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestPersistsFailedDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestPersistsTimeDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestRowsOutputDataPoint(ts, 1, "ingest_task_type-val", "ingest_task_id-val", "ingest_data_source-val", "ingest_group_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestSegmentsCountDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val", "ingest_task_ingestion_mode-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestShuffleBytesDataPoint(ts, 1, "ingest_supervisor_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestShuffleRequestsDataPoint(ts, 1, "ingest_supervisor_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestSinkCountDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIngestTombstonesCountDataPoint(ts, 1, "ingest_task_type-val", "ingest_data_source-val", "ingest_group_id-val", "ingest_tags-val", "ingest_task_id-val", "ingest_task_ingestion_mode-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIntervalCompactedCountDataPoint(ts, 1, "interval_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIntervalSkipCompactCountDataPoint(ts, 1, "interval_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidIntervalWaitCompactCountDataPoint(ts, 1, "interval_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJettyNumOpenConnectionsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJettyThreadPoolBusyDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJettyThreadPoolIdleDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJettyThreadPoolIsLowOnThreadsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJettyThreadPoolMaxDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJettyThreadPoolMinDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJettyThreadPoolQueueSizeDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJettyThreadPoolTotalDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmBufferpoolCapacityDataPoint(ts, 1, "jvm_bufferpool_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmBufferpoolCountDataPoint(ts, 1, "jvm_bufferpool_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmBufferpoolUsedDataPoint(ts, 1, "jvm_bufferpool_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmGcCountDataPoint(ts, 1, "jvm_gc_gen-val", "jvm_gc_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmGcCPUDataPoint(ts, 1, "jvm_gc_gen-val", "jvm_gc_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmMemCommittedDataPoint(ts, 1, "jvm_mem_kind-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmMemInitDataPoint(ts, 1, "jvm_mem_kind-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmMemMaxDataPoint(ts, 1, "jvm_mem_kind-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmMemUsedDataPoint(ts, 1, "jvm_mem_kind-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmPoolCommittedDataPoint(ts, 1, "jvm_pool_name-val", "jvm_pool_kind-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmPoolInitDataPoint(ts, 1, "jvm_pool_name-val", "jvm_pool_kind-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmPoolMaxDataPoint(ts, 1, "jvm_pool_name-val", "jvm_pool_kind-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidJvmPoolUsedDataPoint(ts, 1, "jvm_pool_name-val", "jvm_pool_kind-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidKillPendingSegmentsCountDataPoint(ts, 1, "kill_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidKillTaskCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidKillTaskAvailableSlotCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidKillTaskMaxSlotCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidMergeBufferPendingRequestsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidMetadataKillAuditCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidMetadataKillCompactionCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidMetadataKillDatasourceCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidMetadataKillRuleCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidMetadataKillSupervisorCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidMetadatacacheInitTimeDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidMetadatacacheRefreshCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidMetadatacacheRefreshTimeDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryByteLimitExceededCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryBytesDataPoint(ts, 1, "query_data_source-val", "query_num_metrics-val", "query_dimension-val", "query_has_filters-val", 15, 25, "query_type-val", "query_remote_address-val", "query_id-val", "query_context-val", "query_num_dimensions-val", "query_interval-val", "query_duration-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaAverageBytesDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaErrorsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaEvictionsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaHitRateDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaHitsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaMissesDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaNumEntriesDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaPutErrorDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaPutOkDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaPutOversizedDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaSizeBytesDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheDeltaTimeoutsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheMemcachedDeltaDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheMemcachedTotalDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalAverageBytesDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalErrorsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalEvictionsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalHitRateDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalHitsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalMissesDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalNumEntriesDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalPutErrorDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalPutOkDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalPutOversizedDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalSizeBytesDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCacheTotalTimeoutsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryCPUTimeDataPoint(ts, 1, "query_data_source-val", "query_num_metrics-val", "query_dimension-val", "query_has_filters-val", 15, 25, "query_type-val", "query_remote_address-val", "query_id-val", "query_context-val", "query_num_dimensions-val", "query_interval-val", "query_duration-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryFailedCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryInterruptedCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryNodeBackpressureDataPoint(ts, 1, "query_status-val", "query_server-val", "query_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryNodeBytesDataPoint(ts, 1, "query_status-val", "query_server-val", "query_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryNodeTimeDataPoint(ts, 1, "query_status-val", "query_server-val", "query_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryNodeTtfbDataPoint(ts, 1, "query_status-val", "query_server-val", "query_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryPriorityDataPoint(ts, 1, "query_type-val", "query_data_source-val", "query_lane-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryRowLimitExceededCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQuerySegmentTimeDataPoint(ts, 1, "query_status-val", "query_segment-val", "query_id-val", "query_vectorized-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQuerySegmentAndCacheTimeDataPoint(ts, 1, "query_segment-val", "query_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQuerySegmentsCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQuerySuccessCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryTimeDataPoint(ts, 1, "query_data_source-val", "query_num_metrics-val", "query_dimension-val", "query_has_filters-val", 15, 25, "query_type-val", "query_remote_address-val", "query_id-val", "query_context-val", "query_num_dimensions-val", "query_interval-val", "query_duration-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryTimeoutCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidQueryWaitTimeDataPoint(ts, 1, "query_segment-val", "query_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentAddedBytesDataPoint(ts, 1, "segment_task_type-val", "segment_data_source-val", "segment_group_id-val", "segment_tags-val", "segment_task_id-val", "segment_interval-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentAssignSkippedCountDataPoint(ts, 1, "segment_description-val", "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentAssignedCountDataPoint(ts, 1, "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentCompactedBytesDataPoint(ts, 1, "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentCompactedCountDataPoint(ts, 1, "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentCountDataPoint(ts, 1, "segment_priority-val", "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentDeletedCountDataPoint(ts, 1, "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentDropQueueCountDataPoint(ts, 1, "segment_server-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentDropSkippedCountDataPoint(ts, 1, "segment_description-val", "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentDroppedCountDataPoint(ts, 1, "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentLoadQueueAssignedDataPoint(ts, 1, "segment_server-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentLoadQueueCancelledDataPoint(ts, 1, "segment_server-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentLoadQueueCountDataPoint(ts, 1, "segment_server-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentLoadQueueFailedDataPoint(ts, 1, "segment_server-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentLoadQueueSizeDataPoint(ts, 1, "segment_server-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentLoadQueueSuccessDataPoint(ts, 1, "segment_server-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentMaxDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentMoveSkippedCountDataPoint(ts, 1, "segment_description-val", "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentMovedBytesDataPoint(ts, 1, "segment_task_type-val", "segment_data_source-val", "segment_group_id-val", "segment_tags-val", "segment_task_id-val", "segment_interval-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentMovedCountDataPoint(ts, 1, "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentNukedBytesDataPoint(ts, 1, "segment_task_type-val", "segment_data_source-val", "segment_group_id-val", "segment_tags-val", "segment_task_id-val", "segment_interval-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentOverShadowedCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentPendingDeleteDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentRowCountAvgDataPoint(ts, 1, "segment_priority-val", "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentRowCountRangeCountDataPoint(ts, 1, "segment_priority-val", "segment_tier-val", "segment_data_source-val", "segment_range-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentScanActiveDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentScanPendingDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentSizeDataPoint(ts, 1, "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentSkipCompactBytesDataPoint(ts, 1, "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentSkipCompactCountDataPoint(ts, 1, "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentUnavailableCountDataPoint(ts, 1, "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentUnderReplicatedCountDataPoint(ts, 1, "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentUnneededCountDataPoint(ts, 1, "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentUsedDataPoint(ts, 1, "segment_priority-val", "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentUsedPercentDataPoint(ts, 1, "segment_priority-val", "segment_tier-val", "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentWaitCompactBytesDataPoint(ts, 1, "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSegmentWaitCompactCountDataPoint(ts, 1, "segment_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidServerviewInitTimeDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidServerviewSyncHealthyDataPoint(ts, 1, "serverview_tier-val", "serverview_server-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidServerviewSyncUnstableTimeDataPoint(ts, 1, "serverview_tier-val", "serverview_server-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSQLQueryBytesDataPoint(ts, 1, "sqlQuery_data_source-val", "sqlQuery_native_query_ids-val", "sqlQuery_engine-val", "sqlQuery_remote_address-val", "sqlQuery_id-val", "sqlQuery_success-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSQLQueryPlanningTimeMsDataPoint(ts, 1, "sqlQuery_data_source-val", "sqlQuery_native_query_ids-val", "sqlQuery_engine-val", "sqlQuery_remote_address-val", "sqlQuery_id-val", "sqlQuery_success-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSQLQueryTimeDataPoint(ts, 1, "sqlQuery_data_source-val", "sqlQuery_native_query_ids-val", "sqlQuery_engine-val", "sqlQuery_remote_address-val", "sqlQuery_id-val", "sqlQuery_success-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSubqueryByteLimitCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSubqueryFallbackCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSubqueryFallbackInsufficientTypeCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSubqueryFallbackUnknownReasonCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSubqueryRowLimitCountDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysCPUDataPoint(ts, 1, "sys_cpu_time-val", "sys_cpu_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysDiskQueueDataPoint(ts, 1, "sys_disk_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysDiskReadCountDataPoint(ts, 1, "sys_disk_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysDiskReadSizeDataPoint(ts, 1, "sys_disk_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysDiskTransferTimeDataPoint(ts, 1, "sys_disk_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysDiskWriteCountDataPoint(ts, 1, "sys_disk_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysDiskWriteSizeDataPoint(ts, 1, "sys_disk_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysFsFilesCountDataPoint(ts, 1, "sys_fs_dir_name-val", "sys_fs_dev_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysFsFilesFreeDataPoint(ts, 1, "sys_fs_dir_name-val", "sys_fs_dev_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysFsMaxDataPoint(ts, 1, "sys_fs_dir_name-val", "sys_fs_dev_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysFsUsedDataPoint(ts, 1, "sys_fs_dir_name-val", "sys_fs_dev_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysLa1DataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysLa15DataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysLa5DataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysMemFreeDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysMemMaxDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysMemUsedDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysNetReadDroppedDataPoint(ts, 1, "sys_net_hwaddr-val", "sys_net_name-val", "sys_net_address-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysNetReadErrorsDataPoint(ts, 1, "sys_net_hwaddr-val", "sys_net_name-val", "sys_net_address-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysNetReadPacketsDataPoint(ts, 1, "sys_net_hwaddr-val", "sys_net_name-val", "sys_net_address-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysNetReadSizeDataPoint(ts, 1, "sys_net_hwaddr-val", "sys_net_name-val", "sys_net_address-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysNetWriteCollisionsDataPoint(ts, 1, "sys_net_hwaddr-val", "sys_net_name-val", "sys_net_address-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysNetWriteErrorsDataPoint(ts, 1, "sys_net_hwaddr-val", "sys_net_name-val", "sys_net_address-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysNetWritePacketsDataPoint(ts, 1, "sys_net_hwaddr-val", "sys_net_name-val", "sys_net_address-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysNetWriteSizeDataPoint(ts, 1, "sys_net_hwaddr-val", "sys_net_name-val", "sys_net_address-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysStorageUsedDataPoint(ts, 1, "sys_fs_dir_name-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysSwapFreeDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysSwapMaxDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysSwapPageInDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysSwapPageOutDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysTcpv4ActiveOpensDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysTcpv4AttemptFailsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysTcpv4EstabResetsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysTcpv4InErrsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysTcpv4InSegsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysTcpv4OutRstsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysTcpv4OutSegsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysTcpv4PassiveOpensDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysTcpv4RetransSegsDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidSysUptimeDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskActionBatchAttemptsDataPoint(ts, 1, "task_interval-val", "task_data_source-val", "task_action_type-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskActionBatchQueueTimeDataPoint(ts, 1, "task_interval-val", "task_data_source-val", "task_action_type-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskActionBatchRunTimeDataPoint(ts, 1, "task_interval-val", "task_data_source-val", "task_action_type-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskActionBatchSizeDataPoint(ts, 1, "task_interval-val", "task_data_source-val", "task_action_type-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskActionFailedCountDataPoint(ts, 1, "task_type-val", "task_data_source-val", "task_action_type-val", "task_group_id-val", "task_tags-val", "task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskActionLogTimeDataPoint(ts, 1, "task_type-val", "task_data_source-val", "task_action_type-val", "task_group_id-val", "task_tags-val", "task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskActionRunTimeDataPoint(ts, 1, "task_type-val", "task_data_source-val", "task_action_type-val", "task_group_id-val", "task_tags-val", "task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskActionSuccessCountDataPoint(ts, 1, "task_type-val", "task_data_source-val", "task_action_type-val", "task_group_id-val", "task_tags-val", "task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskFailedCountDataPoint(ts, 1, "task_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskPendingCountDataPoint(ts, 1, "task_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskPendingTimeDataPoint(ts, 1, "task_type-val", "task_data_source-val", "task_group_id-val", "task_tags-val", "task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskRunTimeDataPoint(ts, 1, "task_type-val", "task_data_source-val", "task_group_id-val", "task_status-val", "task_tags-val", "task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskRunningCountDataPoint(ts, 1, "task_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskSegmentAvailabilityWaitTimeDataPoint(ts, 1, "task_type-val", "task_data_source-val", "task_group_id-val", "task_segment_availability_confirmed-val", "task_tags-val", "task_id-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskSuccessCountDataPoint(ts, 1, "task_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskWaitingCountDataPoint(ts, 1, "task_data_source-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskSlotBlacklistedCountDataPoint(ts, 1, "taskSlot_category-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskSlotIdleCountDataPoint(ts, 1, "taskSlot_category-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskSlotLazyCountDataPoint(ts, 1, "taskSlot_category-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskSlotTotalCountDataPoint(ts, 1, "taskSlot_category-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTaskSlotUsedCountDataPoint(ts, 1, "taskSlot_category-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTierHistoricalCountDataPoint(ts, 1, "tier-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTierReplicationFactorDataPoint(ts, 1, "tier-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTierRequiredCapacityDataPoint(ts, 1, "tier-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidTierTotalCapacityDataPoint(ts, 1, "tier-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidWorkerTaskFailedCountDataPoint(ts, 1, "worker_category-val", "worker_version-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidWorkerTaskSuccessCountDataPoint(ts, 1, "worker_category-val", "worker_version-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidWorkerTaskSlotIdleCountDataPoint(ts, 1, "worker_category-val", "worker_version-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidWorkerTaskSlotTotalCountDataPoint(ts, 1, "worker_category-val", "worker_version-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidWorkerTaskSlotUsedCountDataPoint(ts, 1, "worker_category-val", "worker_version-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidZkConnectedDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordApachedruidZkReconnectTimeDataPoint(ts, 1)

			rb := mb.NewResourceBuilder()
			rb.SetApachedruidClusterName("apachedruid.cluster.name-val")
			rb.SetApachedruidNodeHost("apachedruid.node.host-val")
			rb.SetApachedruidNodeService("apachedruid.node.service-val")
			res := rb.Emit()
			metrics := mb.Emit(WithResource(res))

			if test.configSet == testSetNone {
				assert.Equal(t, 0, metrics.ResourceMetrics().Len())
				return
			}

			assert.Equal(t, 1, metrics.ResourceMetrics().Len())
			rm := metrics.ResourceMetrics().At(0)
			assert.Equal(t, res, rm.Resource())
			assert.Equal(t, 1, rm.ScopeMetrics().Len())
			ms := rm.ScopeMetrics().At(0).Metrics()
			if test.configSet == testSetDefault {
				assert.Equal(t, defaultMetricsCount, ms.Len())
			}
			if test.configSet == testSetAll {
				assert.Equal(t, allMetricsCount, ms.Len())
			}
			validatedMetrics := make(map[string]bool)
			for i := 0; i < ms.Len(); i++ {
				switch ms.At(i).Name() {
				case "apachedruid.compact.segment_analyzer.fetch_and_process_millis":
					assert.False(t, validatedMetrics["apachedruid.compact.segment_analyzer.fetch_and_process_millis"], "Found a duplicate in the metrics slice: apachedruid.compact.segment_analyzer.fetch_and_process_millis")
					validatedMetrics["apachedruid.compact.segment_analyzer.fetch_and_process_millis"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Time taken to fetch and process segments to infer the schema for the compaction task to run.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "compact_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "compact_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "compact_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "compact_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "compact_task_id-val", attrVal.Str())
				case "apachedruid.compact.task.count":
					assert.False(t, validatedMetrics["apachedruid.compact.task.count"], "Found a duplicate in the metrics slice: apachedruid.compact.task.count")
					validatedMetrics["apachedruid.compact.task.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of tasks issued in the auto compaction run.", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.compact_task.available_slot.count":
					assert.False(t, validatedMetrics["apachedruid.compact_task.available_slot.count"], "Found a duplicate in the metrics slice: apachedruid.compact_task.available_slot.count")
					validatedMetrics["apachedruid.compact_task.available_slot.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of available task slots that can be used for auto compaction tasks in the auto compaction run. This is the max number of task slots minus any currently running compaction tasks.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.compact_task.max_slot.count":
					assert.False(t, validatedMetrics["apachedruid.compact_task.max_slot.count"], "Found a duplicate in the metrics slice: apachedruid.compact_task.max_slot.count")
					validatedMetrics["apachedruid.compact_task.max_slot.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Maximum number of task slots available for auto compaction tasks in the auto compaction run.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.coordinator.global.time":
					assert.False(t, validatedMetrics["apachedruid.coordinator.global.time"], "Found a duplicate in the metrics slice: apachedruid.coordinator.global.time")
					validatedMetrics["apachedruid.coordinator.global.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Approximate runtime of a full coordination cycle in milliseconds. The `dutyGroup` dimension indicates what type of coordination this run was. For example, Historical Management or Indexing.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("duty_group")
					assert.True(t, ok)
					assert.EqualValues(t, "coordinator_duty_group-val", attrVal.Str())
				case "apachedruid.coordinator.time":
					assert.False(t, validatedMetrics["apachedruid.coordinator.time"], "Found a duplicate in the metrics slice: apachedruid.coordinator.time")
					validatedMetrics["apachedruid.coordinator.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Approximate Coordinator duty runtime in milliseconds.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("duty")
					assert.True(t, ok)
					assert.EqualValues(t, "coordinator_duty-val", attrVal.Str())
				case "apachedruid.ingest.bytes.received":
					assert.False(t, validatedMetrics["apachedruid.ingest.bytes.received"], "Found a duplicate in the metrics slice: apachedruid.ingest.bytes.received")
					validatedMetrics["apachedruid.ingest.bytes.received"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of bytes received by the `EventReceiverFirehose`.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("service_name")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_service_name-val", attrVal.Str())
				case "apachedruid.ingest.count":
					assert.False(t, validatedMetrics["apachedruid.ingest.count"], "Found a duplicate in the metrics slice: apachedruid.ingest.count")
					validatedMetrics["apachedruid.ingest.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Count of `1` every time an ingestion job runs (includes compaction jobs). Aggregate using dimensions.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_ingestion_mode")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_ingestion_mode-val", attrVal.Str())
				case "apachedruid.ingest.events.buffered":
					assert.False(t, validatedMetrics["apachedruid.ingest.events.buffered"], "Found a duplicate in the metrics slice: apachedruid.ingest.events.buffered")
					validatedMetrics["apachedruid.ingest.events.buffered"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of events queued in the `EventReceiverFirehose` buffer.", ms.At(i).Description())
					assert.Equal(t, "{events}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("service_name")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_service_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("buffer_capacity")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_buffer_capacity-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.events.duplicate":
					assert.False(t, validatedMetrics["apachedruid.ingest.events.duplicate"], "Found a duplicate in the metrics slice: apachedruid.ingest.events.duplicate")
					validatedMetrics["apachedruid.ingest.events.duplicate"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of events rejected because the events are duplicated.", ms.At(i).Description())
					assert.Equal(t, "{events}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.events.message_gap":
					assert.False(t, validatedMetrics["apachedruid.ingest.events.message_gap"], "Found a duplicate in the metrics slice: apachedruid.ingest.events.message_gap")
					validatedMetrics["apachedruid.ingest.events.message_gap"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Time gap in milliseconds between the latest ingested event timestamp and the current system timestamp of metrics emission. If the value is increasing but lag is low, Druid may not be receiving new data. This metric is reset as new tasks spawn up.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.events.processed":
					assert.False(t, validatedMetrics["apachedruid.ingest.events.processed"], "Found a duplicate in the metrics slice: apachedruid.ingest.events.processed")
					validatedMetrics["apachedruid.ingest.events.processed"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of events processed per emission period.", ms.At(i).Description())
					assert.Equal(t, "{events}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.events.processed_with_error":
					assert.False(t, validatedMetrics["apachedruid.ingest.events.processed_with_error"], "Found a duplicate in the metrics slice: apachedruid.ingest.events.processed_with_error")
					validatedMetrics["apachedruid.ingest.events.processed_with_error"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of events processed with some partial errors per emission period. Events processed with partial errors are counted towards both this metric and `ingest/events/processed`.", ms.At(i).Description())
					assert.Equal(t, "{events}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.events.thrown_away":
					assert.False(t, validatedMetrics["apachedruid.ingest.events.thrown_away"], "Found a duplicate in the metrics slice: apachedruid.ingest.events.thrown_away")
					validatedMetrics["apachedruid.ingest.events.thrown_away"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of events rejected because they are null, or filtered by `transformSpec`, or outside one of `lateMessageRejectionPeriod`, `earlyMessageRejectionPeriod`, or `windowPeriod`.", ms.At(i).Description())
					assert.Equal(t, "{events}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.events.unparseable":
					assert.False(t, validatedMetrics["apachedruid.ingest.events.unparseable"], "Found a duplicate in the metrics slice: apachedruid.ingest.events.unparseable")
					validatedMetrics["apachedruid.ingest.events.unparseable"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of events rejected because the events are unparseable.", ms.At(i).Description())
					assert.Equal(t, "{events}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.handoff.count":
					assert.False(t, validatedMetrics["apachedruid.ingest.handoff.count"], "Found a duplicate in the metrics slice: apachedruid.ingest.handoff.count")
					validatedMetrics["apachedruid.ingest.handoff.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of handoffs that happened.", ms.At(i).Description())
					assert.Equal(t, "{handoffs}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.handoff.failed":
					assert.False(t, validatedMetrics["apachedruid.ingest.handoff.failed"], "Found a duplicate in the metrics slice: apachedruid.ingest.handoff.failed")
					validatedMetrics["apachedruid.ingest.handoff.failed"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of handoffs that failed.", ms.At(i).Description())
					assert.Equal(t, "{handoffs}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.handoff.time":
					assert.False(t, validatedMetrics["apachedruid.ingest.handoff.time"], "Found a duplicate in the metrics slice: apachedruid.ingest.handoff.time")
					validatedMetrics["apachedruid.ingest.handoff.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total number of milliseconds taken to handoff a set of segments.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.input.bytes":
					assert.False(t, validatedMetrics["apachedruid.ingest.input.bytes"], "Found a duplicate in the metrics slice: apachedruid.ingest.input.bytes")
					validatedMetrics["apachedruid.ingest.input.bytes"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of bytes read from input sources, after decompression but prior to parsing. This covers all data read, including data that does not end up being fully processed and ingested. For example, this includes data that ends up being rejected for being unparseable or filtered out.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.kafka.avg_lag":
					assert.False(t, validatedMetrics["apachedruid.ingest.kafka.avg_lag"], "Found a duplicate in the metrics slice: apachedruid.ingest.kafka.avg_lag")
					validatedMetrics["apachedruid.ingest.kafka.avg_lag"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Average lag between the offsets consumed by the Kafka indexing tasks and latest offsets in Kafka brokers across all partitions. Minimum emission period for this metric is a minute.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("stream")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_stream-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
				case "apachedruid.ingest.kafka.lag":
					assert.False(t, validatedMetrics["apachedruid.ingest.kafka.lag"], "Found a duplicate in the metrics slice: apachedruid.ingest.kafka.lag")
					validatedMetrics["apachedruid.ingest.kafka.lag"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total lag between the offsets consumed by the Kafka indexing tasks and latest offsets in Kafka brokers across all partitions. Minimum emission period for this metric is a minute.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("stream")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_stream-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
				case "apachedruid.ingest.kafka.max_lag":
					assert.False(t, validatedMetrics["apachedruid.ingest.kafka.max_lag"], "Found a duplicate in the metrics slice: apachedruid.ingest.kafka.max_lag")
					validatedMetrics["apachedruid.ingest.kafka.max_lag"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Max lag between the offsets consumed by the Kafka indexing tasks and latest offsets in Kafka brokers across all partitions. Minimum emission period for this metric is a minute.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("stream")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_stream-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
				case "apachedruid.ingest.kafka.partition_lag":
					assert.False(t, validatedMetrics["apachedruid.ingest.kafka.partition_lag"], "Found a duplicate in the metrics slice: apachedruid.ingest.kafka.partition_lag")
					validatedMetrics["apachedruid.ingest.kafka.partition_lag"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Partition-wise lag between the offsets consumed by the Kafka indexing tasks and latest offsets in Kafka brokers. Minimum emission period for this metric is a minute.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("partition")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_partition-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("stream")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_stream-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
				case "apachedruid.ingest.kinesis.avg_lag.time":
					assert.False(t, validatedMetrics["apachedruid.ingest.kinesis.avg_lag.time"], "Found a duplicate in the metrics slice: apachedruid.ingest.kinesis.avg_lag.time")
					validatedMetrics["apachedruid.ingest.kinesis.avg_lag.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Average lag time in milliseconds between the current message sequence number consumed by the Kinesis indexing tasks and latest sequence number in Kinesis across all shards. Minimum emission period for this metric is a minute.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("stream")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_stream-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
				case "apachedruid.ingest.kinesis.lag.time":
					assert.False(t, validatedMetrics["apachedruid.ingest.kinesis.lag.time"], "Found a duplicate in the metrics slice: apachedruid.ingest.kinesis.lag.time")
					validatedMetrics["apachedruid.ingest.kinesis.lag.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total lag time in milliseconds between the current message sequence number consumed by the Kinesis indexing tasks and latest sequence number in Kinesis across all shards. Minimum emission period for this metric is a minute.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("stream")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_stream-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
				case "apachedruid.ingest.kinesis.max_lag.time":
					assert.False(t, validatedMetrics["apachedruid.ingest.kinesis.max_lag.time"], "Found a duplicate in the metrics slice: apachedruid.ingest.kinesis.max_lag.time")
					validatedMetrics["apachedruid.ingest.kinesis.max_lag.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Max lag time in milliseconds between the current message sequence number consumed by the Kinesis indexing tasks and latest sequence number in Kinesis across all shards. Minimum emission period for this metric is a minute.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("stream")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_stream-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
				case "apachedruid.ingest.kinesis.partition_lag.time":
					assert.False(t, validatedMetrics["apachedruid.ingest.kinesis.partition_lag.time"], "Found a duplicate in the metrics slice: apachedruid.ingest.kinesis.partition_lag.time")
					validatedMetrics["apachedruid.ingest.kinesis.partition_lag.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Partition-wise lag time in milliseconds between the current message sequence number consumed by the Kinesis indexing tasks and latest sequence number in Kinesis. Minimum emission period for this metric is a minute.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("partition")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_partition-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("stream")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_stream-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
				case "apachedruid.ingest.merge.cpu":
					assert.False(t, validatedMetrics["apachedruid.ingest.merge.cpu"], "Found a duplicate in the metrics slice: apachedruid.ingest.merge.cpu")
					validatedMetrics["apachedruid.ingest.merge.cpu"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "CPU time in Nanoseconds spent on merging intermediate segments.", ms.At(i).Description())
					assert.Equal(t, "ns", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.merge.time":
					assert.False(t, validatedMetrics["apachedruid.ingest.merge.time"], "Found a duplicate in the metrics slice: apachedruid.ingest.merge.time")
					validatedMetrics["apachedruid.ingest.merge.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Milliseconds spent merging intermediate segments.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.notices.queue_size":
					assert.False(t, validatedMetrics["apachedruid.ingest.notices.queue_size"], "Found a duplicate in the metrics slice: apachedruid.ingest.notices.queue_size")
					validatedMetrics["apachedruid.ingest.notices.queue_size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of pending notices to be processed by the coordinator.", ms.At(i).Description())
					assert.Equal(t, "{notices}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
				case "apachedruid.ingest.notices.time":
					assert.False(t, validatedMetrics["apachedruid.ingest.notices.time"], "Found a duplicate in the metrics slice: apachedruid.ingest.notices.time")
					validatedMetrics["apachedruid.ingest.notices.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken to process a notice by the supervisor.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
				case "apachedruid.ingest.pause.time":
					assert.False(t, validatedMetrics["apachedruid.ingest.pause.time"], "Found a duplicate in the metrics slice: apachedruid.ingest.pause.time")
					validatedMetrics["apachedruid.ingest.pause.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds spent by a task in a paused state without ingesting.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
				case "apachedruid.ingest.persists.back_pressure":
					assert.False(t, validatedMetrics["apachedruid.ingest.persists.back_pressure"], "Found a duplicate in the metrics slice: apachedruid.ingest.persists.back_pressure")
					validatedMetrics["apachedruid.ingest.persists.back_pressure"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Milliseconds spent creating persist tasks and blocking waiting for them to finish.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.persists.count":
					assert.False(t, validatedMetrics["apachedruid.ingest.persists.count"], "Found a duplicate in the metrics slice: apachedruid.ingest.persists.count")
					validatedMetrics["apachedruid.ingest.persists.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of times persist occurred.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.persists.cpu":
					assert.False(t, validatedMetrics["apachedruid.ingest.persists.cpu"], "Found a duplicate in the metrics slice: apachedruid.ingest.persists.cpu")
					validatedMetrics["apachedruid.ingest.persists.cpu"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "CPU time in nanoseconds spent on doing intermediate persist.", ms.At(i).Description())
					assert.Equal(t, "ns", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.persists.failed":
					assert.False(t, validatedMetrics["apachedruid.ingest.persists.failed"], "Found a duplicate in the metrics slice: apachedruid.ingest.persists.failed")
					validatedMetrics["apachedruid.ingest.persists.failed"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of persists that failed.", ms.At(i).Description())
					assert.Equal(t, "{persists}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.persists.time":
					assert.False(t, validatedMetrics["apachedruid.ingest.persists.time"], "Found a duplicate in the metrics slice: apachedruid.ingest.persists.time")
					validatedMetrics["apachedruid.ingest.persists.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Milliseconds spent doing intermediate persist.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.rows.output":
					assert.False(t, validatedMetrics["apachedruid.ingest.rows.output"], "Found a duplicate in the metrics slice: apachedruid.ingest.rows.output")
					validatedMetrics["apachedruid.ingest.rows.output"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of Druid rows persisted.", ms.At(i).Description())
					assert.Equal(t, "{rows}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
				case "apachedruid.ingest.segments.count":
					assert.False(t, validatedMetrics["apachedruid.ingest.segments.count"], "Found a duplicate in the metrics slice: apachedruid.ingest.segments.count")
					validatedMetrics["apachedruid.ingest.segments.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Count of final segments created by job (includes tombstones).", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_ingestion_mode")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_ingestion_mode-val", attrVal.Str())
				case "apachedruid.ingest.shuffle.bytes":
					assert.False(t, validatedMetrics["apachedruid.ingest.shuffle.bytes"], "Found a duplicate in the metrics slice: apachedruid.ingest.shuffle.bytes")
					validatedMetrics["apachedruid.ingest.shuffle.bytes"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of bytes shuffled per emission period.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("supervisor_task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_supervisor_task_id-val", attrVal.Str())
				case "apachedruid.ingest.shuffle.requests":
					assert.False(t, validatedMetrics["apachedruid.ingest.shuffle.requests"], "Found a duplicate in the metrics slice: apachedruid.ingest.shuffle.requests")
					validatedMetrics["apachedruid.ingest.shuffle.requests"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of shuffle requests per emission period.", ms.At(i).Description())
					assert.Equal(t, "{requests}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("supervisor_task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_supervisor_task_id-val", attrVal.Str())
				case "apachedruid.ingest.sink.count":
					assert.False(t, validatedMetrics["apachedruid.ingest.sink.count"], "Found a duplicate in the metrics slice: apachedruid.ingest.sink.count")
					validatedMetrics["apachedruid.ingest.sink.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of sinks not handed off.", ms.At(i).Description())
					assert.Equal(t, "{sinks}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
				case "apachedruid.ingest.tombstones.count":
					assert.False(t, validatedMetrics["apachedruid.ingest.tombstones.count"], "Found a duplicate in the metrics slice: apachedruid.ingest.tombstones.count")
					validatedMetrics["apachedruid.ingest.tombstones.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Count of tombstones created by job.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_ingestion_mode")
					assert.True(t, ok)
					assert.EqualValues(t, "ingest_task_ingestion_mode-val", attrVal.Str())
				case "apachedruid.interval.compacted.count":
					assert.False(t, validatedMetrics["apachedruid.interval.compacted.count"], "Found a duplicate in the metrics slice: apachedruid.interval.compacted.count")
					validatedMetrics["apachedruid.interval.compacted.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Total number of intervals of this datasource that are already compacted with the spec set in the auto compaction config.", ms.At(i).Description())
					assert.Equal(t, "{intervals}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "interval_data_source-val", attrVal.Str())
				case "apachedruid.interval.skip_compact.count":
					assert.False(t, validatedMetrics["apachedruid.interval.skip_compact.count"], "Found a duplicate in the metrics slice: apachedruid.interval.skip_compact.count")
					validatedMetrics["apachedruid.interval.skip_compact.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total number of intervals of this datasource that are skipped (not eligible for auto compaction) by the auto compaction.", ms.At(i).Description())
					assert.Equal(t, "{intervals}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "interval_data_source-val", attrVal.Str())
				case "apachedruid.interval.wait_compact.count":
					assert.False(t, validatedMetrics["apachedruid.interval.wait_compact.count"], "Found a duplicate in the metrics slice: apachedruid.interval.wait_compact.count")
					validatedMetrics["apachedruid.interval.wait_compact.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total number of intervals of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction).", ms.At(i).Description())
					assert.Equal(t, "{intervals}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "interval_data_source-val", attrVal.Str())
				case "apachedruid.jetty.num_open_connections":
					assert.False(t, validatedMetrics["apachedruid.jetty.num_open_connections"], "Found a duplicate in the metrics slice: apachedruid.jetty.num_open_connections")
					validatedMetrics["apachedruid.jetty.num_open_connections"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of open jetty connections.", ms.At(i).Description())
					assert.Equal(t, "{connections}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.jetty.thread_pool.busy":
					assert.False(t, validatedMetrics["apachedruid.jetty.thread_pool.busy"], "Found a duplicate in the metrics slice: apachedruid.jetty.thread_pool.busy")
					validatedMetrics["apachedruid.jetty.thread_pool.busy"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of busy threads that has work to do from the worker queue.", ms.At(i).Description())
					assert.Equal(t, "{threads}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.jetty.thread_pool.idle":
					assert.False(t, validatedMetrics["apachedruid.jetty.thread_pool.idle"], "Found a duplicate in the metrics slice: apachedruid.jetty.thread_pool.idle")
					validatedMetrics["apachedruid.jetty.thread_pool.idle"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of idle threads.", ms.At(i).Description())
					assert.Equal(t, "{threads}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.jetty.thread_pool.is_low_on_threads":
					assert.False(t, validatedMetrics["apachedruid.jetty.thread_pool.is_low_on_threads"], "Found a duplicate in the metrics slice: apachedruid.jetty.thread_pool.is_low_on_threads")
					validatedMetrics["apachedruid.jetty.thread_pool.is_low_on_threads"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "A rough indicator of whether number of total workable threads allocated is enough to handle the works in the work queue.", ms.At(i).Description())
					assert.Equal(t, "{threads}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.jetty.thread_pool.max":
					assert.False(t, validatedMetrics["apachedruid.jetty.thread_pool.max"], "Found a duplicate in the metrics slice: apachedruid.jetty.thread_pool.max")
					validatedMetrics["apachedruid.jetty.thread_pool.max"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of maximum threads allocatable.", ms.At(i).Description())
					assert.Equal(t, "{threads}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.jetty.thread_pool.min":
					assert.False(t, validatedMetrics["apachedruid.jetty.thread_pool.min"], "Found a duplicate in the metrics slice: apachedruid.jetty.thread_pool.min")
					validatedMetrics["apachedruid.jetty.thread_pool.min"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of minimum threads allocatable.", ms.At(i).Description())
					assert.Equal(t, "{threads}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.jetty.thread_pool.queue_size":
					assert.False(t, validatedMetrics["apachedruid.jetty.thread_pool.queue_size"], "Found a duplicate in the metrics slice: apachedruid.jetty.thread_pool.queue_size")
					validatedMetrics["apachedruid.jetty.thread_pool.queue_size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Size of the worker queue.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.jetty.thread_pool.total":
					assert.False(t, validatedMetrics["apachedruid.jetty.thread_pool.total"], "Found a duplicate in the metrics slice: apachedruid.jetty.thread_pool.total")
					validatedMetrics["apachedruid.jetty.thread_pool.total"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of total workable threads allocated.", ms.At(i).Description())
					assert.Equal(t, "{threads}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.jvm.bufferpool.capacity":
					assert.False(t, validatedMetrics["apachedruid.jvm.bufferpool.capacity"], "Found a duplicate in the metrics slice: apachedruid.jvm.bufferpool.capacity")
					validatedMetrics["apachedruid.jvm.bufferpool.capacity"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Bufferpool capacity.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("bufferpool_name")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_bufferpool_name-val", attrVal.Str())
				case "apachedruid.jvm.bufferpool.count":
					assert.False(t, validatedMetrics["apachedruid.jvm.bufferpool.count"], "Found a duplicate in the metrics slice: apachedruid.jvm.bufferpool.count")
					validatedMetrics["apachedruid.jvm.bufferpool.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Bufferpool count.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("bufferpool_name")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_bufferpool_name-val", attrVal.Str())
				case "apachedruid.jvm.bufferpool.used":
					assert.False(t, validatedMetrics["apachedruid.jvm.bufferpool.used"], "Found a duplicate in the metrics slice: apachedruid.jvm.bufferpool.used")
					validatedMetrics["apachedruid.jvm.bufferpool.used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Bufferpool used.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("bufferpool_name")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_bufferpool_name-val", attrVal.Str())
				case "apachedruid.jvm.gc.count":
					assert.False(t, validatedMetrics["apachedruid.jvm.gc.count"], "Found a duplicate in the metrics slice: apachedruid.jvm.gc.count")
					validatedMetrics["apachedruid.jvm.gc.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Garbage collection count.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("gc_gen")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_gc_gen-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("gc_name")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_gc_name-val", attrVal.Str())
				case "apachedruid.jvm.gc.cpu":
					assert.False(t, validatedMetrics["apachedruid.jvm.gc.cpu"], "Found a duplicate in the metrics slice: apachedruid.jvm.gc.cpu")
					validatedMetrics["apachedruid.jvm.gc.cpu"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Count of CPU time in Nanoseconds spent on garbage collection. Note, `jvm/gc/cpu` represents the total time over multiple GC cycles; divide by `jvm/gc/count` to get the mean GC time per cycle.", ms.At(i).Description())
					assert.Equal(t, "ns", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("gc_gen")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_gc_gen-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("gc_name")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_gc_name-val", attrVal.Str())
				case "apachedruid.jvm.mem.committed":
					assert.False(t, validatedMetrics["apachedruid.jvm.mem.committed"], "Found a duplicate in the metrics slice: apachedruid.jvm.mem.committed")
					validatedMetrics["apachedruid.jvm.mem.committed"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Committed memory.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("mem_kind")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_mem_kind-val", attrVal.Str())
				case "apachedruid.jvm.mem.init":
					assert.False(t, validatedMetrics["apachedruid.jvm.mem.init"], "Found a duplicate in the metrics slice: apachedruid.jvm.mem.init")
					validatedMetrics["apachedruid.jvm.mem.init"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Initial memory.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("mem_kind")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_mem_kind-val", attrVal.Str())
				case "apachedruid.jvm.mem.max":
					assert.False(t, validatedMetrics["apachedruid.jvm.mem.max"], "Found a duplicate in the metrics slice: apachedruid.jvm.mem.max")
					validatedMetrics["apachedruid.jvm.mem.max"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Max memory.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("mem_kind")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_mem_kind-val", attrVal.Str())
				case "apachedruid.jvm.mem.used":
					assert.False(t, validatedMetrics["apachedruid.jvm.mem.used"], "Found a duplicate in the metrics slice: apachedruid.jvm.mem.used")
					validatedMetrics["apachedruid.jvm.mem.used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Used memory.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("mem_kind")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_mem_kind-val", attrVal.Str())
				case "apachedruid.jvm.pool.committed":
					assert.False(t, validatedMetrics["apachedruid.jvm.pool.committed"], "Found a duplicate in the metrics slice: apachedruid.jvm.pool.committed")
					validatedMetrics["apachedruid.jvm.pool.committed"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Committed pool.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("pool_name")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_pool_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pool_kind")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_pool_kind-val", attrVal.Str())
				case "apachedruid.jvm.pool.init":
					assert.False(t, validatedMetrics["apachedruid.jvm.pool.init"], "Found a duplicate in the metrics slice: apachedruid.jvm.pool.init")
					validatedMetrics["apachedruid.jvm.pool.init"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Initial pool.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("pool_name")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_pool_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pool_kind")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_pool_kind-val", attrVal.Str())
				case "apachedruid.jvm.pool.max":
					assert.False(t, validatedMetrics["apachedruid.jvm.pool.max"], "Found a duplicate in the metrics slice: apachedruid.jvm.pool.max")
					validatedMetrics["apachedruid.jvm.pool.max"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Max pool.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("pool_name")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_pool_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pool_kind")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_pool_kind-val", attrVal.Str())
				case "apachedruid.jvm.pool.used":
					assert.False(t, validatedMetrics["apachedruid.jvm.pool.used"], "Found a duplicate in the metrics slice: apachedruid.jvm.pool.used")
					validatedMetrics["apachedruid.jvm.pool.used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Pool used.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("pool_name")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_pool_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pool_kind")
					assert.True(t, ok)
					assert.EqualValues(t, "jvm_pool_kind-val", attrVal.Str())
				case "apachedruid.kill.pending_segments.count":
					assert.False(t, validatedMetrics["apachedruid.kill.pending_segments.count"], "Found a duplicate in the metrics slice: apachedruid.kill.pending_segments.count")
					validatedMetrics["apachedruid.kill.pending_segments.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of stale pending segments deleted from the metadata store.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "kill_data_source-val", attrVal.Str())
				case "apachedruid.kill.task.count":
					assert.False(t, validatedMetrics["apachedruid.kill.task.count"], "Found a duplicate in the metrics slice: apachedruid.kill.task.count")
					validatedMetrics["apachedruid.kill.task.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of tasks issued in the auto kill run.", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.kill_task.available_slot.count":
					assert.False(t, validatedMetrics["apachedruid.kill_task.available_slot.count"], "Found a duplicate in the metrics slice: apachedruid.kill_task.available_slot.count")
					validatedMetrics["apachedruid.kill_task.available_slot.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of available task slots that can be used for auto kill tasks in the auto kill run. This is the max number of task slots minus any currently running auto kill tasks.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.kill_task.max_slot.count":
					assert.False(t, validatedMetrics["apachedruid.kill_task.max_slot.count"], "Found a duplicate in the metrics slice: apachedruid.kill_task.max_slot.count")
					validatedMetrics["apachedruid.kill_task.max_slot.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Maximum number of task slots available for auto kill tasks in the auto kill run.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.merge_buffer.pending_requests":
					assert.False(t, validatedMetrics["apachedruid.merge_buffer.pending_requests"], "Found a duplicate in the metrics slice: apachedruid.merge_buffer.pending_requests")
					validatedMetrics["apachedruid.merge_buffer.pending_requests"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of requests waiting to acquire a batch of buffers from the merge buffer pool.", ms.At(i).Description())
					assert.Equal(t, "{requests}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.metadata.kill.audit.count":
					assert.False(t, validatedMetrics["apachedruid.metadata.kill.audit.count"], "Found a duplicate in the metrics slice: apachedruid.metadata.kill.audit.count")
					validatedMetrics["apachedruid.metadata.kill.audit.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Total number of audit logs that were automatically deleted from metadata store per each Coordinator kill audit duty run. This metric can help adjust `druid.coordinator.kill.audit.durationToRetain` configuration based on whether more or less audit logs need to be deleted per cycle. This metric is emitted only when `druid.coordinator.kill.audit.on` is set to true.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.metadata.kill.compaction.count":
					assert.False(t, validatedMetrics["apachedruid.metadata.kill.compaction.count"], "Found a duplicate in the metrics slice: apachedruid.metadata.kill.compaction.count")
					validatedMetrics["apachedruid.metadata.kill.compaction.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total number of compaction configurations that were automatically deleted from metadata store per each Coordinator kill compaction configuration duty run. This metric is only emitted when `druid.coordinator.kill.compaction.on` is set to true.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.metadata.kill.datasource.count":
					assert.False(t, validatedMetrics["apachedruid.metadata.kill.datasource.count"], "Found a duplicate in the metrics slice: apachedruid.metadata.kill.datasource.count")
					validatedMetrics["apachedruid.metadata.kill.datasource.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total number of datasource metadata that were automatically deleted from metadata store per each Coordinator kill datasource duty run. Note that datasource metadata only exists for datasource created from supervisor. This metric can help adjust `druid.coordinator.kill.datasource.durationToRetain` configuration based on whether more or less datasource metadata need to be deleted per cycle. This metric is only emitted when `druid.coordinator.kill.datasource.on` is set to true.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.metadata.kill.rule.count":
					assert.False(t, validatedMetrics["apachedruid.metadata.kill.rule.count"], "Found a duplicate in the metrics slice: apachedruid.metadata.kill.rule.count")
					validatedMetrics["apachedruid.metadata.kill.rule.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total number of rules that were automatically deleted from metadata store per each Coordinator kill rule duty run. This metric can help adjust `druid.coordinator.kill.rule.durationToRetain` configuration based on whether more or less rules need to be deleted per cycle. This metric is only emitted when `druid.coordinator.kill.rule.on` is set to true.", ms.At(i).Description())
					assert.Equal(t, "{rules}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.metadata.kill.supervisor.count":
					assert.False(t, validatedMetrics["apachedruid.metadata.kill.supervisor.count"], "Found a duplicate in the metrics slice: apachedruid.metadata.kill.supervisor.count")
					validatedMetrics["apachedruid.metadata.kill.supervisor.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total number of terminated supervisors that were automatically deleted from metadata store per each Coordinator kill supervisor duty run. This metric can help adjust `druid.coordinator.kill.supervisor.durationToRetain` configuration based on whether more or less terminated supervisors need to be deleted per cycle. This metric is only emitted when `druid.coordinator.kill.supervisor.on` is set to true.", ms.At(i).Description())
					assert.Equal(t, "{supervisors}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.metadatacache.init.time":
					assert.False(t, validatedMetrics["apachedruid.metadatacache.init.time"], "Found a duplicate in the metrics slice: apachedruid.metadatacache.init.time")
					validatedMetrics["apachedruid.metadatacache.init.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Time taken to initialize the broker segment metadata cache. Useful to detect if brokers are taking too long to start.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.metadatacache.refresh.count":
					assert.False(t, validatedMetrics["apachedruid.metadatacache.refresh.count"], "Found a duplicate in the metrics slice: apachedruid.metadatacache.refresh.count")
					validatedMetrics["apachedruid.metadatacache.refresh.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of segments to refresh in broker segment metadata cache.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.metadatacache.refresh.time":
					assert.False(t, validatedMetrics["apachedruid.metadatacache.refresh.time"], "Found a duplicate in the metrics slice: apachedruid.metadatacache.refresh.time")
					validatedMetrics["apachedruid.metadatacache.refresh.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Time taken to refresh segments in broker segment metadata cache.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.byte_limit.exceeded.count":
					assert.False(t, validatedMetrics["apachedruid.query.byte_limit.exceeded.count"], "Found a duplicate in the metrics slice: apachedruid.query.byte_limit.exceeded.count")
					validatedMetrics["apachedruid.query.byte_limit.exceeded.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of queries whose inlined subquery results exceeded the given byte limit.", ms.At(i).Description())
					assert.Equal(t, "{queries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.bytes":
					assert.False(t, validatedMetrics["apachedruid.query.bytes"], "Found a duplicate in the metrics slice: apachedruid.query.bytes")
					validatedMetrics["apachedruid.query.bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "The total number of bytes returned to the requesting client in the query response from the broker. Other services report the total bytes for their portion of the query.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "query_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("num_metrics")
					assert.True(t, ok)
					assert.EqualValues(t, "query_num_metrics-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("dimension")
					assert.True(t, ok)
					assert.EqualValues(t, "query_dimension-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("has_filters")
					assert.True(t, ok)
					assert.EqualValues(t, "query_has_filters-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("threshold")
					assert.True(t, ok)
					assert.EqualValues(t, 15, attrVal.Int())
					attrVal, ok = dp.Attributes().Get("num_complex_metrics")
					assert.True(t, ok)
					assert.EqualValues(t, 25, attrVal.Int())
					attrVal, ok = dp.Attributes().Get("type")
					assert.True(t, ok)
					assert.EqualValues(t, "query_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("remote_address")
					assert.True(t, ok)
					assert.EqualValues(t, "query_remote_address-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "query_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("context")
					assert.True(t, ok)
					assert.EqualValues(t, "query_context-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("num_dimensions")
					assert.True(t, ok)
					assert.EqualValues(t, "query_num_dimensions-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("interval")
					assert.True(t, ok)
					assert.EqualValues(t, "query_interval-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("duration")
					assert.True(t, ok)
					assert.EqualValues(t, "query_duration-val", attrVal.Str())
				case "apachedruid.query.cache.delta.average_bytes":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.average_bytes"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.average_bytes")
					validatedMetrics["apachedruid.query.cache.delta.average_bytes"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Average cache entry byte size.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.delta.errors":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.errors"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.errors")
					validatedMetrics["apachedruid.query.cache.delta.errors"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of cache errors.", ms.At(i).Description())
					assert.Equal(t, "{errors}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.delta.evictions":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.evictions"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.evictions")
					validatedMetrics["apachedruid.query.cache.delta.evictions"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of cache evictions.", ms.At(i).Description())
					assert.Equal(t, "{evictions}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.delta.hit_rate":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.hit_rate"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.hit_rate")
					validatedMetrics["apachedruid.query.cache.delta.hit_rate"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Cache hit rate.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
				case "apachedruid.query.cache.delta.hits":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.hits"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.hits")
					validatedMetrics["apachedruid.query.cache.delta.hits"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of cache hits.", ms.At(i).Description())
					assert.Equal(t, "{hits}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.delta.misses":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.misses"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.misses")
					validatedMetrics["apachedruid.query.cache.delta.misses"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of cache misses.", ms.At(i).Description())
					assert.Equal(t, "{misses}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.delta.num_entries":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.num_entries"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.num_entries")
					validatedMetrics["apachedruid.query.cache.delta.num_entries"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of cache entries.", ms.At(i).Description())
					assert.Equal(t, "{entries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.delta.put.error":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.put.error"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.put.error")
					validatedMetrics["apachedruid.query.cache.delta.put.error"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of new cache entries that could not be cached due to errors.", ms.At(i).Description())
					assert.Equal(t, "{errors}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.delta.put.ok":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.put.ok"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.put.ok")
					validatedMetrics["apachedruid.query.cache.delta.put.ok"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of new cache entries successfully cached.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.delta.put.oversized":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.put.oversized"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.put.oversized")
					validatedMetrics["apachedruid.query.cache.delta.put.oversized"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of potential new cache entries that were skipped due to being too large (based on `druid.{broker,historical,realtime}.cache.maxEntrySize` properties).", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.delta.size_bytes":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.size_bytes"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.size_bytes")
					validatedMetrics["apachedruid.query.cache.delta.size_bytes"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Size in bytes of cache entries.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.delta.timeouts":
					assert.False(t, validatedMetrics["apachedruid.query.cache.delta.timeouts"], "Found a duplicate in the metrics slice: apachedruid.query.cache.delta.timeouts")
					validatedMetrics["apachedruid.query.cache.delta.timeouts"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of cache timeouts.", ms.At(i).Description())
					assert.Equal(t, "{timeouts}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.memcached.delta":
					assert.False(t, validatedMetrics["apachedruid.query.cache.memcached.delta"], "Found a duplicate in the metrics slice: apachedruid.query.cache.memcached.delta")
					validatedMetrics["apachedruid.query.cache.memcached.delta"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Cache metrics unique to memcached (only if `druid.cache.type=memcached`) as their delta from the prior event emission.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.memcached.total":
					assert.False(t, validatedMetrics["apachedruid.query.cache.memcached.total"], "Found a duplicate in the metrics slice: apachedruid.query.cache.memcached.total")
					validatedMetrics["apachedruid.query.cache.memcached.total"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Cache metrics unique to memcached (only if `druid.cache.type=memcached`) as their actual values.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.total.average_bytes":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.average_bytes"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.average_bytes")
					validatedMetrics["apachedruid.query.cache.total.average_bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Average cache entry byte size.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.total.errors":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.errors"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.errors")
					validatedMetrics["apachedruid.query.cache.total.errors"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of cache errors.", ms.At(i).Description())
					assert.Equal(t, "{errors}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.total.evictions":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.evictions"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.evictions")
					validatedMetrics["apachedruid.query.cache.total.evictions"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of cache evictions.", ms.At(i).Description())
					assert.Equal(t, "{evictions}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.total.hit_rate":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.hit_rate"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.hit_rate")
					validatedMetrics["apachedruid.query.cache.total.hit_rate"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Cache hit rate.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
				case "apachedruid.query.cache.total.hits":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.hits"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.hits")
					validatedMetrics["apachedruid.query.cache.total.hits"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of cache hits.", ms.At(i).Description())
					assert.Equal(t, "{hits}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.total.misses":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.misses"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.misses")
					validatedMetrics["apachedruid.query.cache.total.misses"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of cache misses.", ms.At(i).Description())
					assert.Equal(t, "{misses}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.total.num_entries":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.num_entries"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.num_entries")
					validatedMetrics["apachedruid.query.cache.total.num_entries"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of cache entries.", ms.At(i).Description())
					assert.Equal(t, "{entries}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.total.put.error":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.put.error"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.put.error")
					validatedMetrics["apachedruid.query.cache.total.put.error"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of new cache entries that could not be cached due to errors.", ms.At(i).Description())
					assert.Equal(t, "{errors}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.total.put.ok":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.put.ok"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.put.ok")
					validatedMetrics["apachedruid.query.cache.total.put.ok"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of new cache entries successfully cached.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.total.put.oversized":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.put.oversized"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.put.oversized")
					validatedMetrics["apachedruid.query.cache.total.put.oversized"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of potential new cache entries that were skipped due to being too large (based on `druid.{broker,historical,realtime}.cache.maxEntrySize` properties).", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.total.size_bytes":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.size_bytes"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.size_bytes")
					validatedMetrics["apachedruid.query.cache.total.size_bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Size in bytes of cache entries.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cache.total.timeouts":
					assert.False(t, validatedMetrics["apachedruid.query.cache.total.timeouts"], "Found a duplicate in the metrics slice: apachedruid.query.cache.total.timeouts")
					validatedMetrics["apachedruid.query.cache.total.timeouts"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of cache timeouts.", ms.At(i).Description())
					assert.Equal(t, "{timeouts}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.count":
					assert.False(t, validatedMetrics["apachedruid.query.count"], "Found a duplicate in the metrics slice: apachedruid.query.count")
					validatedMetrics["apachedruid.query.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of total queries.", ms.At(i).Description())
					assert.Equal(t, "{queries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.cpu.time":
					assert.False(t, validatedMetrics["apachedruid.query.cpu.time"], "Found a duplicate in the metrics slice: apachedruid.query.cpu.time")
					validatedMetrics["apachedruid.query.cpu.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Microseconds of CPU time taken to complete a query.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "query_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("num_metrics")
					assert.True(t, ok)
					assert.EqualValues(t, "query_num_metrics-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("dimension")
					assert.True(t, ok)
					assert.EqualValues(t, "query_dimension-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("has_filters")
					assert.True(t, ok)
					assert.EqualValues(t, "query_has_filters-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("threshold")
					assert.True(t, ok)
					assert.EqualValues(t, 15, attrVal.Int())
					attrVal, ok = dp.Attributes().Get("num_complex_metrics")
					assert.True(t, ok)
					assert.EqualValues(t, 25, attrVal.Int())
					attrVal, ok = dp.Attributes().Get("type")
					assert.True(t, ok)
					assert.EqualValues(t, "query_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("remote_address")
					assert.True(t, ok)
					assert.EqualValues(t, "query_remote_address-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "query_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("context")
					assert.True(t, ok)
					assert.EqualValues(t, "query_context-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("num_dimensions")
					assert.True(t, ok)
					assert.EqualValues(t, "query_num_dimensions-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("interval")
					assert.True(t, ok)
					assert.EqualValues(t, "query_interval-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("duration")
					assert.True(t, ok)
					assert.EqualValues(t, "query_duration-val", attrVal.Str())
				case "apachedruid.query.failed.count":
					assert.False(t, validatedMetrics["apachedruid.query.failed.count"], "Found a duplicate in the metrics slice: apachedruid.query.failed.count")
					validatedMetrics["apachedruid.query.failed.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of failed queries.", ms.At(i).Description())
					assert.Equal(t, "{queries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.interrupted.count":
					assert.False(t, validatedMetrics["apachedruid.query.interrupted.count"], "Found a duplicate in the metrics slice: apachedruid.query.interrupted.count")
					validatedMetrics["apachedruid.query.interrupted.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of queries interrupted due to cancellation.", ms.At(i).Description())
					assert.Equal(t, "{queries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.node.backpressure":
					assert.False(t, validatedMetrics["apachedruid.query.node.backpressure"], "Found a duplicate in the metrics slice: apachedruid.query.node.backpressure")
					validatedMetrics["apachedruid.query.node.backpressure"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds that the channel to this process has spent suspended due to backpressure.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("status")
					assert.True(t, ok)
					assert.EqualValues(t, "query_status-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "query_server-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "query_id-val", attrVal.Str())
				case "apachedruid.query.node.bytes":
					assert.False(t, validatedMetrics["apachedruid.query.node.bytes"], "Found a duplicate in the metrics slice: apachedruid.query.node.bytes")
					validatedMetrics["apachedruid.query.node.bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of bytes returned from querying individual historical/realtime processes.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("status")
					assert.True(t, ok)
					assert.EqualValues(t, "query_status-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "query_server-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "query_id-val", attrVal.Str())
				case "apachedruid.query.node.time":
					assert.False(t, validatedMetrics["apachedruid.query.node.time"], "Found a duplicate in the metrics slice: apachedruid.query.node.time")
					validatedMetrics["apachedruid.query.node.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken to query individual historical/realtime processes.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("status")
					assert.True(t, ok)
					assert.EqualValues(t, "query_status-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "query_server-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "query_id-val", attrVal.Str())
				case "apachedruid.query.node.ttfb":
					assert.False(t, validatedMetrics["apachedruid.query.node.ttfb"], "Found a duplicate in the metrics slice: apachedruid.query.node.ttfb")
					validatedMetrics["apachedruid.query.node.ttfb"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Time to first byte. Milliseconds elapsed until Broker starts receiving the response from individual historical/realtime processes.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("status")
					assert.True(t, ok)
					assert.EqualValues(t, "query_status-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "query_server-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "query_id-val", attrVal.Str())
				case "apachedruid.query.priority":
					assert.False(t, validatedMetrics["apachedruid.query.priority"], "Found a duplicate in the metrics slice: apachedruid.query.priority")
					validatedMetrics["apachedruid.query.priority"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Assigned lane and priority, only if Laning strategy is enabled. Refer to [Laning strategies](https,//druid.apache.org/docs/latest/configuration#laning-strategies).", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("type")
					assert.True(t, ok)
					assert.EqualValues(t, "query_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "query_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("lane")
					assert.True(t, ok)
					assert.EqualValues(t, "query_lane-val", attrVal.Str())
				case "apachedruid.query.row_limit.exceeded.count":
					assert.False(t, validatedMetrics["apachedruid.query.row_limit.exceeded.count"], "Found a duplicate in the metrics slice: apachedruid.query.row_limit.exceeded.count")
					validatedMetrics["apachedruid.query.row_limit.exceeded.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of queries whose inlined subquery results exceeded the given row limit.", ms.At(i).Description())
					assert.Equal(t, "{queries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.segment.time":
					assert.False(t, validatedMetrics["apachedruid.query.segment.time"], "Found a duplicate in the metrics slice: apachedruid.query.segment.time")
					validatedMetrics["apachedruid.query.segment.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken to query individual segment. Includes time to page in the segment from disk.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("status")
					assert.True(t, ok)
					assert.EqualValues(t, "query_status-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("segment")
					assert.True(t, ok)
					assert.EqualValues(t, "query_segment-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "query_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("vectorized")
					assert.True(t, ok)
					assert.EqualValues(t, "query_vectorized-val", attrVal.Str())
				case "apachedruid.query.segment_and_cache.time":
					assert.False(t, validatedMetrics["apachedruid.query.segment_and_cache.time"], "Found a duplicate in the metrics slice: apachedruid.query.segment_and_cache.time")
					validatedMetrics["apachedruid.query.segment_and_cache.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken to query individual segment or hit the cache (if it is enabled on the Historical process).", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("segment")
					assert.True(t, ok)
					assert.EqualValues(t, "query_segment-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "query_id-val", attrVal.Str())
				case "apachedruid.query.segments.count":
					assert.False(t, validatedMetrics["apachedruid.query.segments.count"], "Found a duplicate in the metrics slice: apachedruid.query.segments.count")
					validatedMetrics["apachedruid.query.segments.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "This metric is not enabled by default. See the `QueryMetrics` Interface for reference regarding enabling this metric. Number of segments that will be touched by the query. In the broker, it makes a plan to distribute the query to realtime tasks and historicals based on a snapshot of segment distribution state. If there are some segments moved after this snapshot is created, certain historicals and realtime tasks can report those segments as missing to the broker. The broker will resend the query to the new servers that serve those segments after move. In this case, those segments can be counted more than once in this metric.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.success.count":
					assert.False(t, validatedMetrics["apachedruid.query.success.count"], "Found a duplicate in the metrics slice: apachedruid.query.success.count")
					validatedMetrics["apachedruid.query.success.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of queries successfully processed.", ms.At(i).Description())
					assert.Equal(t, "{queries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.time":
					assert.False(t, validatedMetrics["apachedruid.query.time"], "Found a duplicate in the metrics slice: apachedruid.query.time")
					validatedMetrics["apachedruid.query.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken to complete a query.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "query_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("num_metrics")
					assert.True(t, ok)
					assert.EqualValues(t, "query_num_metrics-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("dimension")
					assert.True(t, ok)
					assert.EqualValues(t, "query_dimension-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("has_filters")
					assert.True(t, ok)
					assert.EqualValues(t, "query_has_filters-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("threshold")
					assert.True(t, ok)
					assert.EqualValues(t, 15, attrVal.Int())
					attrVal, ok = dp.Attributes().Get("num_complex_metrics")
					assert.True(t, ok)
					assert.EqualValues(t, 25, attrVal.Int())
					attrVal, ok = dp.Attributes().Get("type")
					assert.True(t, ok)
					assert.EqualValues(t, "query_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("remote_address")
					assert.True(t, ok)
					assert.EqualValues(t, "query_remote_address-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "query_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("context")
					assert.True(t, ok)
					assert.EqualValues(t, "query_context-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("num_dimensions")
					assert.True(t, ok)
					assert.EqualValues(t, "query_num_dimensions-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("interval")
					assert.True(t, ok)
					assert.EqualValues(t, "query_interval-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("duration")
					assert.True(t, ok)
					assert.EqualValues(t, "query_duration-val", attrVal.Str())
				case "apachedruid.query.timeout.count":
					assert.False(t, validatedMetrics["apachedruid.query.timeout.count"], "Found a duplicate in the metrics slice: apachedruid.query.timeout.count")
					validatedMetrics["apachedruid.query.timeout.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of timed out queries.", ms.At(i).Description())
					assert.Equal(t, "{queries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.query.wait.time":
					assert.False(t, validatedMetrics["apachedruid.query.wait.time"], "Found a duplicate in the metrics slice: apachedruid.query.wait.time")
					validatedMetrics["apachedruid.query.wait.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds spent waiting for a segment to be scanned.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("segment")
					assert.True(t, ok)
					assert.EqualValues(t, "query_segment-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "query_id-val", attrVal.Str())
				case "apachedruid.segment.added.bytes":
					assert.False(t, validatedMetrics["apachedruid.segment.added.bytes"], "Found a duplicate in the metrics slice: apachedruid.segment.added.bytes")
					validatedMetrics["apachedruid.segment.added.bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Size in bytes of new segments created.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_task_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("interval")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_interval-val", attrVal.Str())
				case "apachedruid.segment.assign_skipped.count":
					assert.False(t, validatedMetrics["apachedruid.segment.assign_skipped.count"], "Found a duplicate in the metrics slice: apachedruid.segment.assign_skipped.count")
					validatedMetrics["apachedruid.segment.assign_skipped.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of segments that could not be assigned to any server for loading. This can occur due to replication throttling, no available disk space, or a full load queue.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("description")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_description-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.assigned.count":
					assert.False(t, validatedMetrics["apachedruid.segment.assigned.count"], "Found a duplicate in the metrics slice: apachedruid.segment.assigned.count")
					validatedMetrics["apachedruid.segment.assigned.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of segments assigned to be loaded in the cluster.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.compacted.bytes":
					assert.False(t, validatedMetrics["apachedruid.segment.compacted.bytes"], "Found a duplicate in the metrics slice: apachedruid.segment.compacted.bytes")
					validatedMetrics["apachedruid.segment.compacted.bytes"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Total bytes of this datasource that are already compacted with the spec set in the auto compaction config.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.compacted.count":
					assert.False(t, validatedMetrics["apachedruid.segment.compacted.count"], "Found a duplicate in the metrics slice: apachedruid.segment.compacted.count")
					validatedMetrics["apachedruid.segment.compacted.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Total number of segments of this datasource that are already compacted with the spec set in the auto compaction config.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.count":
					assert.False(t, validatedMetrics["apachedruid.segment.count"], "Found a duplicate in the metrics slice: apachedruid.segment.count")
					validatedMetrics["apachedruid.segment.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of used segments belonging to a data source. Emitted only for data sources to which at least one used segment belongs.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("priority")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_priority-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.deleted.count":
					assert.False(t, validatedMetrics["apachedruid.segment.deleted.count"], "Found a duplicate in the metrics slice: apachedruid.segment.deleted.count")
					validatedMetrics["apachedruid.segment.deleted.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of segments marked as unused due to drop rules.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.drop_queue.count":
					assert.False(t, validatedMetrics["apachedruid.segment.drop_queue.count"], "Found a duplicate in the metrics slice: apachedruid.segment.drop_queue.count")
					validatedMetrics["apachedruid.segment.drop_queue.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of segments to drop.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_server-val", attrVal.Str())
				case "apachedruid.segment.drop_skipped.count":
					assert.False(t, validatedMetrics["apachedruid.segment.drop_skipped.count"], "Found a duplicate in the metrics slice: apachedruid.segment.drop_skipped.count")
					validatedMetrics["apachedruid.segment.drop_skipped.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of segments that could not be dropped from any server.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("description")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_description-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.dropped.count":
					assert.False(t, validatedMetrics["apachedruid.segment.dropped.count"], "Found a duplicate in the metrics slice: apachedruid.segment.dropped.count")
					validatedMetrics["apachedruid.segment.dropped.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of segments chosen to be dropped from the cluster due to being over-replicated.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.load_queue.assigned":
					assert.False(t, validatedMetrics["apachedruid.segment.load_queue.assigned"], "Found a duplicate in the metrics slice: apachedruid.segment.load_queue.assigned")
					validatedMetrics["apachedruid.segment.load_queue.assigned"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of segments assigned for load or drop to the load queue of a server.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_server-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.load_queue.cancelled":
					assert.False(t, validatedMetrics["apachedruid.segment.load_queue.cancelled"], "Found a duplicate in the metrics slice: apachedruid.segment.load_queue.cancelled")
					validatedMetrics["apachedruid.segment.load_queue.cancelled"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of segment assignments that were canceled before completion.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_server-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.load_queue.count":
					assert.False(t, validatedMetrics["apachedruid.segment.load_queue.count"], "Found a duplicate in the metrics slice: apachedruid.segment.load_queue.count")
					validatedMetrics["apachedruid.segment.load_queue.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of segments to load.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_server-val", attrVal.Str())
				case "apachedruid.segment.load_queue.failed":
					assert.False(t, validatedMetrics["apachedruid.segment.load_queue.failed"], "Found a duplicate in the metrics slice: apachedruid.segment.load_queue.failed")
					validatedMetrics["apachedruid.segment.load_queue.failed"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of segment assignments that failed to complete.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_server-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.load_queue.size":
					assert.False(t, validatedMetrics["apachedruid.segment.load_queue.size"], "Found a duplicate in the metrics slice: apachedruid.segment.load_queue.size")
					validatedMetrics["apachedruid.segment.load_queue.size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Size in bytes of segments to load.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_server-val", attrVal.Str())
				case "apachedruid.segment.load_queue.success":
					assert.False(t, validatedMetrics["apachedruid.segment.load_queue.success"], "Found a duplicate in the metrics slice: apachedruid.segment.load_queue.success")
					validatedMetrics["apachedruid.segment.load_queue.success"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of segment assignments that completed successfully.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_server-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.max":
					assert.False(t, validatedMetrics["apachedruid.segment.max"], "Found a duplicate in the metrics slice: apachedruid.segment.max")
					validatedMetrics["apachedruid.segment.max"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Maximum byte limit available for segments.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.segment.move_skipped.count":
					assert.False(t, validatedMetrics["apachedruid.segment.move_skipped.count"], "Found a duplicate in the metrics slice: apachedruid.segment.move_skipped.count")
					validatedMetrics["apachedruid.segment.move_skipped.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of segments that were chosen for balancing but could not be moved. This can occur when segments are already optimally placed.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("description")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_description-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.moved.bytes":
					assert.False(t, validatedMetrics["apachedruid.segment.moved.bytes"], "Found a duplicate in the metrics slice: apachedruid.segment.moved.bytes")
					validatedMetrics["apachedruid.segment.moved.bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Size in bytes of segments moved/archived via the Move Task.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_task_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("interval")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_interval-val", attrVal.Str())
				case "apachedruid.segment.moved.count":
					assert.False(t, validatedMetrics["apachedruid.segment.moved.count"], "Found a duplicate in the metrics slice: apachedruid.segment.moved.count")
					validatedMetrics["apachedruid.segment.moved.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of segments moved in the cluster.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.nuked.bytes":
					assert.False(t, validatedMetrics["apachedruid.segment.nuked.bytes"], "Found a duplicate in the metrics slice: apachedruid.segment.nuked.bytes")
					validatedMetrics["apachedruid.segment.nuked.bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Size in bytes of segments deleted via the Kill Task.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_task_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("interval")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_interval-val", attrVal.Str())
				case "apachedruid.segment.over_shadowed.count":
					assert.False(t, validatedMetrics["apachedruid.segment.over_shadowed.count"], "Found a duplicate in the metrics slice: apachedruid.segment.over_shadowed.count")
					validatedMetrics["apachedruid.segment.over_shadowed.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of segments marked as unused due to being overshadowed.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.segment.pending_delete":
					assert.False(t, validatedMetrics["apachedruid.segment.pending_delete"], "Found a duplicate in the metrics slice: apachedruid.segment.pending_delete")
					validatedMetrics["apachedruid.segment.pending_delete"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "On-disk size in bytes of segments that are waiting to be cleared out.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.segment.row_count.avg":
					assert.False(t, validatedMetrics["apachedruid.segment.row_count.avg"], "Found a duplicate in the metrics slice: apachedruid.segment.row_count.avg")
					validatedMetrics["apachedruid.segment.row_count.avg"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "The average number of rows per segment on a historical. `SegmentStatsMonitor` must be enabled.", ms.At(i).Description())
					assert.Equal(t, "{rows}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("priority")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_priority-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.row_count.range.count":
					assert.False(t, validatedMetrics["apachedruid.segment.row_count.range.count"], "Found a duplicate in the metrics slice: apachedruid.segment.row_count.range.count")
					validatedMetrics["apachedruid.segment.row_count.range.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "The number of segments in a bucket. `SegmentStatsMonitor` must be enabled.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("priority")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_priority-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("range")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_range-val", attrVal.Str())
				case "apachedruid.segment.scan.active":
					assert.False(t, validatedMetrics["apachedruid.segment.scan.active"], "Found a duplicate in the metrics slice: apachedruid.segment.scan.active")
					validatedMetrics["apachedruid.segment.scan.active"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of segments currently scanned. This metric also indicates how many threads from `druid.processing.numThreads` are currently being used.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.segment.scan.pending":
					assert.False(t, validatedMetrics["apachedruid.segment.scan.pending"], "Found a duplicate in the metrics slice: apachedruid.segment.scan.pending")
					validatedMetrics["apachedruid.segment.scan.pending"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of segments in queue waiting to be scanned.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.segment.size":
					assert.False(t, validatedMetrics["apachedruid.segment.size"], "Found a duplicate in the metrics slice: apachedruid.segment.size")
					validatedMetrics["apachedruid.segment.size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total size of used segments in a data source. Emitted only for data sources to which at least one used segment belongs.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.skip_compact.bytes":
					assert.False(t, validatedMetrics["apachedruid.segment.skip_compact.bytes"], "Found a duplicate in the metrics slice: apachedruid.segment.skip_compact.bytes")
					validatedMetrics["apachedruid.segment.skip_compact.bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total bytes of this datasource that are skipped (not eligible for auto compaction) by the auto compaction.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.skip_compact.count":
					assert.False(t, validatedMetrics["apachedruid.segment.skip_compact.count"], "Found a duplicate in the metrics slice: apachedruid.segment.skip_compact.count")
					validatedMetrics["apachedruid.segment.skip_compact.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total number of segments of this datasource that are skipped (not eligible for auto compaction) by the auto compaction.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.unavailable.count":
					assert.False(t, validatedMetrics["apachedruid.segment.unavailable.count"], "Found a duplicate in the metrics slice: apachedruid.segment.unavailable.count")
					validatedMetrics["apachedruid.segment.unavailable.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of unique segments left to load until all used segments are available for queries.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.under_replicated.count":
					assert.False(t, validatedMetrics["apachedruid.segment.under_replicated.count"], "Found a duplicate in the metrics slice: apachedruid.segment.under_replicated.count")
					validatedMetrics["apachedruid.segment.under_replicated.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of segments, including replicas, left to load until all used segments are available for queries.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.unneeded.count":
					assert.False(t, validatedMetrics["apachedruid.segment.unneeded.count"], "Found a duplicate in the metrics slice: apachedruid.segment.unneeded.count")
					validatedMetrics["apachedruid.segment.unneeded.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of segments dropped due to being marked as unused.", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.used":
					assert.False(t, validatedMetrics["apachedruid.segment.used"], "Found a duplicate in the metrics slice: apachedruid.segment.used")
					validatedMetrics["apachedruid.segment.used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Bytes used for served segments.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("priority")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_priority-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.used_percent":
					assert.False(t, validatedMetrics["apachedruid.segment.used_percent"], "Found a duplicate in the metrics slice: apachedruid.segment.used_percent")
					validatedMetrics["apachedruid.segment.used_percent"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Percentage of space used by served segments.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("priority")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_priority-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.wait_compact.bytes":
					assert.False(t, validatedMetrics["apachedruid.segment.wait_compact.bytes"], "Found a duplicate in the metrics slice: apachedruid.segment.wait_compact.bytes")
					validatedMetrics["apachedruid.segment.wait_compact.bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total bytes of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction).", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.segment.wait_compact.count":
					assert.False(t, validatedMetrics["apachedruid.segment.wait_compact.count"], "Found a duplicate in the metrics slice: apachedruid.segment.wait_compact.count")
					validatedMetrics["apachedruid.segment.wait_compact.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total number of segments of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction).", ms.At(i).Description())
					assert.Equal(t, "{segments}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "segment_data_source-val", attrVal.Str())
				case "apachedruid.serverview.init.time":
					assert.False(t, validatedMetrics["apachedruid.serverview.init.time"], "Found a duplicate in the metrics slice: apachedruid.serverview.init.time")
					validatedMetrics["apachedruid.serverview.init.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Time taken to initialize the broker server view. Useful to detect if brokers are taking too long to start.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.serverview.sync.healthy":
					assert.False(t, validatedMetrics["apachedruid.serverview.sync.healthy"], "Found a duplicate in the metrics slice: apachedruid.serverview.sync.healthy")
					validatedMetrics["apachedruid.serverview.sync.healthy"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Sync status of the Broker with a segment-loading server such as a Historical or Peon. Emitted only when [HTTP-based server view](https,//druid.apache.org/docs/latest/configuration#segment-management) is enabled. This metric can be used in conjunction with `serverview/sync/unstableTime` to debug slow startup of Brokers.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "serverview_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "serverview_server-val", attrVal.Str())
				case "apachedruid.serverview.sync.unstable_time":
					assert.False(t, validatedMetrics["apachedruid.serverview.sync.unstable_time"], "Found a duplicate in the metrics slice: apachedruid.serverview.sync.unstable_time")
					validatedMetrics["apachedruid.serverview.sync.unstable_time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Time in milliseconds for which the Broker has been failing to sync with a segment-loading server. Emitted only when [HTTP-based server view](https,//druid.apache.org/docs/latest/configuration#segment-management) is enabled.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "serverview_tier-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("server")
					assert.True(t, ok)
					assert.EqualValues(t, "serverview_server-val", attrVal.Str())
				case "apachedruid.sql_query.bytes":
					assert.False(t, validatedMetrics["apachedruid.sql_query.bytes"], "Found a duplicate in the metrics slice: apachedruid.sql_query.bytes")
					validatedMetrics["apachedruid.sql_query.bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of bytes returned in the SQL query response.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("native_query_ids")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_native_query_ids-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("engine")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_engine-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("remote_address")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_remote_address-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("success")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_success-val", attrVal.Str())
				case "apachedruid.sql_query.planning_time_ms":
					assert.False(t, validatedMetrics["apachedruid.sql_query.planning_time_ms"], "Found a duplicate in the metrics slice: apachedruid.sql_query.planning_time_ms")
					validatedMetrics["apachedruid.sql_query.planning_time_ms"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken to plan a SQL to native query.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("native_query_ids")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_native_query_ids-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("engine")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_engine-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("remote_address")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_remote_address-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("success")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_success-val", attrVal.Str())
				case "apachedruid.sql_query.time":
					assert.False(t, validatedMetrics["apachedruid.sql_query.time"], "Found a duplicate in the metrics slice: apachedruid.sql_query.time")
					validatedMetrics["apachedruid.sql_query.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken to complete a SQL query.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("native_query_ids")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_native_query_ids-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("engine")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_engine-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("remote_address")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_remote_address-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("id")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("success")
					assert.True(t, ok)
					assert.EqualValues(t, "sqlQuery_success-val", attrVal.Str())
				case "apachedruid.subquery.byte_limit.count":
					assert.False(t, validatedMetrics["apachedruid.subquery.byte_limit.count"], "Found a duplicate in the metrics slice: apachedruid.subquery.byte_limit.count")
					validatedMetrics["apachedruid.subquery.byte_limit.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of subqueries whose results are materialized as frames (Druid's internal byte representation of rows).", ms.At(i).Description())
					assert.Equal(t, "{subqueries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.subquery.fallback.count":
					assert.False(t, validatedMetrics["apachedruid.subquery.fallback.count"], "Found a duplicate in the metrics slice: apachedruid.subquery.fallback.count")
					validatedMetrics["apachedruid.subquery.fallback.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of subqueries which cannot be materialized as frames.", ms.At(i).Description())
					assert.Equal(t, "{subqueries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.subquery.fallback.insufficient_type.count":
					assert.False(t, validatedMetrics["apachedruid.subquery.fallback.insufficient_type.count"], "Found a duplicate in the metrics slice: apachedruid.subquery.fallback.insufficient_type.count")
					validatedMetrics["apachedruid.subquery.fallback.insufficient_type.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of subqueries which cannot be materialized as frames due to insufficient type information in the row signature.", ms.At(i).Description())
					assert.Equal(t, "{subqueries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.subquery.fallback.unknown_reason.count":
					assert.False(t, validatedMetrics["apachedruid.subquery.fallback.unknown_reason.count"], "Found a duplicate in the metrics slice: apachedruid.subquery.fallback.unknown_reason.count")
					validatedMetrics["apachedruid.subquery.fallback.unknown_reason.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of subqueries which cannot be materialized as frames due other reasons.", ms.At(i).Description())
					assert.Equal(t, "{subqueries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.subquery.row_limit.count":
					assert.False(t, validatedMetrics["apachedruid.subquery.row_limit.count"], "Found a duplicate in the metrics slice: apachedruid.subquery.row_limit.count")
					validatedMetrics["apachedruid.subquery.row_limit.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of subqueries whose results are materialized as rows (Java objects on heap).", ms.At(i).Description())
					assert.Equal(t, "{subqueries}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.cpu":
					assert.False(t, validatedMetrics["apachedruid.sys.cpu"], "Found a duplicate in the metrics slice: apachedruid.sys.cpu")
					validatedMetrics["apachedruid.sys.cpu"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "CPU used.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("cpu_time")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_cpu_time-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("cpu_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_cpu_name-val", attrVal.Str())
				case "apachedruid.sys.disk.queue":
					assert.False(t, validatedMetrics["apachedruid.sys.disk.queue"], "Found a duplicate in the metrics slice: apachedruid.sys.disk.queue")
					validatedMetrics["apachedruid.sys.disk.queue"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Disk queue length. Measures number of requests waiting to be processed by disk.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("disk_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_disk_name-val", attrVal.Str())
				case "apachedruid.sys.disk.read.count":
					assert.False(t, validatedMetrics["apachedruid.sys.disk.read.count"], "Found a duplicate in the metrics slice: apachedruid.sys.disk.read.count")
					validatedMetrics["apachedruid.sys.disk.read.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Reads from disk.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("disk_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_disk_name-val", attrVal.Str())
				case "apachedruid.sys.disk.read.size":
					assert.False(t, validatedMetrics["apachedruid.sys.disk.read.size"], "Found a duplicate in the metrics slice: apachedruid.sys.disk.read.size")
					validatedMetrics["apachedruid.sys.disk.read.size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Bytes read from disk. One indicator of the amount of paging occurring for segments.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("disk_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_disk_name-val", attrVal.Str())
				case "apachedruid.sys.disk.transfer_time":
					assert.False(t, validatedMetrics["apachedruid.sys.disk.transfer_time"], "Found a duplicate in the metrics slice: apachedruid.sys.disk.transfer_time")
					validatedMetrics["apachedruid.sys.disk.transfer_time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Transfer time to read from or write to disk.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("disk_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_disk_name-val", attrVal.Str())
				case "apachedruid.sys.disk.write.count":
					assert.False(t, validatedMetrics["apachedruid.sys.disk.write.count"], "Found a duplicate in the metrics slice: apachedruid.sys.disk.write.count")
					validatedMetrics["apachedruid.sys.disk.write.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Writes to disk.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("disk_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_disk_name-val", attrVal.Str())
				case "apachedruid.sys.disk.write.size":
					assert.False(t, validatedMetrics["apachedruid.sys.disk.write.size"], "Found a duplicate in the metrics slice: apachedruid.sys.disk.write.size")
					validatedMetrics["apachedruid.sys.disk.write.size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Bytes written to disk. One indicator of the amount of paging occurring for segments.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("disk_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_disk_name-val", attrVal.Str())
				case "apachedruid.sys.fs.files.count":
					assert.False(t, validatedMetrics["apachedruid.sys.fs.files.count"], "Found a duplicate in the metrics slice: apachedruid.sys.fs.files.count")
					validatedMetrics["apachedruid.sys.fs.files.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Filesystem total IO nodes.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("fs_dir_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_fs_dir_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("fs_dev_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_fs_dev_name-val", attrVal.Str())
				case "apachedruid.sys.fs.files.free":
					assert.False(t, validatedMetrics["apachedruid.sys.fs.files.free"], "Found a duplicate in the metrics slice: apachedruid.sys.fs.files.free")
					validatedMetrics["apachedruid.sys.fs.files.free"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Filesystem free IO nodes.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("fs_dir_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_fs_dir_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("fs_dev_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_fs_dev_name-val", attrVal.Str())
				case "apachedruid.sys.fs.max":
					assert.False(t, validatedMetrics["apachedruid.sys.fs.max"], "Found a duplicate in the metrics slice: apachedruid.sys.fs.max")
					validatedMetrics["apachedruid.sys.fs.max"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Filesystem bytes max.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("fs_dir_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_fs_dir_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("fs_dev_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_fs_dev_name-val", attrVal.Str())
				case "apachedruid.sys.fs.used":
					assert.False(t, validatedMetrics["apachedruid.sys.fs.used"], "Found a duplicate in the metrics slice: apachedruid.sys.fs.used")
					validatedMetrics["apachedruid.sys.fs.used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Filesystem bytes used.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("fs_dir_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_fs_dir_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("fs_dev_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_fs_dev_name-val", attrVal.Str())
				case "apachedruid.sys.la.1":
					assert.False(t, validatedMetrics["apachedruid.sys.la.1"], "Found a duplicate in the metrics slice: apachedruid.sys.la.1")
					validatedMetrics["apachedruid.sys.la.1"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "System CPU load averages over past `i` minutes, where `i={1,5,15}`.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.la.15":
					assert.False(t, validatedMetrics["apachedruid.sys.la.15"], "Found a duplicate in the metrics slice: apachedruid.sys.la.15")
					validatedMetrics["apachedruid.sys.la.15"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "System CPU load averages over past `i` minutes, where `i={1,5,15}`.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.la.5":
					assert.False(t, validatedMetrics["apachedruid.sys.la.5"], "Found a duplicate in the metrics slice: apachedruid.sys.la.5")
					validatedMetrics["apachedruid.sys.la.5"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "System CPU load averages over past `i` minutes, where `i={1,5,15}`.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.mem.free":
					assert.False(t, validatedMetrics["apachedruid.sys.mem.free"], "Found a duplicate in the metrics slice: apachedruid.sys.mem.free")
					validatedMetrics["apachedruid.sys.mem.free"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Memory free.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.mem.max":
					assert.False(t, validatedMetrics["apachedruid.sys.mem.max"], "Found a duplicate in the metrics slice: apachedruid.sys.mem.max")
					validatedMetrics["apachedruid.sys.mem.max"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Memory max.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.mem.used":
					assert.False(t, validatedMetrics["apachedruid.sys.mem.used"], "Found a duplicate in the metrics slice: apachedruid.sys.mem.used")
					validatedMetrics["apachedruid.sys.mem.used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Memory used.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.net.read.dropped":
					assert.False(t, validatedMetrics["apachedruid.sys.net.read.dropped"], "Found a duplicate in the metrics slice: apachedruid.sys.net.read.dropped")
					validatedMetrics["apachedruid.sys.net.read.dropped"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total packets dropped coming from network.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("net_hwaddr")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_hwaddr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_address")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_address-val", attrVal.Str())
				case "apachedruid.sys.net.read.errors":
					assert.False(t, validatedMetrics["apachedruid.sys.net.read.errors"], "Found a duplicate in the metrics slice: apachedruid.sys.net.read.errors")
					validatedMetrics["apachedruid.sys.net.read.errors"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total network read errors.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("net_hwaddr")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_hwaddr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_address")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_address-val", attrVal.Str())
				case "apachedruid.sys.net.read.packets":
					assert.False(t, validatedMetrics["apachedruid.sys.net.read.packets"], "Found a duplicate in the metrics slice: apachedruid.sys.net.read.packets")
					validatedMetrics["apachedruid.sys.net.read.packets"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total packets read from the network.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("net_hwaddr")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_hwaddr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_address")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_address-val", attrVal.Str())
				case "apachedruid.sys.net.read.size":
					assert.False(t, validatedMetrics["apachedruid.sys.net.read.size"], "Found a duplicate in the metrics slice: apachedruid.sys.net.read.size")
					validatedMetrics["apachedruid.sys.net.read.size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Bytes read from the network.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("net_hwaddr")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_hwaddr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_address")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_address-val", attrVal.Str())
				case "apachedruid.sys.net.write.collisions":
					assert.False(t, validatedMetrics["apachedruid.sys.net.write.collisions"], "Found a duplicate in the metrics slice: apachedruid.sys.net.write.collisions")
					validatedMetrics["apachedruid.sys.net.write.collisions"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total network write collisions.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("net_hwaddr")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_hwaddr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_address")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_address-val", attrVal.Str())
				case "apachedruid.sys.net.write.errors":
					assert.False(t, validatedMetrics["apachedruid.sys.net.write.errors"], "Found a duplicate in the metrics slice: apachedruid.sys.net.write.errors")
					validatedMetrics["apachedruid.sys.net.write.errors"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total network write errors.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("net_hwaddr")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_hwaddr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_address")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_address-val", attrVal.Str())
				case "apachedruid.sys.net.write.packets":
					assert.False(t, validatedMetrics["apachedruid.sys.net.write.packets"], "Found a duplicate in the metrics slice: apachedruid.sys.net.write.packets")
					validatedMetrics["apachedruid.sys.net.write.packets"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total packets written to the network.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("net_hwaddr")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_hwaddr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_address")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_address-val", attrVal.Str())
				case "apachedruid.sys.net.write.size":
					assert.False(t, validatedMetrics["apachedruid.sys.net.write.size"], "Found a duplicate in the metrics slice: apachedruid.sys.net.write.size")
					validatedMetrics["apachedruid.sys.net.write.size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Bytes written to the network.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("net_hwaddr")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_hwaddr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_name-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("net_address")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_net_address-val", attrVal.Str())
				case "apachedruid.sys.storage.used":
					assert.False(t, validatedMetrics["apachedruid.sys.storage.used"], "Found a duplicate in the metrics slice: apachedruid.sys.storage.used")
					validatedMetrics["apachedruid.sys.storage.used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Disk space used.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("fs_dir_name")
					assert.True(t, ok)
					assert.EqualValues(t, "sys_fs_dir_name-val", attrVal.Str())
				case "apachedruid.sys.swap.free":
					assert.False(t, validatedMetrics["apachedruid.sys.swap.free"], "Found a duplicate in the metrics slice: apachedruid.sys.swap.free")
					validatedMetrics["apachedruid.sys.swap.free"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Free swap.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.swap.max":
					assert.False(t, validatedMetrics["apachedruid.sys.swap.max"], "Found a duplicate in the metrics slice: apachedruid.sys.swap.max")
					validatedMetrics["apachedruid.sys.swap.max"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Max swap.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.swap.page_in":
					assert.False(t, validatedMetrics["apachedruid.sys.swap.page_in"], "Found a duplicate in the metrics slice: apachedruid.sys.swap.page_in")
					validatedMetrics["apachedruid.sys.swap.page_in"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Paged in swap.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.swap.page_out":
					assert.False(t, validatedMetrics["apachedruid.sys.swap.page_out"], "Found a duplicate in the metrics slice: apachedruid.sys.swap.page_out")
					validatedMetrics["apachedruid.sys.swap.page_out"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Paged out swap.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.tcpv4.active_opens":
					assert.False(t, validatedMetrics["apachedruid.sys.tcpv4.active_opens"], "Found a duplicate in the metrics slice: apachedruid.sys.tcpv4.active_opens")
					validatedMetrics["apachedruid.sys.tcpv4.active_opens"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total TCP active open connections.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.tcpv4.attempt_fails":
					assert.False(t, validatedMetrics["apachedruid.sys.tcpv4.attempt_fails"], "Found a duplicate in the metrics slice: apachedruid.sys.tcpv4.attempt_fails")
					validatedMetrics["apachedruid.sys.tcpv4.attempt_fails"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total TCP active connection failures.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.tcpv4.estab_resets":
					assert.False(t, validatedMetrics["apachedruid.sys.tcpv4.estab_resets"], "Found a duplicate in the metrics slice: apachedruid.sys.tcpv4.estab_resets")
					validatedMetrics["apachedruid.sys.tcpv4.estab_resets"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total TCP connection resets.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.tcpv4.in.errs":
					assert.False(t, validatedMetrics["apachedruid.sys.tcpv4.in.errs"], "Found a duplicate in the metrics slice: apachedruid.sys.tcpv4.in.errs")
					validatedMetrics["apachedruid.sys.tcpv4.in.errs"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Errors while reading segments.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.tcpv4.in.segs":
					assert.False(t, validatedMetrics["apachedruid.sys.tcpv4.in.segs"], "Found a duplicate in the metrics slice: apachedruid.sys.tcpv4.in.segs")
					validatedMetrics["apachedruid.sys.tcpv4.in.segs"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total segments received in connection.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.tcpv4.out.rsts":
					assert.False(t, validatedMetrics["apachedruid.sys.tcpv4.out.rsts"], "Found a duplicate in the metrics slice: apachedruid.sys.tcpv4.out.rsts")
					validatedMetrics["apachedruid.sys.tcpv4.out.rsts"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total `out reset` packets sent to reset the connection.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.tcpv4.out.segs":
					assert.False(t, validatedMetrics["apachedruid.sys.tcpv4.out.segs"], "Found a duplicate in the metrics slice: apachedruid.sys.tcpv4.out.segs")
					validatedMetrics["apachedruid.sys.tcpv4.out.segs"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total segments sent.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.tcpv4.passive_opens":
					assert.False(t, validatedMetrics["apachedruid.sys.tcpv4.passive_opens"], "Found a duplicate in the metrics slice: apachedruid.sys.tcpv4.passive_opens")
					validatedMetrics["apachedruid.sys.tcpv4.passive_opens"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total TCP passive open connections.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.tcpv4.retrans.segs":
					assert.False(t, validatedMetrics["apachedruid.sys.tcpv4.retrans.segs"], "Found a duplicate in the metrics slice: apachedruid.sys.tcpv4.retrans.segs")
					validatedMetrics["apachedruid.sys.tcpv4.retrans.segs"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total segments re-transmitted.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.sys.uptime":
					assert.False(t, validatedMetrics["apachedruid.sys.uptime"], "Found a duplicate in the metrics slice: apachedruid.sys.uptime")
					validatedMetrics["apachedruid.sys.uptime"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total system uptime.", ms.At(i).Description())
					assert.Equal(t, "s", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.task.action.batch.attempts":
					assert.False(t, validatedMetrics["apachedruid.task.action.batch.attempts"], "Found a duplicate in the metrics slice: apachedruid.task.action.batch.attempts")
					validatedMetrics["apachedruid.task.action.batch.attempts"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of execution attempts for a single batch of task actions. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).", ms.At(i).Description())
					assert.Equal(t, "{attempts}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("interval")
					assert.True(t, ok)
					assert.EqualValues(t, "task_interval-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_action_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_action_type-val", attrVal.Str())
				case "apachedruid.task.action.batch.queue_time":
					assert.False(t, validatedMetrics["apachedruid.task.action.batch.queue_time"], "Found a duplicate in the metrics slice: apachedruid.task.action.batch.queue_time")
					validatedMetrics["apachedruid.task.action.batch.queue_time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds spent by a batch of task actions in queue. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("interval")
					assert.True(t, ok)
					assert.EqualValues(t, "task_interval-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_action_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_action_type-val", attrVal.Str())
				case "apachedruid.task.action.batch.run_time":
					assert.False(t, validatedMetrics["apachedruid.task.action.batch.run_time"], "Found a duplicate in the metrics slice: apachedruid.task.action.batch.run_time")
					validatedMetrics["apachedruid.task.action.batch.run_time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken to execute a batch of task actions. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("interval")
					assert.True(t, ok)
					assert.EqualValues(t, "task_interval-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_action_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_action_type-val", attrVal.Str())
				case "apachedruid.task.action.batch.size":
					assert.False(t, validatedMetrics["apachedruid.task.action.batch.size"], "Found a duplicate in the metrics slice: apachedruid.task.action.batch.size")
					validatedMetrics["apachedruid.task.action.batch.size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of task actions in a batch that was executed during the emission period. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).", ms.At(i).Description())
					assert.Equal(t, "{actions}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("interval")
					assert.True(t, ok)
					assert.EqualValues(t, "task_interval-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_action_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_action_type-val", attrVal.Str())
				case "apachedruid.task.action.failed.count":
					assert.False(t, validatedMetrics["apachedruid.task.action.failed.count"], "Found a duplicate in the metrics slice: apachedruid.task.action.failed.count")
					validatedMetrics["apachedruid.task.action.failed.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of task actions that failed during the emission period. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).", ms.At(i).Description())
					assert.Equal(t, "{actions}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_action_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_action_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "task_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_id-val", attrVal.Str())
				case "apachedruid.task.action.log.time":
					assert.False(t, validatedMetrics["apachedruid.task.action.log.time"], "Found a duplicate in the metrics slice: apachedruid.task.action.log.time")
					validatedMetrics["apachedruid.task.action.log.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken to log a task action to the audit log.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_action_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_action_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "task_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_id-val", attrVal.Str())
				case "apachedruid.task.action.run.time":
					assert.False(t, validatedMetrics["apachedruid.task.action.run.time"], "Found a duplicate in the metrics slice: apachedruid.task.action.run.time")
					validatedMetrics["apachedruid.task.action.run.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken to execute a task action.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_action_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_action_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "task_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_id-val", attrVal.Str())
				case "apachedruid.task.action.success.count":
					assert.False(t, validatedMetrics["apachedruid.task.action.success.count"], "Found a duplicate in the metrics slice: apachedruid.task.action.success.count")
					validatedMetrics["apachedruid.task.action.success.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of task actions that were executed successfully during the emission period. Currently only being emitted for [batched `segmentAllocate` actions](https,//druid.apache.org/docs/latest/ingestion/tasks#batching-segmentallocate-actions).", ms.At(i).Description())
					assert.Equal(t, "{actions}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_action_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_action_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "task_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_id-val", attrVal.Str())
				case "apachedruid.task.failed.count":
					assert.False(t, validatedMetrics["apachedruid.task.failed.count"], "Found a duplicate in the metrics slice: apachedruid.task.failed.count")
					validatedMetrics["apachedruid.task.failed.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of failed tasks per emission period. This metric is only available if the `TaskCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
				case "apachedruid.task.pending.count":
					assert.False(t, validatedMetrics["apachedruid.task.pending.count"], "Found a duplicate in the metrics slice: apachedruid.task.pending.count")
					validatedMetrics["apachedruid.task.pending.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of current pending tasks. This metric is only available if the `TaskCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
				case "apachedruid.task.pending.time":
					assert.False(t, validatedMetrics["apachedruid.task.pending.time"], "Found a duplicate in the metrics slice: apachedruid.task.pending.time")
					validatedMetrics["apachedruid.task.pending.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken for a task to wait for running.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "task_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_id-val", attrVal.Str())
				case "apachedruid.task.run.time":
					assert.False(t, validatedMetrics["apachedruid.task.run.time"], "Found a duplicate in the metrics slice: apachedruid.task.run.time")
					validatedMetrics["apachedruid.task.run.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Milliseconds taken to run a task.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_status")
					assert.True(t, ok)
					assert.EqualValues(t, "task_status-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "task_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_id-val", attrVal.Str())
				case "apachedruid.task.running.count":
					assert.False(t, validatedMetrics["apachedruid.task.running.count"], "Found a duplicate in the metrics slice: apachedruid.task.running.count")
					validatedMetrics["apachedruid.task.running.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of current running tasks. This metric is only available if the `TaskCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
				case "apachedruid.task.segment_availability.wait.time":
					assert.False(t, validatedMetrics["apachedruid.task.segment_availability.wait.time"], "Found a duplicate in the metrics slice: apachedruid.task.segment_availability.wait.time")
					validatedMetrics["apachedruid.task.segment_availability.wait.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "The amount of milliseconds a batch indexing task waited for newly created segments to become available for querying.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("task_type")
					assert.True(t, ok)
					assert.EqualValues(t, "task_type-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("group_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_group_id-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("segment_availability_confirmed")
					assert.True(t, ok)
					assert.EqualValues(t, "task_segment_availability_confirmed-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("tags")
					assert.True(t, ok)
					assert.EqualValues(t, "task_tags-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task_id")
					assert.True(t, ok)
					assert.EqualValues(t, "task_id-val", attrVal.Str())
				case "apachedruid.task.success.count":
					assert.False(t, validatedMetrics["apachedruid.task.success.count"], "Found a duplicate in the metrics slice: apachedruid.task.success.count")
					validatedMetrics["apachedruid.task.success.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of successful tasks per emission period. This metric is only available if the `TaskCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
				case "apachedruid.task.waiting.count":
					assert.False(t, validatedMetrics["apachedruid.task.waiting.count"], "Found a duplicate in the metrics slice: apachedruid.task.waiting.count")
					validatedMetrics["apachedruid.task.waiting.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of current waiting tasks. This metric is only available if the `TaskCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("data_source")
					assert.True(t, ok)
					assert.EqualValues(t, "task_data_source-val", attrVal.Str())
				case "apachedruid.task_slot.blacklisted.count":
					assert.False(t, validatedMetrics["apachedruid.task_slot.blacklisted.count"], "Found a duplicate in the metrics slice: apachedruid.task_slot.blacklisted.count")
					validatedMetrics["apachedruid.task_slot.blacklisted.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of total task slots in blacklisted Middle Managers and Indexers per emission period. This metric is only available if the `TaskSlotCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("category")
					assert.True(t, ok)
					assert.EqualValues(t, "taskSlot_category-val", attrVal.Str())
				case "apachedruid.task_slot.idle.count":
					assert.False(t, validatedMetrics["apachedruid.task_slot.idle.count"], "Found a duplicate in the metrics slice: apachedruid.task_slot.idle.count")
					validatedMetrics["apachedruid.task_slot.idle.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of idle task slots per emission period. This metric is only available if the `TaskSlotCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("category")
					assert.True(t, ok)
					assert.EqualValues(t, "taskSlot_category-val", attrVal.Str())
				case "apachedruid.task_slot.lazy.count":
					assert.False(t, validatedMetrics["apachedruid.task_slot.lazy.count"], "Found a duplicate in the metrics slice: apachedruid.task_slot.lazy.count")
					validatedMetrics["apachedruid.task_slot.lazy.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of total task slots in lazy marked Middle Managers and Indexers per emission period. This metric is only available if the `TaskSlotCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("category")
					assert.True(t, ok)
					assert.EqualValues(t, "taskSlot_category-val", attrVal.Str())
				case "apachedruid.task_slot.total.count":
					assert.False(t, validatedMetrics["apachedruid.task_slot.total.count"], "Found a duplicate in the metrics slice: apachedruid.task_slot.total.count")
					validatedMetrics["apachedruid.task_slot.total.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of total task slots per emission period. This metric is only available if the `TaskSlotCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("category")
					assert.True(t, ok)
					assert.EqualValues(t, "taskSlot_category-val", attrVal.Str())
				case "apachedruid.task_slot.used.count":
					assert.False(t, validatedMetrics["apachedruid.task_slot.used.count"], "Found a duplicate in the metrics slice: apachedruid.task_slot.used.count")
					validatedMetrics["apachedruid.task_slot.used.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of busy task slots per emission period. This metric is only available if the `TaskSlotCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("category")
					assert.True(t, ok)
					assert.EqualValues(t, "taskSlot_category-val", attrVal.Str())
				case "apachedruid.tier.historical.count":
					assert.False(t, validatedMetrics["apachedruid.tier.historical.count"], "Found a duplicate in the metrics slice: apachedruid.tier.historical.count")
					validatedMetrics["apachedruid.tier.historical.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Number of available historical nodes in each tier.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "tier-val", attrVal.Str())
				case "apachedruid.tier.replication.factor":
					assert.False(t, validatedMetrics["apachedruid.tier.replication.factor"], "Found a duplicate in the metrics slice: apachedruid.tier.replication.factor")
					validatedMetrics["apachedruid.tier.replication.factor"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Configured maximum replication factor in each tier.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "tier-val", attrVal.Str())
				case "apachedruid.tier.required.capacity":
					assert.False(t, validatedMetrics["apachedruid.tier.required.capacity"], "Found a duplicate in the metrics slice: apachedruid.tier.required.capacity")
					validatedMetrics["apachedruid.tier.required.capacity"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total capacity in bytes required in each tier.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "tier-val", attrVal.Str())
				case "apachedruid.tier.total.capacity":
					assert.False(t, validatedMetrics["apachedruid.tier.total.capacity"], "Found a duplicate in the metrics slice: apachedruid.tier.total.capacity")
					validatedMetrics["apachedruid.tier.total.capacity"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Total capacity in bytes available in each tier.", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("tier")
					assert.True(t, ok)
					assert.EqualValues(t, "tier-val", attrVal.Str())
				case "apachedruid.worker.task.failed.count":
					assert.False(t, validatedMetrics["apachedruid.worker.task.failed.count"], "Found a duplicate in the metrics slice: apachedruid.worker.task.failed.count")
					validatedMetrics["apachedruid.worker.task.failed.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of failed tasks run on the reporting worker per emission period. This metric is only available if the `WorkerTaskCountStatsMonitor` module is included, and is only supported for Middle Manager nodes.", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("category")
					assert.True(t, ok)
					assert.EqualValues(t, "worker_category-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("worker_version")
					assert.True(t, ok)
					assert.EqualValues(t, "worker_version-val", attrVal.Str())
				case "apachedruid.worker.task.success.count":
					assert.False(t, validatedMetrics["apachedruid.worker.task.success.count"], "Found a duplicate in the metrics slice: apachedruid.worker.task.success.count")
					validatedMetrics["apachedruid.worker.task.success.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of successful tasks run on the reporting worker per emission period. This metric is only available if the `WorkerTaskCountStatsMonitor` module is included, and is only supported for Middle Manager nodes.", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("category")
					assert.True(t, ok)
					assert.EqualValues(t, "worker_category-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("worker_version")
					assert.True(t, ok)
					assert.EqualValues(t, "worker_version-val", attrVal.Str())
				case "apachedruid.worker.task_slot.idle.count":
					assert.False(t, validatedMetrics["apachedruid.worker.task_slot.idle.count"], "Found a duplicate in the metrics slice: apachedruid.worker.task_slot.idle.count")
					validatedMetrics["apachedruid.worker.task_slot.idle.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of idle task slots on the reporting worker per emission period. This metric is only available if the `WorkerTaskCountStatsMonitor` module is included, and is only supported for Middle Manager nodes.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("category")
					assert.True(t, ok)
					assert.EqualValues(t, "worker_category-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("worker_version")
					assert.True(t, ok)
					assert.EqualValues(t, "worker_version-val", attrVal.Str())
				case "apachedruid.worker.task_slot.total.count":
					assert.False(t, validatedMetrics["apachedruid.worker.task_slot.total.count"], "Found a duplicate in the metrics slice: apachedruid.worker.task_slot.total.count")
					validatedMetrics["apachedruid.worker.task_slot.total.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of total task slots on the reporting worker per emission period. This metric is only available if the `WorkerTaskCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("category")
					assert.True(t, ok)
					assert.EqualValues(t, "worker_category-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("worker_version")
					assert.True(t, ok)
					assert.EqualValues(t, "worker_version-val", attrVal.Str())
				case "apachedruid.worker.task_slot.used.count":
					assert.False(t, validatedMetrics["apachedruid.worker.task_slot.used.count"], "Found a duplicate in the metrics slice: apachedruid.worker.task_slot.used.count")
					validatedMetrics["apachedruid.worker.task_slot.used.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "Number of busy task slots on the reporting worker per emission period. This metric is only available if the `WorkerTaskCountStatsMonitor` module is included.", ms.At(i).Description())
					assert.Equal(t, "{slots}", ms.At(i).Unit())
					assert.Equal(t, true, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("category")
					assert.True(t, ok)
					assert.EqualValues(t, "worker_category-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("worker_version")
					assert.True(t, ok)
					assert.EqualValues(t, "worker_version-val", attrVal.Str())
				case "apachedruid.zk.connected":
					assert.False(t, validatedMetrics["apachedruid.zk.connected"], "Found a duplicate in the metrics slice: apachedruid.zk.connected")
					validatedMetrics["apachedruid.zk.connected"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Indicator of connection status. `1` for connected, `0` for disconnected. Emitted once per monitor period.", ms.At(i).Description())
					assert.Equal(t, "1", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "apachedruid.zk.reconnect.time":
					assert.False(t, validatedMetrics["apachedruid.zk.reconnect.time"], "Found a duplicate in the metrics slice: apachedruid.zk.reconnect.time")
					validatedMetrics["apachedruid.zk.reconnect.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "Amount of time, in milliseconds, that a server was disconnected from ZooKeeper before reconnecting. Emitted on reconnection. Not emitted if connection to ZooKeeper is permanently lost, because in this case, there is no reconnection.", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				}
			}
		})
	}
}
