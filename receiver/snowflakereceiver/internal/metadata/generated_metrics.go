// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
)

// MetricSettings provides common settings for a particular metric.
type MetricSettings struct {
	Enabled bool `mapstructure:"enabled"`
}

// MetricsSettings provides settings for snowflakereceiver metrics.
type MetricsSettings struct {
	SnowflakeBillingCloudServiceSum              MetricSettings `mapstructure:"snowflake.billing.cloud_service.sum"`
	SnowflakeBillingTotalCreditSum               MetricSettings `mapstructure:"snowflake.billing.total_credit.sum"`
	SnowflakeBillingVirtualWarehouseSum          MetricSettings `mapstructure:"snowflake.billing.virtual_warehouse.sum"`
	SnowflakeBillingWarehouseCloudServiceSum     MetricSettings `mapstructure:"snowflake.billing.warehouse.cloud_service.sum"`
	SnowflakeBillingWarehouseTotalCreditSum      MetricSettings `mapstructure:"snowflake.billing.warehouse.total_credit.sum"`
	SnowflakeBillingWarehouseVirtualWarehouseSum MetricSettings `mapstructure:"snowflake.billing.warehouse.virtual_warehouse.sum"`
	SnowflakeDatabaseBytesScannedAvg             MetricSettings `mapstructure:"snowflake.database.bytes_scanned.avg"`
	SnowflakeDatabaseQueryCount                  MetricSettings `mapstructure:"snowflake.database.query.count"`
	SnowflakeLoginsTotal                         MetricSettings `mapstructure:"snowflake.logins.total"`
	SnowflakePipeCreditsUsedSum                  MetricSettings `mapstructure:"snowflake.pipe.credits_used.sum"`
	SnowflakeQueryBlocked                        MetricSettings `mapstructure:"snowflake.query.blocked"`
	SnowflakeQueryBytesDeletedSum                MetricSettings `mapstructure:"snowflake.query.bytes_deleted.sum"`
	SnowflakeQueryBytesScannedSum                MetricSettings `mapstructure:"snowflake.query.bytes_scanned.sum"`
	SnowflakeQueryBytesSpilledLocalSum           MetricSettings `mapstructure:"snowflake.query.bytes_spilled.local.sum"`
	SnowflakeQueryBytesSpilledRemoteSum          MetricSettings `mapstructure:"snowflake.query.bytes_spilled.remote.sum"`
	SnowflakeQueryBytesWrittenSum                MetricSettings `mapstructure:"snowflake.query.bytes_written.sum"`
	SnowflakeQueryCompilationTimeSum             MetricSettings `mapstructure:"snowflake.query.compilation_time.sum"`
	SnowflakeQueryDataScannedCacheAvg            MetricSettings `mapstructure:"snowflake.query.data_scanned_cache.avg"`
	SnowflakeQueryExecuted                       MetricSettings `mapstructure:"snowflake.query.executed"`
	SnowflakeQueryExecutionTimeSum               MetricSettings `mapstructure:"snowflake.query.execution_time.sum"`
	SnowflakeQueryPartitionsScannedSum           MetricSettings `mapstructure:"snowflake.query.partitions_scanned.sum"`
	SnowflakeQueryQueuedOverload                 MetricSettings `mapstructure:"snowflake.query.queued_overload"`
	SnowflakeQueryQueuedProvision                MetricSettings `mapstructure:"snowflake.query.queued_provision"`
	SnowflakeQueuedOverloadTimeAvg               MetricSettings `mapstructure:"snowflake.queued_overload_time.avg"`
	SnowflakeQueuedOverloadTimeSum               MetricSettings `mapstructure:"snowflake.queued_overload_time.sum"`
	SnowflakeQueuedProvisioningTimeAvg           MetricSettings `mapstructure:"snowflake.queued_provisioning_time.avg"`
	SnowflakeQueuedProvisioningTimeSum           MetricSettings `mapstructure:"snowflake.queued_provisioning_time.sum"`
	SnowflakeQueuedRepairTimeAvg                 MetricSettings `mapstructure:"snowflake.queued_repair_time.avg"`
	SnowflakeQueuedRepairTimeSum                 MetricSettings `mapstructure:"snowflake.queued_repair_time.sum"`
	SnowflakeRowsDeletedSum                      MetricSettings `mapstructure:"snowflake.rows_deleted.sum"`
	SnowflakeRowsInsertedSum                     MetricSettings `mapstructure:"snowflake.rows_inserted.sum"`
	SnowflakeRowsProducedSum                     MetricSettings `mapstructure:"snowflake.rows_produced.sum"`
	SnowflakeRowsUnloadedSum                     MetricSettings `mapstructure:"snowflake.rows_unloaded.sum"`
	SnowflakeRowsUpdatedSum                      MetricSettings `mapstructure:"snowflake.rows_updated.sum"`
	SnowflakeSessionIDCount                      MetricSettings `mapstructure:"snowflake.session_id.count"`
	SnowflakeStorageFailsafeBytesTotal           MetricSettings `mapstructure:"snowflake.storage.failsafe_bytes.total"`
	SnowflakeStorageStageBytesTotal              MetricSettings `mapstructure:"snowflake.storage.stage_bytes.total"`
	SnowflakeStorageStorageBytesTotal            MetricSettings `mapstructure:"snowflake.storage.storage_bytes.total"`
	SnowflakeTotalElapsedTimeAvg                 MetricSettings `mapstructure:"snowflake.total_elapsed_time.avg"`
	SnowflakeTotalElapsedTimeSum                 MetricSettings `mapstructure:"snowflake.total_elapsed_time.sum"`
}

func DefaultMetricsSettings() MetricsSettings {
	return MetricsSettings{
		SnowflakeBillingCloudServiceSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeBillingTotalCreditSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeBillingVirtualWarehouseSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeBillingWarehouseCloudServiceSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeBillingWarehouseTotalCreditSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeBillingWarehouseVirtualWarehouseSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeDatabaseBytesScannedAvg: MetricSettings{
			Enabled: true,
		},
		SnowflakeDatabaseQueryCount: MetricSettings{
			Enabled: true,
		},
		SnowflakeLoginsTotal: MetricSettings{
			Enabled: true,
		},
		SnowflakePipeCreditsUsedSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryBlocked: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryBytesDeletedSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryBytesScannedSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryBytesSpilledLocalSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryBytesSpilledRemoteSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryBytesWrittenSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryCompilationTimeSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryDataScannedCacheAvg: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryExecuted: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryExecutionTimeSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryPartitionsScannedSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryQueuedOverload: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryQueuedProvision: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueuedOverloadTimeAvg: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueuedOverloadTimeSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueuedProvisioningTimeAvg: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueuedProvisioningTimeSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueuedRepairTimeAvg: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueuedRepairTimeSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeRowsDeletedSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeRowsInsertedSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeRowsProducedSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeRowsUnloadedSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeRowsUpdatedSum: MetricSettings{
			Enabled: true,
		},
		SnowflakeSessionIDCount: MetricSettings{
			Enabled: true,
		},
		SnowflakeStorageFailsafeBytesTotal: MetricSettings{
			Enabled: true,
		},
		SnowflakeStorageStageBytesTotal: MetricSettings{
			Enabled: true,
		},
		SnowflakeStorageStorageBytesTotal: MetricSettings{
			Enabled: true,
		},
		SnowflakeTotalElapsedTimeAvg: MetricSettings{
			Enabled: true,
		},
		SnowflakeTotalElapsedTimeSum: MetricSettings{
			Enabled: true,
		},
	}
}

type metricSnowflakeBillingCloudServiceSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.cloud_service.sum metric with initial data.
func (m *metricSnowflakeBillingCloudServiceSum) init() {
	m.data.SetName("snowflake.billing.cloud_service.sum")
	m.data.SetDescription("cloud services sum")
	m.data.SetUnit("credits")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingCloudServiceSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("service.type", serviceTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingCloudServiceSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingCloudServiceSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingCloudServiceSum(settings MetricSettings) metricSnowflakeBillingCloudServiceSum {
	m := metricSnowflakeBillingCloudServiceSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingTotalCreditSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.total_credit.sum metric with initial data.
func (m *metricSnowflakeBillingTotalCreditSum) init() {
	m.data.SetName("snowflake.billing.total_credit.sum")
	m.data.SetDescription("billing total sum")
	m.data.SetUnit("credits")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingTotalCreditSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("service.type", serviceTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingTotalCreditSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingTotalCreditSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingTotalCreditSum(settings MetricSettings) metricSnowflakeBillingTotalCreditSum {
	m := metricSnowflakeBillingTotalCreditSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingVirtualWarehouseSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.virtual_warehouse.sum metric with initial data.
func (m *metricSnowflakeBillingVirtualWarehouseSum) init() {
	m.data.SetName("snowflake.billing.virtual_warehouse.sum")
	m.data.SetDescription("compute credits used sum")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingVirtualWarehouseSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("service.type", serviceTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingVirtualWarehouseSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingVirtualWarehouseSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingVirtualWarehouseSum(settings MetricSettings) metricSnowflakeBillingVirtualWarehouseSum {
	m := metricSnowflakeBillingVirtualWarehouseSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingWarehouseCloudServiceSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.warehouse.cloud_service.sum metric with initial data.
func (m *metricSnowflakeBillingWarehouseCloudServiceSum) init() {
	m.data.SetName("snowflake.billing.warehouse.cloud_service.sum")
	m.data.SetDescription("ware billing for cloud service")
	m.data.SetUnit("credits")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingWarehouseCloudServiceSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingWarehouseCloudServiceSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingWarehouseCloudServiceSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingWarehouseCloudServiceSum(settings MetricSettings) metricSnowflakeBillingWarehouseCloudServiceSum {
	m := metricSnowflakeBillingWarehouseCloudServiceSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingWarehouseTotalCreditSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.warehouse.total_credit.sum metric with initial data.
func (m *metricSnowflakeBillingWarehouseTotalCreditSum) init() {
	m.data.SetName("snowflake.billing.warehouse.total_credit.sum")
	m.data.SetDescription("warehouse total credits")
	m.data.SetUnit("credits")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingWarehouseTotalCreditSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingWarehouseTotalCreditSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingWarehouseTotalCreditSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingWarehouseTotalCreditSum(settings MetricSettings) metricSnowflakeBillingWarehouseTotalCreditSum {
	m := metricSnowflakeBillingWarehouseTotalCreditSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingWarehouseVirtualWarehouseSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.warehouse.virtual_warehouse.sum metric with initial data.
func (m *metricSnowflakeBillingWarehouseVirtualWarehouseSum) init() {
	m.data.SetName("snowflake.billing.warehouse.virtual_warehouse.sum")
	m.data.SetDescription("virtual warehouse credits used")
	m.data.SetUnit("credits")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingWarehouseVirtualWarehouseSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingWarehouseVirtualWarehouseSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingWarehouseVirtualWarehouseSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingWarehouseVirtualWarehouseSum(settings MetricSettings) metricSnowflakeBillingWarehouseVirtualWarehouseSum {
	m := metricSnowflakeBillingWarehouseVirtualWarehouseSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeDatabaseBytesScannedAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.database.bytes_scanned.avg metric with initial data.
func (m *metricSnowflakeDatabaseBytesScannedAvg) init() {
	m.data.SetName("snowflake.database.bytes_scanned.avg")
	m.data.SetDescription("average bytes scanned")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeDatabaseBytesScannedAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeDatabaseBytesScannedAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeDatabaseBytesScannedAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeDatabaseBytesScannedAvg(settings MetricSettings) metricSnowflakeDatabaseBytesScannedAvg {
	m := metricSnowflakeDatabaseBytesScannedAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeDatabaseQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.database.query.count metric with initial data.
func (m *metricSnowflakeDatabaseQueryCount) init() {
	m.data.SetName("snowflake.database.query.count")
	m.data.SetDescription("total number of queries")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeDatabaseQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeDatabaseQueryCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeDatabaseQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeDatabaseQueryCount(settings MetricSettings) metricSnowflakeDatabaseQueryCount {
	m := metricSnowflakeDatabaseQueryCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeLoginsTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.logins.total metric with initial data.
func (m *metricSnowflakeLoginsTotal) init() {
	m.data.SetName("snowflake.logins.total")
	m.data.SetDescription("total logins")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeLoginsTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, errorMessageAttributeValue string, reportedClientTypeAttributeValue string, isSuccessAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("reported.client.type", reportedClientTypeAttributeValue)
	dp.Attributes().PutStr("is.success", isSuccessAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeLoginsTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeLoginsTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeLoginsTotal(settings MetricSettings) metricSnowflakeLoginsTotal {
	m := metricSnowflakeLoginsTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakePipeCreditsUsedSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.pipe.credits_used.sum metric with initial data.
func (m *metricSnowflakePipeCreditsUsedSum) init() {
	m.data.SetName("snowflake.pipe.credits_used.sum")
	m.data.SetDescription("snow pipe credits consumed")
	m.data.SetUnit("credits")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakePipeCreditsUsedSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, pipeNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("pipe.name", pipeNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakePipeCreditsUsedSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakePipeCreditsUsedSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakePipeCreditsUsedSum(settings MetricSettings) metricSnowflakePipeCreditsUsedSum {
	m := metricSnowflakePipeCreditsUsedSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBlocked struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.blocked metric with initial data.
func (m *metricSnowflakeQueryBlocked) init() {
	m.data.SetName("snowflake.query.blocked")
	m.data.SetDescription("number of blocked queries")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBlocked) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBlocked) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBlocked) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBlocked(settings MetricSettings) metricSnowflakeQueryBlocked {
	m := metricSnowflakeQueryBlocked{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesDeletedSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_deleted.sum metric with initial data.
func (m *metricSnowflakeQueryBytesDeletedSum) init() {
	m.data.SetName("snowflake.query.bytes_deleted.sum")
	m.data.SetDescription("total bytes bytes deleted")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesDeletedSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesDeletedSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesDeletedSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesDeletedSum(settings MetricSettings) metricSnowflakeQueryBytesDeletedSum {
	m := metricSnowflakeQueryBytesDeletedSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesScannedSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_scanned.sum metric with initial data.
func (m *metricSnowflakeQueryBytesScannedSum) init() {
	m.data.SetName("snowflake.query.bytes_scanned.sum")
	m.data.SetDescription("total bytes scanend")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesScannedSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesScannedSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesScannedSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesScannedSum(settings MetricSettings) metricSnowflakeQueryBytesScannedSum {
	m := metricSnowflakeQueryBytesScannedSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesSpilledLocalSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_spilled.local.sum metric with initial data.
func (m *metricSnowflakeQueryBytesSpilledLocalSum) init() {
	m.data.SetName("snowflake.query.bytes_spilled.local.sum")
	m.data.SetDescription("total bytes spilled")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesSpilledLocalSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesSpilledLocalSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesSpilledLocalSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesSpilledLocalSum(settings MetricSettings) metricSnowflakeQueryBytesSpilledLocalSum {
	m := metricSnowflakeQueryBytesSpilledLocalSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesSpilledRemoteSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_spilled.remote.sum metric with initial data.
func (m *metricSnowflakeQueryBytesSpilledRemoteSum) init() {
	m.data.SetName("snowflake.query.bytes_spilled.remote.sum")
	m.data.SetDescription("remote bytes spilled")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesSpilledRemoteSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesSpilledRemoteSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesSpilledRemoteSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesSpilledRemoteSum(settings MetricSettings) metricSnowflakeQueryBytesSpilledRemoteSum {
	m := metricSnowflakeQueryBytesSpilledRemoteSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesWrittenSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_written.sum metric with initial data.
func (m *metricSnowflakeQueryBytesWrittenSum) init() {
	m.data.SetName("snowflake.query.bytes_written.sum")
	m.data.SetDescription("total bytes bytes")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesWrittenSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesWrittenSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesWrittenSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesWrittenSum(settings MetricSettings) metricSnowflakeQueryBytesWrittenSum {
	m := metricSnowflakeQueryBytesWrittenSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryCompilationTimeSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.compilation_time.sum metric with initial data.
func (m *metricSnowflakeQueryCompilationTimeSum) init() {
	m.data.SetName("snowflake.query.compilation_time.sum")
	m.data.SetDescription("total compilation time")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryCompilationTimeSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryCompilationTimeSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryCompilationTimeSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryCompilationTimeSum(settings MetricSettings) metricSnowflakeQueryCompilationTimeSum {
	m := metricSnowflakeQueryCompilationTimeSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryDataScannedCacheAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.data_scanned_cache.avg metric with initial data.
func (m *metricSnowflakeQueryDataScannedCacheAvg) init() {
	m.data.SetName("snowflake.query.data_scanned_cache.avg")
	m.data.SetDescription("average data scanned cache")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryDataScannedCacheAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryDataScannedCacheAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryDataScannedCacheAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryDataScannedCacheAvg(settings MetricSettings) metricSnowflakeQueryDataScannedCacheAvg {
	m := metricSnowflakeQueryDataScannedCacheAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryExecuted struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.executed metric with initial data.
func (m *metricSnowflakeQueryExecuted) init() {
	m.data.SetName("snowflake.query.executed")
	m.data.SetDescription("number of executed queries")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryExecuted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryExecuted) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryExecuted) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryExecuted(settings MetricSettings) metricSnowflakeQueryExecuted {
	m := metricSnowflakeQueryExecuted{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryExecutionTimeSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.execution_time.sum metric with initial data.
func (m *metricSnowflakeQueryExecutionTimeSum) init() {
	m.data.SetName("snowflake.query.execution_time.sum")
	m.data.SetDescription("query execution time")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryExecutionTimeSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryExecutionTimeSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryExecutionTimeSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryExecutionTimeSum(settings MetricSettings) metricSnowflakeQueryExecutionTimeSum {
	m := metricSnowflakeQueryExecutionTimeSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryPartitionsScannedSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.partitions_scanned.sum metric with initial data.
func (m *metricSnowflakeQueryPartitionsScannedSum) init() {
	m.data.SetName("snowflake.query.partitions_scanned.sum")
	m.data.SetDescription("number of partitions scanned")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryPartitionsScannedSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryPartitionsScannedSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryPartitionsScannedSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryPartitionsScannedSum(settings MetricSettings) metricSnowflakeQueryPartitionsScannedSum {
	m := metricSnowflakeQueryPartitionsScannedSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryQueuedOverload struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.queued_overload metric with initial data.
func (m *metricSnowflakeQueryQueuedOverload) init() {
	m.data.SetName("snowflake.query.queued_overload")
	m.data.SetDescription("queue overload count")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryQueuedOverload) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryQueuedOverload) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryQueuedOverload) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryQueuedOverload(settings MetricSettings) metricSnowflakeQueryQueuedOverload {
	m := metricSnowflakeQueryQueuedOverload{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryQueuedProvision struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.queued_provision metric with initial data.
func (m *metricSnowflakeQueryQueuedProvision) init() {
	m.data.SetName("snowflake.query.queued_provision")
	m.data.SetDescription("queue provision count")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryQueuedProvision) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryQueuedProvision) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryQueuedProvision) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryQueuedProvision(settings MetricSettings) metricSnowflakeQueryQueuedProvision {
	m := metricSnowflakeQueryQueuedProvision{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedOverloadTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_overload_time.avg metric with initial data.
func (m *metricSnowflakeQueuedOverloadTimeAvg) init() {
	m.data.SetName("snowflake.queued_overload_time.avg")
	m.data.SetDescription("average queued overload time")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedOverloadTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedOverloadTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedOverloadTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedOverloadTimeAvg(settings MetricSettings) metricSnowflakeQueuedOverloadTimeAvg {
	m := metricSnowflakeQueuedOverloadTimeAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedOverloadTimeSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_overload_time.sum metric with initial data.
func (m *metricSnowflakeQueuedOverloadTimeSum) init() {
	m.data.SetName("snowflake.queued_overload_time.sum")
	m.data.SetDescription("total queued overload time")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedOverloadTimeSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedOverloadTimeSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedOverloadTimeSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedOverloadTimeSum(settings MetricSettings) metricSnowflakeQueuedOverloadTimeSum {
	m := metricSnowflakeQueuedOverloadTimeSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedProvisioningTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_provisioning_time.avg metric with initial data.
func (m *metricSnowflakeQueuedProvisioningTimeAvg) init() {
	m.data.SetName("snowflake.queued_provisioning_time.avg")
	m.data.SetDescription("average queued provisioning time")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedProvisioningTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedProvisioningTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedProvisioningTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedProvisioningTimeAvg(settings MetricSettings) metricSnowflakeQueuedProvisioningTimeAvg {
	m := metricSnowflakeQueuedProvisioningTimeAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedProvisioningTimeSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_provisioning_time.sum metric with initial data.
func (m *metricSnowflakeQueuedProvisioningTimeSum) init() {
	m.data.SetName("snowflake.queued_provisioning_time.sum")
	m.data.SetDescription("total queued provisioning time")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedProvisioningTimeSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedProvisioningTimeSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedProvisioningTimeSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedProvisioningTimeSum(settings MetricSettings) metricSnowflakeQueuedProvisioningTimeSum {
	m := metricSnowflakeQueuedProvisioningTimeSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedRepairTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_repair_time.avg metric with initial data.
func (m *metricSnowflakeQueuedRepairTimeAvg) init() {
	m.data.SetName("snowflake.queued_repair_time.avg")
	m.data.SetDescription("average queued repair time")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedRepairTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedRepairTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedRepairTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedRepairTimeAvg(settings MetricSettings) metricSnowflakeQueuedRepairTimeAvg {
	m := metricSnowflakeQueuedRepairTimeAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedRepairTimeSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_repair_time.sum metric with initial data.
func (m *metricSnowflakeQueuedRepairTimeSum) init() {
	m.data.SetName("snowflake.queued_repair_time.sum")
	m.data.SetDescription("total queued repair time")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedRepairTimeSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedRepairTimeSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedRepairTimeSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedRepairTimeSum(settings MetricSettings) metricSnowflakeQueuedRepairTimeSum {
	m := metricSnowflakeQueuedRepairTimeSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsDeletedSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_deleted.sum metric with initial data.
func (m *metricSnowflakeRowsDeletedSum) init() {
	m.data.SetName("snowflake.rows_deleted.sum")
	m.data.SetDescription("rows deleted")
	m.data.SetUnit("rows")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsDeletedSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsDeletedSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsDeletedSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsDeletedSum(settings MetricSettings) metricSnowflakeRowsDeletedSum {
	m := metricSnowflakeRowsDeletedSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsInsertedSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_inserted.sum metric with initial data.
func (m *metricSnowflakeRowsInsertedSum) init() {
	m.data.SetName("snowflake.rows_inserted.sum")
	m.data.SetDescription("rows inserted")
	m.data.SetUnit("rows")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsInsertedSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsInsertedSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsInsertedSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsInsertedSum(settings MetricSettings) metricSnowflakeRowsInsertedSum {
	m := metricSnowflakeRowsInsertedSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsProducedSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_produced.sum metric with initial data.
func (m *metricSnowflakeRowsProducedSum) init() {
	m.data.SetName("snowflake.rows_produced.sum")
	m.data.SetDescription("rows produced")
	m.data.SetUnit("rows")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsProducedSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsProducedSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsProducedSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsProducedSum(settings MetricSettings) metricSnowflakeRowsProducedSum {
	m := metricSnowflakeRowsProducedSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsUnloadedSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_unloaded.sum metric with initial data.
func (m *metricSnowflakeRowsUnloadedSum) init() {
	m.data.SetName("snowflake.rows_unloaded.sum")
	m.data.SetDescription("rows unloaded")
	m.data.SetUnit("rows")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsUnloadedSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsUnloadedSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsUnloadedSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsUnloadedSum(settings MetricSettings) metricSnowflakeRowsUnloadedSum {
	m := metricSnowflakeRowsUnloadedSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsUpdatedSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_updated.sum metric with initial data.
func (m *metricSnowflakeRowsUpdatedSum) init() {
	m.data.SetName("snowflake.rows_updated.sum")
	m.data.SetDescription("rows updated")
	m.data.SetUnit("rows")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsUpdatedSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsUpdatedSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsUpdatedSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsUpdatedSum(settings MetricSettings) metricSnowflakeRowsUpdatedSum {
	m := metricSnowflakeRowsUpdatedSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeSessionIDCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.session_id.count metric with initial data.
func (m *metricSnowflakeSessionIDCount) init() {
	m.data.SetName("snowflake.session_id.count")
	m.data.SetDescription("distinct session ids")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeSessionIDCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, userNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("user.name", userNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeSessionIDCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeSessionIDCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeSessionIDCount(settings MetricSettings) metricSnowflakeSessionIDCount {
	m := metricSnowflakeSessionIDCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeStorageFailsafeBytesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.storage.failsafe_bytes.total metric with initial data.
func (m *metricSnowflakeStorageFailsafeBytesTotal) init() {
	m.data.SetName("snowflake.storage.failsafe_bytes.total")
	m.data.SetDescription("total failsafe bytes in snowflake")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
}

func (m *metricSnowflakeStorageFailsafeBytesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeStorageFailsafeBytesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeStorageFailsafeBytesTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeStorageFailsafeBytesTotal(settings MetricSettings) metricSnowflakeStorageFailsafeBytesTotal {
	m := metricSnowflakeStorageFailsafeBytesTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeStorageStageBytesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.storage.stage_bytes.total metric with initial data.
func (m *metricSnowflakeStorageStageBytesTotal) init() {
	m.data.SetName("snowflake.storage.stage_bytes.total")
	m.data.SetDescription("total stage bytes in snowflake")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
}

func (m *metricSnowflakeStorageStageBytesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeStorageStageBytesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeStorageStageBytesTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeStorageStageBytesTotal(settings MetricSettings) metricSnowflakeStorageStageBytesTotal {
	m := metricSnowflakeStorageStageBytesTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeStorageStorageBytesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.storage.storage_bytes.total metric with initial data.
func (m *metricSnowflakeStorageStorageBytesTotal) init() {
	m.data.SetName("snowflake.storage.storage_bytes.total")
	m.data.SetDescription("total storage bytes in snowflake")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
}

func (m *metricSnowflakeStorageStorageBytesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeStorageStorageBytesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeStorageStorageBytesTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeStorageStorageBytesTotal(settings MetricSettings) metricSnowflakeStorageStorageBytesTotal {
	m := metricSnowflakeStorageStorageBytesTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeTotalElapsedTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.total_elapsed_time.avg metric with initial data.
func (m *metricSnowflakeTotalElapsedTimeAvg) init() {
	m.data.SetName("snowflake.total_elapsed_time.avg")
	m.data.SetDescription("average elapsed time")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeTotalElapsedTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeTotalElapsedTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeTotalElapsedTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeTotalElapsedTimeAvg(settings MetricSettings) metricSnowflakeTotalElapsedTimeAvg {
	m := metricSnowflakeTotalElapsedTimeAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeTotalElapsedTimeSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.total_elapsed_time.sum metric with initial data.
func (m *metricSnowflakeTotalElapsedTimeSum) init() {
	m.data.SetName("snowflake.total_elapsed_time.sum")
	m.data.SetDescription("total elapsed time")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeTotalElapsedTimeSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema.name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution.status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error.message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query.type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse.name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database.name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse.size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeTotalElapsedTimeSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeTotalElapsedTimeSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeTotalElapsedTimeSum(settings MetricSettings) metricSnowflakeTotalElapsedTimeSum {
	m := metricSnowflakeTotalElapsedTimeSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user settings.
type MetricsBuilder struct {
	startTime                                          pcommon.Timestamp   // start time that will be applied to all recorded data points.
	metricsCapacity                                    int                 // maximum observed number of metrics per resource.
	resourceCapacity                                   int                 // maximum observed number of resource attributes.
	metricsBuffer                                      pmetric.Metrics     // accumulates metrics data before emitting.
	buildInfo                                          component.BuildInfo // contains version information
	metricSnowflakeBillingCloudServiceSum              metricSnowflakeBillingCloudServiceSum
	metricSnowflakeBillingTotalCreditSum               metricSnowflakeBillingTotalCreditSum
	metricSnowflakeBillingVirtualWarehouseSum          metricSnowflakeBillingVirtualWarehouseSum
	metricSnowflakeBillingWarehouseCloudServiceSum     metricSnowflakeBillingWarehouseCloudServiceSum
	metricSnowflakeBillingWarehouseTotalCreditSum      metricSnowflakeBillingWarehouseTotalCreditSum
	metricSnowflakeBillingWarehouseVirtualWarehouseSum metricSnowflakeBillingWarehouseVirtualWarehouseSum
	metricSnowflakeDatabaseBytesScannedAvg             metricSnowflakeDatabaseBytesScannedAvg
	metricSnowflakeDatabaseQueryCount                  metricSnowflakeDatabaseQueryCount
	metricSnowflakeLoginsTotal                         metricSnowflakeLoginsTotal
	metricSnowflakePipeCreditsUsedSum                  metricSnowflakePipeCreditsUsedSum
	metricSnowflakeQueryBlocked                        metricSnowflakeQueryBlocked
	metricSnowflakeQueryBytesDeletedSum                metricSnowflakeQueryBytesDeletedSum
	metricSnowflakeQueryBytesScannedSum                metricSnowflakeQueryBytesScannedSum
	metricSnowflakeQueryBytesSpilledLocalSum           metricSnowflakeQueryBytesSpilledLocalSum
	metricSnowflakeQueryBytesSpilledRemoteSum          metricSnowflakeQueryBytesSpilledRemoteSum
	metricSnowflakeQueryBytesWrittenSum                metricSnowflakeQueryBytesWrittenSum
	metricSnowflakeQueryCompilationTimeSum             metricSnowflakeQueryCompilationTimeSum
	metricSnowflakeQueryDataScannedCacheAvg            metricSnowflakeQueryDataScannedCacheAvg
	metricSnowflakeQueryExecuted                       metricSnowflakeQueryExecuted
	metricSnowflakeQueryExecutionTimeSum               metricSnowflakeQueryExecutionTimeSum
	metricSnowflakeQueryPartitionsScannedSum           metricSnowflakeQueryPartitionsScannedSum
	metricSnowflakeQueryQueuedOverload                 metricSnowflakeQueryQueuedOverload
	metricSnowflakeQueryQueuedProvision                metricSnowflakeQueryQueuedProvision
	metricSnowflakeQueuedOverloadTimeAvg               metricSnowflakeQueuedOverloadTimeAvg
	metricSnowflakeQueuedOverloadTimeSum               metricSnowflakeQueuedOverloadTimeSum
	metricSnowflakeQueuedProvisioningTimeAvg           metricSnowflakeQueuedProvisioningTimeAvg
	metricSnowflakeQueuedProvisioningTimeSum           metricSnowflakeQueuedProvisioningTimeSum
	metricSnowflakeQueuedRepairTimeAvg                 metricSnowflakeQueuedRepairTimeAvg
	metricSnowflakeQueuedRepairTimeSum                 metricSnowflakeQueuedRepairTimeSum
	metricSnowflakeRowsDeletedSum                      metricSnowflakeRowsDeletedSum
	metricSnowflakeRowsInsertedSum                     metricSnowflakeRowsInsertedSum
	metricSnowflakeRowsProducedSum                     metricSnowflakeRowsProducedSum
	metricSnowflakeRowsUnloadedSum                     metricSnowflakeRowsUnloadedSum
	metricSnowflakeRowsUpdatedSum                      metricSnowflakeRowsUpdatedSum
	metricSnowflakeSessionIDCount                      metricSnowflakeSessionIDCount
	metricSnowflakeStorageFailsafeBytesTotal           metricSnowflakeStorageFailsafeBytesTotal
	metricSnowflakeStorageStageBytesTotal              metricSnowflakeStorageStageBytesTotal
	metricSnowflakeStorageStorageBytesTotal            metricSnowflakeStorageStorageBytesTotal
	metricSnowflakeTotalElapsedTimeAvg                 metricSnowflakeTotalElapsedTimeAvg
	metricSnowflakeTotalElapsedTimeSum                 metricSnowflakeTotalElapsedTimeSum
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func NewMetricsBuilder(settings MetricsSettings, buildInfo component.BuildInfo, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		startTime:                                          pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                                      pmetric.NewMetrics(),
		buildInfo:                                          buildInfo,
		metricSnowflakeBillingCloudServiceSum:              newMetricSnowflakeBillingCloudServiceSum(settings.SnowflakeBillingCloudServiceSum),
		metricSnowflakeBillingTotalCreditSum:               newMetricSnowflakeBillingTotalCreditSum(settings.SnowflakeBillingTotalCreditSum),
		metricSnowflakeBillingVirtualWarehouseSum:          newMetricSnowflakeBillingVirtualWarehouseSum(settings.SnowflakeBillingVirtualWarehouseSum),
		metricSnowflakeBillingWarehouseCloudServiceSum:     newMetricSnowflakeBillingWarehouseCloudServiceSum(settings.SnowflakeBillingWarehouseCloudServiceSum),
		metricSnowflakeBillingWarehouseTotalCreditSum:      newMetricSnowflakeBillingWarehouseTotalCreditSum(settings.SnowflakeBillingWarehouseTotalCreditSum),
		metricSnowflakeBillingWarehouseVirtualWarehouseSum: newMetricSnowflakeBillingWarehouseVirtualWarehouseSum(settings.SnowflakeBillingWarehouseVirtualWarehouseSum),
		metricSnowflakeDatabaseBytesScannedAvg:             newMetricSnowflakeDatabaseBytesScannedAvg(settings.SnowflakeDatabaseBytesScannedAvg),
		metricSnowflakeDatabaseQueryCount:                  newMetricSnowflakeDatabaseQueryCount(settings.SnowflakeDatabaseQueryCount),
		metricSnowflakeLoginsTotal:                         newMetricSnowflakeLoginsTotal(settings.SnowflakeLoginsTotal),
		metricSnowflakePipeCreditsUsedSum:                  newMetricSnowflakePipeCreditsUsedSum(settings.SnowflakePipeCreditsUsedSum),
		metricSnowflakeQueryBlocked:                        newMetricSnowflakeQueryBlocked(settings.SnowflakeQueryBlocked),
		metricSnowflakeQueryBytesDeletedSum:                newMetricSnowflakeQueryBytesDeletedSum(settings.SnowflakeQueryBytesDeletedSum),
		metricSnowflakeQueryBytesScannedSum:                newMetricSnowflakeQueryBytesScannedSum(settings.SnowflakeQueryBytesScannedSum),
		metricSnowflakeQueryBytesSpilledLocalSum:           newMetricSnowflakeQueryBytesSpilledLocalSum(settings.SnowflakeQueryBytesSpilledLocalSum),
		metricSnowflakeQueryBytesSpilledRemoteSum:          newMetricSnowflakeQueryBytesSpilledRemoteSum(settings.SnowflakeQueryBytesSpilledRemoteSum),
		metricSnowflakeQueryBytesWrittenSum:                newMetricSnowflakeQueryBytesWrittenSum(settings.SnowflakeQueryBytesWrittenSum),
		metricSnowflakeQueryCompilationTimeSum:             newMetricSnowflakeQueryCompilationTimeSum(settings.SnowflakeQueryCompilationTimeSum),
		metricSnowflakeQueryDataScannedCacheAvg:            newMetricSnowflakeQueryDataScannedCacheAvg(settings.SnowflakeQueryDataScannedCacheAvg),
		metricSnowflakeQueryExecuted:                       newMetricSnowflakeQueryExecuted(settings.SnowflakeQueryExecuted),
		metricSnowflakeQueryExecutionTimeSum:               newMetricSnowflakeQueryExecutionTimeSum(settings.SnowflakeQueryExecutionTimeSum),
		metricSnowflakeQueryPartitionsScannedSum:           newMetricSnowflakeQueryPartitionsScannedSum(settings.SnowflakeQueryPartitionsScannedSum),
		metricSnowflakeQueryQueuedOverload:                 newMetricSnowflakeQueryQueuedOverload(settings.SnowflakeQueryQueuedOverload),
		metricSnowflakeQueryQueuedProvision:                newMetricSnowflakeQueryQueuedProvision(settings.SnowflakeQueryQueuedProvision),
		metricSnowflakeQueuedOverloadTimeAvg:               newMetricSnowflakeQueuedOverloadTimeAvg(settings.SnowflakeQueuedOverloadTimeAvg),
		metricSnowflakeQueuedOverloadTimeSum:               newMetricSnowflakeQueuedOverloadTimeSum(settings.SnowflakeQueuedOverloadTimeSum),
		metricSnowflakeQueuedProvisioningTimeAvg:           newMetricSnowflakeQueuedProvisioningTimeAvg(settings.SnowflakeQueuedProvisioningTimeAvg),
		metricSnowflakeQueuedProvisioningTimeSum:           newMetricSnowflakeQueuedProvisioningTimeSum(settings.SnowflakeQueuedProvisioningTimeSum),
		metricSnowflakeQueuedRepairTimeAvg:                 newMetricSnowflakeQueuedRepairTimeAvg(settings.SnowflakeQueuedRepairTimeAvg),
		metricSnowflakeQueuedRepairTimeSum:                 newMetricSnowflakeQueuedRepairTimeSum(settings.SnowflakeQueuedRepairTimeSum),
		metricSnowflakeRowsDeletedSum:                      newMetricSnowflakeRowsDeletedSum(settings.SnowflakeRowsDeletedSum),
		metricSnowflakeRowsInsertedSum:                     newMetricSnowflakeRowsInsertedSum(settings.SnowflakeRowsInsertedSum),
		metricSnowflakeRowsProducedSum:                     newMetricSnowflakeRowsProducedSum(settings.SnowflakeRowsProducedSum),
		metricSnowflakeRowsUnloadedSum:                     newMetricSnowflakeRowsUnloadedSum(settings.SnowflakeRowsUnloadedSum),
		metricSnowflakeRowsUpdatedSum:                      newMetricSnowflakeRowsUpdatedSum(settings.SnowflakeRowsUpdatedSum),
		metricSnowflakeSessionIDCount:                      newMetricSnowflakeSessionIDCount(settings.SnowflakeSessionIDCount),
		metricSnowflakeStorageFailsafeBytesTotal:           newMetricSnowflakeStorageFailsafeBytesTotal(settings.SnowflakeStorageFailsafeBytesTotal),
		metricSnowflakeStorageStageBytesTotal:              newMetricSnowflakeStorageStageBytesTotal(settings.SnowflakeStorageStageBytesTotal),
		metricSnowflakeStorageStorageBytesTotal:            newMetricSnowflakeStorageStorageBytesTotal(settings.SnowflakeStorageStorageBytesTotal),
		metricSnowflakeTotalElapsedTimeAvg:                 newMetricSnowflakeTotalElapsedTimeAvg(settings.SnowflakeTotalElapsedTimeAvg),
		metricSnowflakeTotalElapsedTimeSum:                 newMetricSnowflakeTotalElapsedTimeSum(settings.SnowflakeTotalElapsedTimeSum),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
	if mb.resourceCapacity < rm.Resource().Attributes().Len() {
		mb.resourceCapacity = rm.Resource().Attributes().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption func(pmetric.ResourceMetrics)

// WithSnowflakeAccountName sets provided value as "snowflake.account.name" attribute for current resource.
func WithSnowflakeAccountName(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().PutStr("snowflake.account.name", val)
	}
}

// WithSnowflakeUsername sets provided value as "snowflake.username" attribute for current resource.
func WithSnowflakeUsername(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().PutStr("snowflake.username", val)
	}
}

// WithSnowflakeWarehouseName sets provided value as "snowflake.warehouse.name" attribute for current resource.
func WithSnowflakeWarehouseName(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().PutStr("snowflake.warehouse.name", val)
	}
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	}
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(rmo ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	rm.Resource().Attributes().EnsureCapacity(mb.resourceCapacity)
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName("otelcol/snowflakereceiver")
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricSnowflakeBillingCloudServiceSum.emit(ils.Metrics())
	mb.metricSnowflakeBillingTotalCreditSum.emit(ils.Metrics())
	mb.metricSnowflakeBillingVirtualWarehouseSum.emit(ils.Metrics())
	mb.metricSnowflakeBillingWarehouseCloudServiceSum.emit(ils.Metrics())
	mb.metricSnowflakeBillingWarehouseTotalCreditSum.emit(ils.Metrics())
	mb.metricSnowflakeBillingWarehouseVirtualWarehouseSum.emit(ils.Metrics())
	mb.metricSnowflakeDatabaseBytesScannedAvg.emit(ils.Metrics())
	mb.metricSnowflakeDatabaseQueryCount.emit(ils.Metrics())
	mb.metricSnowflakeLoginsTotal.emit(ils.Metrics())
	mb.metricSnowflakePipeCreditsUsedSum.emit(ils.Metrics())
	mb.metricSnowflakeQueryBlocked.emit(ils.Metrics())
	mb.metricSnowflakeQueryBytesDeletedSum.emit(ils.Metrics())
	mb.metricSnowflakeQueryBytesScannedSum.emit(ils.Metrics())
	mb.metricSnowflakeQueryBytesSpilledLocalSum.emit(ils.Metrics())
	mb.metricSnowflakeQueryBytesSpilledRemoteSum.emit(ils.Metrics())
	mb.metricSnowflakeQueryBytesWrittenSum.emit(ils.Metrics())
	mb.metricSnowflakeQueryCompilationTimeSum.emit(ils.Metrics())
	mb.metricSnowflakeQueryDataScannedCacheAvg.emit(ils.Metrics())
	mb.metricSnowflakeQueryExecuted.emit(ils.Metrics())
	mb.metricSnowflakeQueryExecutionTimeSum.emit(ils.Metrics())
	mb.metricSnowflakeQueryPartitionsScannedSum.emit(ils.Metrics())
	mb.metricSnowflakeQueryQueuedOverload.emit(ils.Metrics())
	mb.metricSnowflakeQueryQueuedProvision.emit(ils.Metrics())
	mb.metricSnowflakeQueuedOverloadTimeAvg.emit(ils.Metrics())
	mb.metricSnowflakeQueuedOverloadTimeSum.emit(ils.Metrics())
	mb.metricSnowflakeQueuedProvisioningTimeAvg.emit(ils.Metrics())
	mb.metricSnowflakeQueuedProvisioningTimeSum.emit(ils.Metrics())
	mb.metricSnowflakeQueuedRepairTimeAvg.emit(ils.Metrics())
	mb.metricSnowflakeQueuedRepairTimeSum.emit(ils.Metrics())
	mb.metricSnowflakeRowsDeletedSum.emit(ils.Metrics())
	mb.metricSnowflakeRowsInsertedSum.emit(ils.Metrics())
	mb.metricSnowflakeRowsProducedSum.emit(ils.Metrics())
	mb.metricSnowflakeRowsUnloadedSum.emit(ils.Metrics())
	mb.metricSnowflakeRowsUpdatedSum.emit(ils.Metrics())
	mb.metricSnowflakeSessionIDCount.emit(ils.Metrics())
	mb.metricSnowflakeStorageFailsafeBytesTotal.emit(ils.Metrics())
	mb.metricSnowflakeStorageStageBytesTotal.emit(ils.Metrics())
	mb.metricSnowflakeStorageStorageBytesTotal.emit(ils.Metrics())
	mb.metricSnowflakeTotalElapsedTimeAvg.emit(ils.Metrics())
	mb.metricSnowflakeTotalElapsedTimeSum.emit(ils.Metrics())
	for _, op := range rmo {
		op(rm)
	}
	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user settings, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(rmo ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(rmo...)
	metrics := pmetric.NewMetrics()
	mb.metricsBuffer.MoveTo(metrics)
	return metrics
}

// RecordSnowflakeBillingCloudServiceSumDataPoint adds a data point to snowflake.billing.cloud_service.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingCloudServiceSumDataPoint(ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	mb.metricSnowflakeBillingCloudServiceSum.recordDataPoint(mb.startTime, ts, val, serviceTypeAttributeValue)
}

// RecordSnowflakeBillingTotalCreditSumDataPoint adds a data point to snowflake.billing.total_credit.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingTotalCreditSumDataPoint(ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	mb.metricSnowflakeBillingTotalCreditSum.recordDataPoint(mb.startTime, ts, val, serviceTypeAttributeValue)
}

// RecordSnowflakeBillingVirtualWarehouseSumDataPoint adds a data point to snowflake.billing.virtual_warehouse.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingVirtualWarehouseSumDataPoint(ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	mb.metricSnowflakeBillingVirtualWarehouseSum.recordDataPoint(mb.startTime, ts, val, serviceTypeAttributeValue)
}

// RecordSnowflakeBillingWarehouseCloudServiceSumDataPoint adds a data point to snowflake.billing.warehouse.cloud_service.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingWarehouseCloudServiceSumDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeBillingWarehouseCloudServiceSum.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeBillingWarehouseTotalCreditSumDataPoint adds a data point to snowflake.billing.warehouse.total_credit.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingWarehouseTotalCreditSumDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeBillingWarehouseTotalCreditSum.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeBillingWarehouseVirtualWarehouseSumDataPoint adds a data point to snowflake.billing.warehouse.virtual_warehouse.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingWarehouseVirtualWarehouseSumDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeBillingWarehouseVirtualWarehouseSum.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeDatabaseBytesScannedAvgDataPoint adds a data point to snowflake.database.bytes_scanned.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeDatabaseBytesScannedAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeDatabaseBytesScannedAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeDatabaseQueryCountDataPoint adds a data point to snowflake.database.query.count metric.
func (mb *MetricsBuilder) RecordSnowflakeDatabaseQueryCountDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeDatabaseQueryCount.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeLoginsTotalDataPoint adds a data point to snowflake.logins.total metric.
func (mb *MetricsBuilder) RecordSnowflakeLoginsTotalDataPoint(ts pcommon.Timestamp, val int64, errorMessageAttributeValue string, reportedClientTypeAttributeValue string, isSuccessAttributeValue string) {
	mb.metricSnowflakeLoginsTotal.recordDataPoint(mb.startTime, ts, val, errorMessageAttributeValue, reportedClientTypeAttributeValue, isSuccessAttributeValue)
}

// RecordSnowflakePipeCreditsUsedSumDataPoint adds a data point to snowflake.pipe.credits_used.sum metric.
func (mb *MetricsBuilder) RecordSnowflakePipeCreditsUsedSumDataPoint(ts pcommon.Timestamp, val float64, pipeNameAttributeValue string) {
	mb.metricSnowflakePipeCreditsUsedSum.recordDataPoint(mb.startTime, ts, val, pipeNameAttributeValue)
}

// RecordSnowflakeQueryBlockedDataPoint adds a data point to snowflake.query.blocked metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBlockedDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeQueryBlocked.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueryBytesDeletedSumDataPoint adds a data point to snowflake.query.bytes_deleted.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBytesDeletedSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryBytesDeletedSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryBytesScannedSumDataPoint adds a data point to snowflake.query.bytes_scanned.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBytesScannedSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryBytesScannedSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryBytesSpilledLocalSumDataPoint adds a data point to snowflake.query.bytes_spilled.local.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBytesSpilledLocalSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryBytesSpilledLocalSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryBytesSpilledRemoteSumDataPoint adds a data point to snowflake.query.bytes_spilled.remote.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBytesSpilledRemoteSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryBytesSpilledRemoteSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryBytesWrittenSumDataPoint adds a data point to snowflake.query.bytes_written.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBytesWrittenSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryBytesWrittenSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryCompilationTimeSumDataPoint adds a data point to snowflake.query.compilation_time.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryCompilationTimeSumDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryCompilationTimeSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryDataScannedCacheAvgDataPoint adds a data point to snowflake.query.data_scanned_cache.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryDataScannedCacheAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryDataScannedCacheAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryExecutedDataPoint adds a data point to snowflake.query.executed metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryExecutedDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeQueryExecuted.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueryExecutionTimeSumDataPoint adds a data point to snowflake.query.execution_time.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryExecutionTimeSumDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryExecutionTimeSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryPartitionsScannedSumDataPoint adds a data point to snowflake.query.partitions_scanned.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryPartitionsScannedSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryPartitionsScannedSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryQueuedOverloadDataPoint adds a data point to snowflake.query.queued_overload metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryQueuedOverloadDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeQueryQueuedOverload.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueryQueuedProvisionDataPoint adds a data point to snowflake.query.queued_provision metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryQueuedProvisionDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeQueryQueuedProvision.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueuedOverloadTimeAvgDataPoint adds a data point to snowflake.queued_overload_time.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedOverloadTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedOverloadTimeAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedOverloadTimeSumDataPoint adds a data point to snowflake.queued_overload_time.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedOverloadTimeSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedOverloadTimeSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedProvisioningTimeAvgDataPoint adds a data point to snowflake.queued_provisioning_time.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedProvisioningTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedProvisioningTimeAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedProvisioningTimeSumDataPoint adds a data point to snowflake.queued_provisioning_time.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedProvisioningTimeSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedProvisioningTimeSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedRepairTimeAvgDataPoint adds a data point to snowflake.queued_repair_time.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedRepairTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedRepairTimeAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedRepairTimeSumDataPoint adds a data point to snowflake.queued_repair_time.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedRepairTimeSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedRepairTimeSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsDeletedSumDataPoint adds a data point to snowflake.rows_deleted.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeRowsDeletedSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeRowsDeletedSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsInsertedSumDataPoint adds a data point to snowflake.rows_inserted.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeRowsInsertedSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeRowsInsertedSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsProducedSumDataPoint adds a data point to snowflake.rows_produced.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeRowsProducedSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeRowsProducedSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsUnloadedSumDataPoint adds a data point to snowflake.rows_unloaded.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeRowsUnloadedSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeRowsUnloadedSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsUpdatedSumDataPoint adds a data point to snowflake.rows_updated.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeRowsUpdatedSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeRowsUpdatedSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeSessionIDCountDataPoint adds a data point to snowflake.session_id.count metric.
func (mb *MetricsBuilder) RecordSnowflakeSessionIDCountDataPoint(ts pcommon.Timestamp, val int64, userNameAttributeValue string) {
	mb.metricSnowflakeSessionIDCount.recordDataPoint(mb.startTime, ts, val, userNameAttributeValue)
}

// RecordSnowflakeStorageFailsafeBytesTotalDataPoint adds a data point to snowflake.storage.failsafe_bytes.total metric.
func (mb *MetricsBuilder) RecordSnowflakeStorageFailsafeBytesTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricSnowflakeStorageFailsafeBytesTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordSnowflakeStorageStageBytesTotalDataPoint adds a data point to snowflake.storage.stage_bytes.total metric.
func (mb *MetricsBuilder) RecordSnowflakeStorageStageBytesTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricSnowflakeStorageStageBytesTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordSnowflakeStorageStorageBytesTotalDataPoint adds a data point to snowflake.storage.storage_bytes.total metric.
func (mb *MetricsBuilder) RecordSnowflakeStorageStorageBytesTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricSnowflakeStorageStorageBytesTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordSnowflakeTotalElapsedTimeAvgDataPoint adds a data point to snowflake.total_elapsed_time.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeTotalElapsedTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeTotalElapsedTimeAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeTotalElapsedTimeSumDataPoint adds a data point to snowflake.total_elapsed_time.sum metric.
func (mb *MetricsBuilder) RecordSnowflakeTotalElapsedTimeSumDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeTotalElapsedTimeSum.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}
