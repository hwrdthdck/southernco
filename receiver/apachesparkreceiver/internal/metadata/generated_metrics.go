// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/confmap"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver"
)

// MetricSettings provides common settings for a particular metric.
type MetricSettings struct {
	Enabled bool `mapstructure:"enabled"`

	enabledSetByUser bool
}

func (ms *MetricSettings) Unmarshal(parser *confmap.Conf) error {
	if parser == nil {
		return nil
	}
	err := parser.Unmarshal(ms, confmap.WithErrorUnused())
	if err != nil {
		return err
	}
	ms.enabledSetByUser = parser.IsSet("enabled")
	return nil
}

// MetricsSettings provides settings for apachesparkreceiver metrics.
type MetricsSettings struct {
	SparkDriverBlockManagerDiskDiskSpaceUsed                MetricSettings `mapstructure:"spark.driver.block_manager.disk.diskSpaceUsed"`
	SparkDriverBlockManagerMemoryRemaining                  MetricSettings `mapstructure:"spark.driver.block_manager.memory.remaining"`
	SparkDriverBlockManagerMemoryUsed                       MetricSettings `mapstructure:"spark.driver.block_manager.memory.used"`
	SparkDriverCodeGeneratorCompilationAverageTime          MetricSettings `mapstructure:"spark.driver.code_generator.compilation.average_time"`
	SparkDriverCodeGeneratorCompilationCount                MetricSettings `mapstructure:"spark.driver.code_generator.compilation.count"`
	SparkDriverCodeGeneratorGeneratedClassAverageSize       MetricSettings `mapstructure:"spark.driver.code_generator.generated_class.average_size"`
	SparkDriverCodeGeneratorGeneratedClassCount             MetricSettings `mapstructure:"spark.driver.code_generator.generated_class.count"`
	SparkDriverCodeGeneratorGeneratedMethodAverageSize      MetricSettings `mapstructure:"spark.driver.code_generator.generated_method.average_size"`
	SparkDriverCodeGeneratorGeneratedMethodCount            MetricSettings `mapstructure:"spark.driver.code_generator.generated_method.count"`
	SparkDriverCodeGeneratorSourceCodeAverageSize           MetricSettings `mapstructure:"spark.driver.code_generator.source_code.average_size"`
	SparkDriverCodeGeneratorSourceCodeCount                 MetricSettings `mapstructure:"spark.driver.code_generator.source_code.count"`
	SparkDriverDagSchedulerJobActiveJobs                    MetricSettings `mapstructure:"spark.driver.dag_scheduler.job.active_jobs"`
	SparkDriverDagSchedulerJobAllJobs                       MetricSettings `mapstructure:"spark.driver.dag_scheduler.job.all_jobs"`
	SparkDriverDagSchedulerStageFailedStages                MetricSettings `mapstructure:"spark.driver.dag_scheduler.stage.failed_stages"`
	SparkDriverDagSchedulerStageRunningStages               MetricSettings `mapstructure:"spark.driver.dag_scheduler.stage.running_stages"`
	SparkDriverDagSchedulerStageWaitingStages               MetricSettings `mapstructure:"spark.driver.dag_scheduler.stage.waiting_stages"`
	SparkDriverExecutorMetricsExecutionMemory               MetricSettings `mapstructure:"spark.driver.executor_metrics.execution_memory"`
	SparkDriverExecutorMetricsGcCount                       MetricSettings `mapstructure:"spark.driver.executor_metrics.gc_count"`
	SparkDriverExecutorMetricsGcTime                        MetricSettings `mapstructure:"spark.driver.executor_metrics.gc_time"`
	SparkDriverExecutorMetricsJvmMemory                     MetricSettings `mapstructure:"spark.driver.executor_metrics.jvm_memory"`
	SparkDriverExecutorMetricsPoolMemory                    MetricSettings `mapstructure:"spark.driver.executor_metrics.pool_memory"`
	SparkDriverExecutorMetricsStorageMemory                 MetricSettings `mapstructure:"spark.driver.executor_metrics.storage_memory"`
	SparkDriverHiveExternalCatalogFileCacheHits             MetricSettings `mapstructure:"spark.driver.hive_external_catalog.file_cache_hits"`
	SparkDriverHiveExternalCatalogFilesDiscovered           MetricSettings `mapstructure:"spark.driver.hive_external_catalog.files_discovered"`
	SparkDriverHiveExternalCatalogHiveClientCalls           MetricSettings `mapstructure:"spark.driver.hive_external_catalog.hive_client_calls"`
	SparkDriverHiveExternalCatalogParallelListingJobs       MetricSettings `mapstructure:"spark.driver.hive_external_catalog.parallel_listing_jobs"`
	SparkDriverHiveExternalCatalogPartitionsFetched         MetricSettings `mapstructure:"spark.driver.hive_external_catalog.partitions_fetched"`
	SparkDriverJvmCPUTime                                   MetricSettings `mapstructure:"spark.driver.jvm_cpu_time"`
	SparkDriverLiveListenerBusEventsDropped                 MetricSettings `mapstructure:"spark.driver.live_listener_bus.events_dropped"`
	SparkDriverLiveListenerBusEventsPosted                  MetricSettings `mapstructure:"spark.driver.live_listener_bus.events_posted"`
	SparkDriverLiveListenerBusListenerProcessingTimeAverage MetricSettings `mapstructure:"spark.driver.live_listener_bus.listener_processing_time.average"`
	SparkDriverLiveListenerBusQueueSize                     MetricSettings `mapstructure:"spark.driver.live_listener_bus.queue_size"`
	SparkExecutorActiveTasks                                MetricSettings `mapstructure:"spark.executor.active_tasks"`
	SparkExecutorCompletedTasks                             MetricSettings `mapstructure:"spark.executor.completed_tasks"`
	SparkExecutorDiskUsed                                   MetricSettings `mapstructure:"spark.executor.disk_used"`
	SparkExecutorDuration                                   MetricSettings `mapstructure:"spark.executor.duration"`
	SparkExecutorFailedTasks                                MetricSettings `mapstructure:"spark.executor.failed_tasks"`
	SparkExecutorGcTime                                     MetricSettings `mapstructure:"spark.executor.gc_time"`
	SparkExecutorInputBytes                                 MetricSettings `mapstructure:"spark.executor.input_bytes"`
	SparkExecutorMaxTasks                                   MetricSettings `mapstructure:"spark.executor.max_tasks"`
	SparkExecutorMemoryUsed                                 MetricSettings `mapstructure:"spark.executor.memory_used"`
	SparkExecutorShuffleReadBytes                           MetricSettings `mapstructure:"spark.executor.shuffle_read_bytes"`
	SparkExecutorShuffleWriteBytes                          MetricSettings `mapstructure:"spark.executor.shuffle_write_bytes"`
	SparkExecutorTotalStorageMemory                         MetricSettings `mapstructure:"spark.executor.total_storage_memory"`
	SparkExecutorUsedStorageMemory                          MetricSettings `mapstructure:"spark.executor.used_storage_memory"`
	SparkJobActiveStages                                    MetricSettings `mapstructure:"spark.job.active_stages"`
	SparkJobActiveTasks                                     MetricSettings `mapstructure:"spark.job.active_tasks"`
	SparkJobCompletedStages                                 MetricSettings `mapstructure:"spark.job.completed_stages"`
	SparkJobCompletedTasks                                  MetricSettings `mapstructure:"spark.job.completed_tasks"`
	SparkJobFailedStages                                    MetricSettings `mapstructure:"spark.job.failed_stages"`
	SparkJobFailedTasks                                     MetricSettings `mapstructure:"spark.job.failed_tasks"`
	SparkJobSkippedStages                                   MetricSettings `mapstructure:"spark.job.skipped_stages"`
	SparkJobSkippedTasks                                    MetricSettings `mapstructure:"spark.job.skipped_tasks"`
	SparkStageActiveTasks                                   MetricSettings `mapstructure:"spark.stage.active_tasks"`
	SparkStageCompleteTasks                                 MetricSettings `mapstructure:"spark.stage.complete_tasks"`
	SparkStageDiskSpaceSpilled                              MetricSettings `mapstructure:"spark.stage.disk_space_spilled"`
	SparkStageExecutorCPUTime                               MetricSettings `mapstructure:"spark.stage.executor_cpu_time"`
	SparkStageExecutorRunTime                               MetricSettings `mapstructure:"spark.stage.executor_run_time"`
	SparkStageFailedTasks                                   MetricSettings `mapstructure:"spark.stage.failed_tasks"`
	SparkStageInputBytes                                    MetricSettings `mapstructure:"spark.stage.input_bytes"`
	SparkStageInputRecords                                  MetricSettings `mapstructure:"spark.stage.input_records"`
	SparkStageJvmGcTime                                     MetricSettings `mapstructure:"spark.stage.jvm_gc_time"`
	SparkStageKilledTasks                                   MetricSettings `mapstructure:"spark.stage.killed_tasks"`
	SparkStageMemorySpilled                                 MetricSettings `mapstructure:"spark.stage.memory_spilled"`
	SparkStageOutputBytes                                   MetricSettings `mapstructure:"spark.stage.output_bytes"`
	SparkStageOutputRecords                                 MetricSettings `mapstructure:"spark.stage.output_records"`
	SparkStagePeakExecutionMemory                           MetricSettings `mapstructure:"spark.stage.peak_execution_memory"`
	SparkStageResultSize                                    MetricSettings `mapstructure:"spark.stage.result_size"`
	SparkStageShuffleBlocksFetched                          MetricSettings `mapstructure:"spark.stage.shuffle.blocks_fetched"`
	SparkStageShuffleBytesRead                              MetricSettings `mapstructure:"spark.stage.shuffle.bytes_read"`
	SparkStageShuffleFetchWaitTime                          MetricSettings `mapstructure:"spark.stage.shuffle.fetch_wait_time"`
	SparkStageShuffleReadBytes                              MetricSettings `mapstructure:"spark.stage.shuffle.read_bytes"`
	SparkStageShuffleReadRecords                            MetricSettings `mapstructure:"spark.stage.shuffle.read_records"`
	SparkStageShuffleRemoteBytesReadToDisk                  MetricSettings `mapstructure:"spark.stage.shuffle.remote_bytes_read_to_disk"`
	SparkStageShuffleWriteBytes                             MetricSettings `mapstructure:"spark.stage.shuffle.write_bytes"`
	SparkStageShuffleWriteRecords                           MetricSettings `mapstructure:"spark.stage.shuffle.write_records"`
	SparkStageShuffleWriteTime                              MetricSettings `mapstructure:"spark.stage.shuffle.write_time"`
}

func DefaultMetricsSettings() MetricsSettings {
	return MetricsSettings{
		SparkDriverBlockManagerDiskDiskSpaceUsed: MetricSettings{
			Enabled: true,
		},
		SparkDriverBlockManagerMemoryRemaining: MetricSettings{
			Enabled: true,
		},
		SparkDriverBlockManagerMemoryUsed: MetricSettings{
			Enabled: true,
		},
		SparkDriverCodeGeneratorCompilationAverageTime: MetricSettings{
			Enabled: true,
		},
		SparkDriverCodeGeneratorCompilationCount: MetricSettings{
			Enabled: true,
		},
		SparkDriverCodeGeneratorGeneratedClassAverageSize: MetricSettings{
			Enabled: true,
		},
		SparkDriverCodeGeneratorGeneratedClassCount: MetricSettings{
			Enabled: true,
		},
		SparkDriverCodeGeneratorGeneratedMethodAverageSize: MetricSettings{
			Enabled: true,
		},
		SparkDriverCodeGeneratorGeneratedMethodCount: MetricSettings{
			Enabled: true,
		},
		SparkDriverCodeGeneratorSourceCodeAverageSize: MetricSettings{
			Enabled: true,
		},
		SparkDriverCodeGeneratorSourceCodeCount: MetricSettings{
			Enabled: true,
		},
		SparkDriverDagSchedulerJobActiveJobs: MetricSettings{
			Enabled: true,
		},
		SparkDriverDagSchedulerJobAllJobs: MetricSettings{
			Enabled: true,
		},
		SparkDriverDagSchedulerStageFailedStages: MetricSettings{
			Enabled: true,
		},
		SparkDriverDagSchedulerStageRunningStages: MetricSettings{
			Enabled: true,
		},
		SparkDriverDagSchedulerStageWaitingStages: MetricSettings{
			Enabled: true,
		},
		SparkDriverExecutorMetricsExecutionMemory: MetricSettings{
			Enabled: true,
		},
		SparkDriverExecutorMetricsGcCount: MetricSettings{
			Enabled: true,
		},
		SparkDriverExecutorMetricsGcTime: MetricSettings{
			Enabled: true,
		},
		SparkDriverExecutorMetricsJvmMemory: MetricSettings{
			Enabled: true,
		},
		SparkDriverExecutorMetricsPoolMemory: MetricSettings{
			Enabled: true,
		},
		SparkDriverExecutorMetricsStorageMemory: MetricSettings{
			Enabled: true,
		},
		SparkDriverHiveExternalCatalogFileCacheHits: MetricSettings{
			Enabled: true,
		},
		SparkDriverHiveExternalCatalogFilesDiscovered: MetricSettings{
			Enabled: true,
		},
		SparkDriverHiveExternalCatalogHiveClientCalls: MetricSettings{
			Enabled: true,
		},
		SparkDriverHiveExternalCatalogParallelListingJobs: MetricSettings{
			Enabled: true,
		},
		SparkDriverHiveExternalCatalogPartitionsFetched: MetricSettings{
			Enabled: true,
		},
		SparkDriverJvmCPUTime: MetricSettings{
			Enabled: true,
		},
		SparkDriverLiveListenerBusEventsDropped: MetricSettings{
			Enabled: true,
		},
		SparkDriverLiveListenerBusEventsPosted: MetricSettings{
			Enabled: true,
		},
		SparkDriverLiveListenerBusListenerProcessingTimeAverage: MetricSettings{
			Enabled: true,
		},
		SparkDriverLiveListenerBusQueueSize: MetricSettings{
			Enabled: true,
		},
		SparkExecutorActiveTasks: MetricSettings{
			Enabled: true,
		},
		SparkExecutorCompletedTasks: MetricSettings{
			Enabled: true,
		},
		SparkExecutorDiskUsed: MetricSettings{
			Enabled: true,
		},
		SparkExecutorDuration: MetricSettings{
			Enabled: true,
		},
		SparkExecutorFailedTasks: MetricSettings{
			Enabled: true,
		},
		SparkExecutorGcTime: MetricSettings{
			Enabled: true,
		},
		SparkExecutorInputBytes: MetricSettings{
			Enabled: true,
		},
		SparkExecutorMaxTasks: MetricSettings{
			Enabled: true,
		},
		SparkExecutorMemoryUsed: MetricSettings{
			Enabled: true,
		},
		SparkExecutorShuffleReadBytes: MetricSettings{
			Enabled: true,
		},
		SparkExecutorShuffleWriteBytes: MetricSettings{
			Enabled: true,
		},
		SparkExecutorTotalStorageMemory: MetricSettings{
			Enabled: true,
		},
		SparkExecutorUsedStorageMemory: MetricSettings{
			Enabled: true,
		},
		SparkJobActiveStages: MetricSettings{
			Enabled: true,
		},
		SparkJobActiveTasks: MetricSettings{
			Enabled: true,
		},
		SparkJobCompletedStages: MetricSettings{
			Enabled: true,
		},
		SparkJobCompletedTasks: MetricSettings{
			Enabled: true,
		},
		SparkJobFailedStages: MetricSettings{
			Enabled: true,
		},
		SparkJobFailedTasks: MetricSettings{
			Enabled: true,
		},
		SparkJobSkippedStages: MetricSettings{
			Enabled: true,
		},
		SparkJobSkippedTasks: MetricSettings{
			Enabled: true,
		},
		SparkStageActiveTasks: MetricSettings{
			Enabled: true,
		},
		SparkStageCompleteTasks: MetricSettings{
			Enabled: true,
		},
		SparkStageDiskSpaceSpilled: MetricSettings{
			Enabled: true,
		},
		SparkStageExecutorCPUTime: MetricSettings{
			Enabled: true,
		},
		SparkStageExecutorRunTime: MetricSettings{
			Enabled: true,
		},
		SparkStageFailedTasks: MetricSettings{
			Enabled: true,
		},
		SparkStageInputBytes: MetricSettings{
			Enabled: true,
		},
		SparkStageInputRecords: MetricSettings{
			Enabled: true,
		},
		SparkStageJvmGcTime: MetricSettings{
			Enabled: true,
		},
		SparkStageKilledTasks: MetricSettings{
			Enabled: true,
		},
		SparkStageMemorySpilled: MetricSettings{
			Enabled: true,
		},
		SparkStageOutputBytes: MetricSettings{
			Enabled: true,
		},
		SparkStageOutputRecords: MetricSettings{
			Enabled: true,
		},
		SparkStagePeakExecutionMemory: MetricSettings{
			Enabled: true,
		},
		SparkStageResultSize: MetricSettings{
			Enabled: true,
		},
		SparkStageShuffleBlocksFetched: MetricSettings{
			Enabled: true,
		},
		SparkStageShuffleBytesRead: MetricSettings{
			Enabled: true,
		},
		SparkStageShuffleFetchWaitTime: MetricSettings{
			Enabled: true,
		},
		SparkStageShuffleReadBytes: MetricSettings{
			Enabled: true,
		},
		SparkStageShuffleReadRecords: MetricSettings{
			Enabled: true,
		},
		SparkStageShuffleRemoteBytesReadToDisk: MetricSettings{
			Enabled: true,
		},
		SparkStageShuffleWriteBytes: MetricSettings{
			Enabled: true,
		},
		SparkStageShuffleWriteRecords: MetricSettings{
			Enabled: true,
		},
		SparkStageShuffleWriteTime: MetricSettings{
			Enabled: true,
		},
	}
}

// ResourceAttributeSettings provides common settings for a particular metric.
type ResourceAttributeSettings struct {
	Enabled bool `mapstructure:"enabled"`
}

// ResourceAttributesSettings provides settings for apachesparkreceiver metrics.
type ResourceAttributesSettings struct {
}

func DefaultResourceAttributesSettings() ResourceAttributesSettings {
	return ResourceAttributesSettings{}
}

// AttributeGcType specifies the a value gc_type attribute.
type AttributeGcType int

const (
	_ AttributeGcType = iota
	AttributeGcTypeMajor
	AttributeGcTypeMinor
)

// String returns the string representation of the AttributeGcType.
func (av AttributeGcType) String() string {
	switch av {
	case AttributeGcTypeMajor:
		return "major"
	case AttributeGcTypeMinor:
		return "minor"
	}
	return ""
}

// MapAttributeGcType is a helper map of string to AttributeGcType attribute value.
var MapAttributeGcType = map[string]AttributeGcType{
	"major": AttributeGcTypeMajor,
	"minor": AttributeGcTypeMinor,
}

// AttributeLocation specifies the a value location attribute.
type AttributeLocation int

const (
	_ AttributeLocation = iota
	AttributeLocationOnHeap
	AttributeLocationOffHeap
)

// String returns the string representation of the AttributeLocation.
func (av AttributeLocation) String() string {
	switch av {
	case AttributeLocationOnHeap:
		return "on_heap"
	case AttributeLocationOffHeap:
		return "off_heap"
	}
	return ""
}

// MapAttributeLocation is a helper map of string to AttributeLocation attribute value.
var MapAttributeLocation = map[string]AttributeLocation{
	"on_heap":  AttributeLocationOnHeap,
	"off_heap": AttributeLocationOffHeap,
}

// AttributePoolMemoryType specifies the a value pool_memory_type attribute.
type AttributePoolMemoryType int

const (
	_ AttributePoolMemoryType = iota
	AttributePoolMemoryTypeDirect
	AttributePoolMemoryTypeMapped
)

// String returns the string representation of the AttributePoolMemoryType.
func (av AttributePoolMemoryType) String() string {
	switch av {
	case AttributePoolMemoryTypeDirect:
		return "direct"
	case AttributePoolMemoryTypeMapped:
		return "mapped"
	}
	return ""
}

// MapAttributePoolMemoryType is a helper map of string to AttributePoolMemoryType attribute value.
var MapAttributePoolMemoryType = map[string]AttributePoolMemoryType{
	"direct": AttributePoolMemoryTypeDirect,
	"mapped": AttributePoolMemoryTypeMapped,
}

// AttributeSource specifies the a value source attribute.
type AttributeSource int

const (
	_ AttributeSource = iota
	AttributeSourceLocal
	AttributeSourceRemote
)

// String returns the string representation of the AttributeSource.
func (av AttributeSource) String() string {
	switch av {
	case AttributeSourceLocal:
		return "local"
	case AttributeSourceRemote:
		return "remote"
	}
	return ""
}

// MapAttributeSource is a helper map of string to AttributeSource attribute value.
var MapAttributeSource = map[string]AttributeSource{
	"local":  AttributeSourceLocal,
	"remote": AttributeSourceRemote,
}

// AttributeStageStatus specifies the a value stage_status attribute.
type AttributeStageStatus int

const (
	_ AttributeStageStatus = iota
	AttributeStageStatusACTIVE
	AttributeStageStatusCOMPLETE
	AttributeStageStatusPENDING
	AttributeStageStatusFAILED
)

// String returns the string representation of the AttributeStageStatus.
func (av AttributeStageStatus) String() string {
	switch av {
	case AttributeStageStatusACTIVE:
		return "ACTIVE"
	case AttributeStageStatusCOMPLETE:
		return "COMPLETE"
	case AttributeStageStatusPENDING:
		return "PENDING"
	case AttributeStageStatusFAILED:
		return "FAILED"
	}
	return ""
}

// MapAttributeStageStatus is a helper map of string to AttributeStageStatus attribute value.
var MapAttributeStageStatus = map[string]AttributeStageStatus{
	"ACTIVE":   AttributeStageStatusACTIVE,
	"COMPLETE": AttributeStageStatusCOMPLETE,
	"PENDING":  AttributeStageStatusPENDING,
	"FAILED":   AttributeStageStatusFAILED,
}

type metricSparkDriverBlockManagerDiskDiskSpaceUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.block_manager.disk.diskSpaceUsed metric with initial data.
func (m *metricSparkDriverBlockManagerDiskDiskSpaceUsed) init() {
	m.data.SetName("spark.driver.block_manager.disk.diskSpaceUsed")
	m.data.SetDescription("Disk space used by the BlockManager.")
	m.data.SetUnit("mb")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverBlockManagerDiskDiskSpaceUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverBlockManagerDiskDiskSpaceUsed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverBlockManagerDiskDiskSpaceUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverBlockManagerDiskDiskSpaceUsed(settings MetricSettings) metricSparkDriverBlockManagerDiskDiskSpaceUsed {
	m := metricSparkDriverBlockManagerDiskDiskSpaceUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverBlockManagerMemoryRemaining struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.block_manager.memory.remaining metric with initial data.
func (m *metricSparkDriverBlockManagerMemoryRemaining) init() {
	m.data.SetName("spark.driver.block_manager.memory.remaining")
	m.data.SetDescription("Memory remaining for the BlockManager.")
	m.data.SetUnit("mb")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverBlockManagerMemoryRemaining) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, locationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("location", locationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverBlockManagerMemoryRemaining) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverBlockManagerMemoryRemaining) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverBlockManagerMemoryRemaining(settings MetricSettings) metricSparkDriverBlockManagerMemoryRemaining {
	m := metricSparkDriverBlockManagerMemoryRemaining{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverBlockManagerMemoryUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.block_manager.memory.used metric with initial data.
func (m *metricSparkDriverBlockManagerMemoryUsed) init() {
	m.data.SetName("spark.driver.block_manager.memory.used")
	m.data.SetDescription("Memory used by the BlockManager.")
	m.data.SetUnit("mb")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverBlockManagerMemoryUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, locationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("location", locationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverBlockManagerMemoryUsed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverBlockManagerMemoryUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverBlockManagerMemoryUsed(settings MetricSettings) metricSparkDriverBlockManagerMemoryUsed {
	m := metricSparkDriverBlockManagerMemoryUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverCodeGeneratorCompilationAverageTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.code_generator.compilation.average_time metric with initial data.
func (m *metricSparkDriverCodeGeneratorCompilationAverageTime) init() {
	m.data.SetName("spark.driver.code_generator.compilation.average_time")
	m.data.SetDescription("Average time spent during CodeGenerator source code compilation operations.")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverCodeGeneratorCompilationAverageTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverCodeGeneratorCompilationAverageTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverCodeGeneratorCompilationAverageTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverCodeGeneratorCompilationAverageTime(settings MetricSettings) metricSparkDriverCodeGeneratorCompilationAverageTime {
	m := metricSparkDriverCodeGeneratorCompilationAverageTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverCodeGeneratorCompilationCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.code_generator.compilation.count metric with initial data.
func (m *metricSparkDriverCodeGeneratorCompilationCount) init() {
	m.data.SetName("spark.driver.code_generator.compilation.count")
	m.data.SetDescription("Number of source code compilation operations performed by the CodeGenerator.")
	m.data.SetUnit("{ compilations }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverCodeGeneratorCompilationCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverCodeGeneratorCompilationCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverCodeGeneratorCompilationCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverCodeGeneratorCompilationCount(settings MetricSettings) metricSparkDriverCodeGeneratorCompilationCount {
	m := metricSparkDriverCodeGeneratorCompilationCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverCodeGeneratorGeneratedClassAverageSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.code_generator.generated_class.average_size metric with initial data.
func (m *metricSparkDriverCodeGeneratorGeneratedClassAverageSize) init() {
	m.data.SetName("spark.driver.code_generator.generated_class.average_size")
	m.data.SetDescription("Average class size of the classes generated by the CodeGenerator.")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverCodeGeneratorGeneratedClassAverageSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverCodeGeneratorGeneratedClassAverageSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverCodeGeneratorGeneratedClassAverageSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverCodeGeneratorGeneratedClassAverageSize(settings MetricSettings) metricSparkDriverCodeGeneratorGeneratedClassAverageSize {
	m := metricSparkDriverCodeGeneratorGeneratedClassAverageSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverCodeGeneratorGeneratedClassCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.code_generator.generated_class.count metric with initial data.
func (m *metricSparkDriverCodeGeneratorGeneratedClassCount) init() {
	m.data.SetName("spark.driver.code_generator.generated_class.count")
	m.data.SetDescription("Number of classes generated by the CodeGenerator.")
	m.data.SetUnit("{ classes }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverCodeGeneratorGeneratedClassCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverCodeGeneratorGeneratedClassCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverCodeGeneratorGeneratedClassCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverCodeGeneratorGeneratedClassCount(settings MetricSettings) metricSparkDriverCodeGeneratorGeneratedClassCount {
	m := metricSparkDriverCodeGeneratorGeneratedClassCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverCodeGeneratorGeneratedMethodAverageSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.code_generator.generated_method.average_size metric with initial data.
func (m *metricSparkDriverCodeGeneratorGeneratedMethodAverageSize) init() {
	m.data.SetName("spark.driver.code_generator.generated_method.average_size")
	m.data.SetDescription("Average method size of the classes generated by the CodeGenerator.")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverCodeGeneratorGeneratedMethodAverageSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverCodeGeneratorGeneratedMethodAverageSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverCodeGeneratorGeneratedMethodAverageSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverCodeGeneratorGeneratedMethodAverageSize(settings MetricSettings) metricSparkDriverCodeGeneratorGeneratedMethodAverageSize {
	m := metricSparkDriverCodeGeneratorGeneratedMethodAverageSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverCodeGeneratorGeneratedMethodCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.code_generator.generated_method.count metric with initial data.
func (m *metricSparkDriverCodeGeneratorGeneratedMethodCount) init() {
	m.data.SetName("spark.driver.code_generator.generated_method.count")
	m.data.SetDescription("Number of methods generated by the CodeGenerator.")
	m.data.SetUnit("{ methods }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverCodeGeneratorGeneratedMethodCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverCodeGeneratorGeneratedMethodCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverCodeGeneratorGeneratedMethodCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverCodeGeneratorGeneratedMethodCount(settings MetricSettings) metricSparkDriverCodeGeneratorGeneratedMethodCount {
	m := metricSparkDriverCodeGeneratorGeneratedMethodCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverCodeGeneratorSourceCodeAverageSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.code_generator.source_code.average_size metric with initial data.
func (m *metricSparkDriverCodeGeneratorSourceCodeAverageSize) init() {
	m.data.SetName("spark.driver.code_generator.source_code.average_size")
	m.data.SetDescription("Average size of the source code generated by a CodeGenerator code generation operation.")
	m.data.SetUnit("bytes")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverCodeGeneratorSourceCodeAverageSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverCodeGeneratorSourceCodeAverageSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverCodeGeneratorSourceCodeAverageSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverCodeGeneratorSourceCodeAverageSize(settings MetricSettings) metricSparkDriverCodeGeneratorSourceCodeAverageSize {
	m := metricSparkDriverCodeGeneratorSourceCodeAverageSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverCodeGeneratorSourceCodeCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.code_generator.source_code.count metric with initial data.
func (m *metricSparkDriverCodeGeneratorSourceCodeCount) init() {
	m.data.SetName("spark.driver.code_generator.source_code.count")
	m.data.SetDescription("Number of source code generation operations performed by the CodeGenerator.")
	m.data.SetUnit("{ operations }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverCodeGeneratorSourceCodeCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverCodeGeneratorSourceCodeCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverCodeGeneratorSourceCodeCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverCodeGeneratorSourceCodeCount(settings MetricSettings) metricSparkDriverCodeGeneratorSourceCodeCount {
	m := metricSparkDriverCodeGeneratorSourceCodeCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverDagSchedulerJobActiveJobs struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.dag_scheduler.job.active_jobs metric with initial data.
func (m *metricSparkDriverDagSchedulerJobActiveJobs) init() {
	m.data.SetName("spark.driver.dag_scheduler.job.active_jobs")
	m.data.SetDescription("Number of active jobs currently being processed by the DAGScheduler.")
	m.data.SetUnit("{ jobs }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverDagSchedulerJobActiveJobs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverDagSchedulerJobActiveJobs) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverDagSchedulerJobActiveJobs) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverDagSchedulerJobActiveJobs(settings MetricSettings) metricSparkDriverDagSchedulerJobActiveJobs {
	m := metricSparkDriverDagSchedulerJobActiveJobs{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverDagSchedulerJobAllJobs struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.dag_scheduler.job.all_jobs metric with initial data.
func (m *metricSparkDriverDagSchedulerJobAllJobs) init() {
	m.data.SetName("spark.driver.dag_scheduler.job.all_jobs")
	m.data.SetDescription("Number of jobs that have been submitted to the DAGScheduler.")
	m.data.SetUnit("{ jobs }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverDagSchedulerJobAllJobs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverDagSchedulerJobAllJobs) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverDagSchedulerJobAllJobs) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverDagSchedulerJobAllJobs(settings MetricSettings) metricSparkDriverDagSchedulerJobAllJobs {
	m := metricSparkDriverDagSchedulerJobAllJobs{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverDagSchedulerStageFailedStages struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.dag_scheduler.stage.failed_stages metric with initial data.
func (m *metricSparkDriverDagSchedulerStageFailedStages) init() {
	m.data.SetName("spark.driver.dag_scheduler.stage.failed_stages")
	m.data.SetDescription("Number of failed stages run by the DAGScheduler.")
	m.data.SetUnit("{ stages }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverDagSchedulerStageFailedStages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverDagSchedulerStageFailedStages) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverDagSchedulerStageFailedStages) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverDagSchedulerStageFailedStages(settings MetricSettings) metricSparkDriverDagSchedulerStageFailedStages {
	m := metricSparkDriverDagSchedulerStageFailedStages{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverDagSchedulerStageRunningStages struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.dag_scheduler.stage.running_stages metric with initial data.
func (m *metricSparkDriverDagSchedulerStageRunningStages) init() {
	m.data.SetName("spark.driver.dag_scheduler.stage.running_stages")
	m.data.SetDescription("Number of stages the DAGScheduler is currently running.")
	m.data.SetUnit("{ stages }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverDagSchedulerStageRunningStages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverDagSchedulerStageRunningStages) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverDagSchedulerStageRunningStages) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverDagSchedulerStageRunningStages(settings MetricSettings) metricSparkDriverDagSchedulerStageRunningStages {
	m := metricSparkDriverDagSchedulerStageRunningStages{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverDagSchedulerStageWaitingStages struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.dag_scheduler.stage.waiting_stages metric with initial data.
func (m *metricSparkDriverDagSchedulerStageWaitingStages) init() {
	m.data.SetName("spark.driver.dag_scheduler.stage.waiting_stages")
	m.data.SetDescription("Number of stages waiting to be run by the DAGScheduler.")
	m.data.SetUnit("{ stages }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverDagSchedulerStageWaitingStages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverDagSchedulerStageWaitingStages) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverDagSchedulerStageWaitingStages) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverDagSchedulerStageWaitingStages(settings MetricSettings) metricSparkDriverDagSchedulerStageWaitingStages {
	m := metricSparkDriverDagSchedulerStageWaitingStages{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverExecutorMetricsExecutionMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.executor_metrics.execution_memory metric with initial data.
func (m *metricSparkDriverExecutorMetricsExecutionMemory) init() {
	m.data.SetName("spark.driver.executor_metrics.execution_memory")
	m.data.SetDescription("Amount of execution memory currently used by the driver.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverExecutorMetricsExecutionMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, locationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("location", locationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverExecutorMetricsExecutionMemory) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverExecutorMetricsExecutionMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverExecutorMetricsExecutionMemory(settings MetricSettings) metricSparkDriverExecutorMetricsExecutionMemory {
	m := metricSparkDriverExecutorMetricsExecutionMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverExecutorMetricsGcCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.executor_metrics.gc_count metric with initial data.
func (m *metricSparkDriverExecutorMetricsGcCount) init() {
	m.data.SetName("spark.driver.executor_metrics.gc_count")
	m.data.SetDescription("Number of garbage collection operations performed.")
	m.data.SetUnit("{ gc_operations }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverExecutorMetricsGcCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, gcTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("gc_type", gcTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverExecutorMetricsGcCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverExecutorMetricsGcCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverExecutorMetricsGcCount(settings MetricSettings) metricSparkDriverExecutorMetricsGcCount {
	m := metricSparkDriverExecutorMetricsGcCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverExecutorMetricsGcTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.executor_metrics.gc_time metric with initial data.
func (m *metricSparkDriverExecutorMetricsGcTime) init() {
	m.data.SetName("spark.driver.executor_metrics.gc_time")
	m.data.SetDescription("Total elapsed time during garbage collection operations.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverExecutorMetricsGcTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, gcTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("gc_type", gcTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverExecutorMetricsGcTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverExecutorMetricsGcTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverExecutorMetricsGcTime(settings MetricSettings) metricSparkDriverExecutorMetricsGcTime {
	m := metricSparkDriverExecutorMetricsGcTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverExecutorMetricsJvmMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.executor_metrics.jvm_memory metric with initial data.
func (m *metricSparkDriverExecutorMetricsJvmMemory) init() {
	m.data.SetName("spark.driver.executor_metrics.jvm_memory")
	m.data.SetDescription("Amount of memory used by the driver JVM.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverExecutorMetricsJvmMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, locationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("location", locationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverExecutorMetricsJvmMemory) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverExecutorMetricsJvmMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverExecutorMetricsJvmMemory(settings MetricSettings) metricSparkDriverExecutorMetricsJvmMemory {
	m := metricSparkDriverExecutorMetricsJvmMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverExecutorMetricsPoolMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.executor_metrics.pool_memory metric with initial data.
func (m *metricSparkDriverExecutorMetricsPoolMemory) init() {
	m.data.SetName("spark.driver.executor_metrics.pool_memory")
	m.data.SetDescription("Amount of pool memory currently used by the driver.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverExecutorMetricsPoolMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, poolMemoryTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("pool_memory_type", poolMemoryTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverExecutorMetricsPoolMemory) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverExecutorMetricsPoolMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverExecutorMetricsPoolMemory(settings MetricSettings) metricSparkDriverExecutorMetricsPoolMemory {
	m := metricSparkDriverExecutorMetricsPoolMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverExecutorMetricsStorageMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.executor_metrics.storage_memory metric with initial data.
func (m *metricSparkDriverExecutorMetricsStorageMemory) init() {
	m.data.SetName("spark.driver.executor_metrics.storage_memory")
	m.data.SetDescription("Amount of storage memory currently used by the driver.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverExecutorMetricsStorageMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, locationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("location", locationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverExecutorMetricsStorageMemory) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverExecutorMetricsStorageMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverExecutorMetricsStorageMemory(settings MetricSettings) metricSparkDriverExecutorMetricsStorageMemory {
	m := metricSparkDriverExecutorMetricsStorageMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverHiveExternalCatalogFileCacheHits struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.hive_external_catalog.file_cache_hits metric with initial data.
func (m *metricSparkDriverHiveExternalCatalogFileCacheHits) init() {
	m.data.SetName("spark.driver.hive_external_catalog.file_cache_hits")
	m.data.SetDescription("Number of file cache hits on the HiveExternalCatalog.")
	m.data.SetUnit("{ hits }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverHiveExternalCatalogFileCacheHits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverHiveExternalCatalogFileCacheHits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverHiveExternalCatalogFileCacheHits) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverHiveExternalCatalogFileCacheHits(settings MetricSettings) metricSparkDriverHiveExternalCatalogFileCacheHits {
	m := metricSparkDriverHiveExternalCatalogFileCacheHits{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverHiveExternalCatalogFilesDiscovered struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.hive_external_catalog.files_discovered metric with initial data.
func (m *metricSparkDriverHiveExternalCatalogFilesDiscovered) init() {
	m.data.SetName("spark.driver.hive_external_catalog.files_discovered")
	m.data.SetDescription("Number of files discovered while listing the partitions of a table in the Hive metastore")
	m.data.SetUnit("{ files }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverHiveExternalCatalogFilesDiscovered) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverHiveExternalCatalogFilesDiscovered) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverHiveExternalCatalogFilesDiscovered) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverHiveExternalCatalogFilesDiscovered(settings MetricSettings) metricSparkDriverHiveExternalCatalogFilesDiscovered {
	m := metricSparkDriverHiveExternalCatalogFilesDiscovered{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverHiveExternalCatalogHiveClientCalls struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.hive_external_catalog.hive_client_calls metric with initial data.
func (m *metricSparkDriverHiveExternalCatalogHiveClientCalls) init() {
	m.data.SetName("spark.driver.hive_external_catalog.hive_client_calls")
	m.data.SetDescription("Number of calls to the underlying Hive Metastore client made by the Spark application.")
	m.data.SetUnit("{ calls }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverHiveExternalCatalogHiveClientCalls) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverHiveExternalCatalogHiveClientCalls) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverHiveExternalCatalogHiveClientCalls) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverHiveExternalCatalogHiveClientCalls(settings MetricSettings) metricSparkDriverHiveExternalCatalogHiveClientCalls {
	m := metricSparkDriverHiveExternalCatalogHiveClientCalls{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverHiveExternalCatalogParallelListingJobs struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.hive_external_catalog.parallel_listing_jobs metric with initial data.
func (m *metricSparkDriverHiveExternalCatalogParallelListingJobs) init() {
	m.data.SetName("spark.driver.hive_external_catalog.parallel_listing_jobs")
	m.data.SetDescription("Number of parallel listing jobs initiated by the HiveExternalCatalog when listing partitions of a table.")
	m.data.SetUnit("{ listing_jobs }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverHiveExternalCatalogParallelListingJobs) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverHiveExternalCatalogParallelListingJobs) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverHiveExternalCatalogParallelListingJobs) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverHiveExternalCatalogParallelListingJobs(settings MetricSettings) metricSparkDriverHiveExternalCatalogParallelListingJobs {
	m := metricSparkDriverHiveExternalCatalogParallelListingJobs{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverHiveExternalCatalogPartitionsFetched struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.hive_external_catalog.partitions_fetched metric with initial data.
func (m *metricSparkDriverHiveExternalCatalogPartitionsFetched) init() {
	m.data.SetName("spark.driver.hive_external_catalog.partitions_fetched")
	m.data.SetDescription("Table partitions fetched by the HiveExternalCatalog.")
	m.data.SetUnit("{ partitions }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverHiveExternalCatalogPartitionsFetched) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverHiveExternalCatalogPartitionsFetched) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverHiveExternalCatalogPartitionsFetched) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverHiveExternalCatalogPartitionsFetched(settings MetricSettings) metricSparkDriverHiveExternalCatalogPartitionsFetched {
	m := metricSparkDriverHiveExternalCatalogPartitionsFetched{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverJvmCPUTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.jvm_cpu_time metric with initial data.
func (m *metricSparkDriverJvmCPUTime) init() {
	m.data.SetName("spark.driver.jvm_cpu_time")
	m.data.SetDescription("Current CPU time taken by the Spark driver.")
	m.data.SetUnit("ns")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverJvmCPUTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverJvmCPUTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverJvmCPUTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverJvmCPUTime(settings MetricSettings) metricSparkDriverJvmCPUTime {
	m := metricSparkDriverJvmCPUTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverLiveListenerBusEventsDropped struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.live_listener_bus.events_dropped metric with initial data.
func (m *metricSparkDriverLiveListenerBusEventsDropped) init() {
	m.data.SetName("spark.driver.live_listener_bus.events_dropped")
	m.data.SetDescription("Number of events that have been dropped by the LiveListenerBus.")
	m.data.SetUnit("{ events }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverLiveListenerBusEventsDropped) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverLiveListenerBusEventsDropped) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverLiveListenerBusEventsDropped) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverLiveListenerBusEventsDropped(settings MetricSettings) metricSparkDriverLiveListenerBusEventsDropped {
	m := metricSparkDriverLiveListenerBusEventsDropped{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverLiveListenerBusEventsPosted struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.live_listener_bus.events_posted metric with initial data.
func (m *metricSparkDriverLiveListenerBusEventsPosted) init() {
	m.data.SetName("spark.driver.live_listener_bus.events_posted")
	m.data.SetDescription("Number of events that have been posted on the LiveListenerBus.")
	m.data.SetUnit("{ events }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverLiveListenerBusEventsPosted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverLiveListenerBusEventsPosted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverLiveListenerBusEventsPosted) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverLiveListenerBusEventsPosted(settings MetricSettings) metricSparkDriverLiveListenerBusEventsPosted {
	m := metricSparkDriverLiveListenerBusEventsPosted{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverLiveListenerBusListenerProcessingTimeAverage struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.live_listener_bus.listener_processing_time.average metric with initial data.
func (m *metricSparkDriverLiveListenerBusListenerProcessingTimeAverage) init() {
	m.data.SetName("spark.driver.live_listener_bus.listener_processing_time.average")
	m.data.SetDescription("Average time taken for the LiveListenerBus to process an event posted to it.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverLiveListenerBusListenerProcessingTimeAverage) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverLiveListenerBusListenerProcessingTimeAverage) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverLiveListenerBusListenerProcessingTimeAverage) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverLiveListenerBusListenerProcessingTimeAverage(settings MetricSettings) metricSparkDriverLiveListenerBusListenerProcessingTimeAverage {
	m := metricSparkDriverLiveListenerBusListenerProcessingTimeAverage{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkDriverLiveListenerBusQueueSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.driver.live_listener_bus.queue_size metric with initial data.
func (m *metricSparkDriverLiveListenerBusQueueSize) init() {
	m.data.SetName("spark.driver.live_listener_bus.queue_size")
	m.data.SetDescription("Number of events currently waiting to be processed by the LiveListenerBus.")
	m.data.SetUnit("{ events }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkDriverLiveListenerBusQueueSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkDriverLiveListenerBusQueueSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkDriverLiveListenerBusQueueSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkDriverLiveListenerBusQueueSize(settings MetricSettings) metricSparkDriverLiveListenerBusQueueSize {
	m := metricSparkDriverLiveListenerBusQueueSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorActiveTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.active_tasks metric with initial data.
func (m *metricSparkExecutorActiveTasks) init() {
	m.data.SetName("spark.executor.active_tasks")
	m.data.SetDescription("Number of tasks currently running in this executor.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorActiveTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorActiveTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorActiveTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorActiveTasks(settings MetricSettings) metricSparkExecutorActiveTasks {
	m := metricSparkExecutorActiveTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorCompletedTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.completed_tasks metric with initial data.
func (m *metricSparkExecutorCompletedTasks) init() {
	m.data.SetName("spark.executor.completed_tasks")
	m.data.SetDescription("Number of tasks that have been completed by this executor.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorCompletedTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorCompletedTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorCompletedTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorCompletedTasks(settings MetricSettings) metricSparkExecutorCompletedTasks {
	m := metricSparkExecutorCompletedTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorDiskUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.disk_used metric with initial data.
func (m *metricSparkExecutorDiskUsed) init() {
	m.data.SetName("spark.executor.disk_used")
	m.data.SetDescription("Disk space used by this executor for RDD storage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorDiskUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorDiskUsed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorDiskUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorDiskUsed(settings MetricSettings) metricSparkExecutorDiskUsed {
	m := metricSparkExecutorDiskUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorDuration struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.duration metric with initial data.
func (m *metricSparkExecutorDuration) init() {
	m.data.SetName("spark.executor.duration")
	m.data.SetDescription("Elapsed time the JVM spent executing tasks in this executor.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorDuration) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorDuration) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorDuration) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorDuration(settings MetricSettings) metricSparkExecutorDuration {
	m := metricSparkExecutorDuration{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorFailedTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.failed_tasks metric with initial data.
func (m *metricSparkExecutorFailedTasks) init() {
	m.data.SetName("spark.executor.failed_tasks")
	m.data.SetDescription("Number of tasks that have failed in this executor.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorFailedTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorFailedTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorFailedTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorFailedTasks(settings MetricSettings) metricSparkExecutorFailedTasks {
	m := metricSparkExecutorFailedTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorGcTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.gc_time metric with initial data.
func (m *metricSparkExecutorGcTime) init() {
	m.data.SetName("spark.executor.gc_time")
	m.data.SetDescription("Elapsed time the JVM spent in garbage collection in this executor.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorGcTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorGcTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorGcTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorGcTime(settings MetricSettings) metricSparkExecutorGcTime {
	m := metricSparkExecutorGcTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorInputBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.input_bytes metric with initial data.
func (m *metricSparkExecutorInputBytes) init() {
	m.data.SetName("spark.executor.input_bytes")
	m.data.SetDescription("Input bytes for this executor.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorInputBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorInputBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorInputBytes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorInputBytes(settings MetricSettings) metricSparkExecutorInputBytes {
	m := metricSparkExecutorInputBytes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorMaxTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.max_tasks metric with initial data.
func (m *metricSparkExecutorMaxTasks) init() {
	m.data.SetName("spark.executor.max_tasks")
	m.data.SetDescription("Maximum number of tasks that can run concurrently in this executor.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorMaxTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorMaxTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorMaxTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorMaxTasks(settings MetricSettings) metricSparkExecutorMaxTasks {
	m := metricSparkExecutorMaxTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorMemoryUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.memory_used metric with initial data.
func (m *metricSparkExecutorMemoryUsed) init() {
	m.data.SetName("spark.executor.memory_used")
	m.data.SetDescription("Storage memory used by this executor.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorMemoryUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorMemoryUsed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorMemoryUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorMemoryUsed(settings MetricSettings) metricSparkExecutorMemoryUsed {
	m := metricSparkExecutorMemoryUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorShuffleReadBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.shuffle_read_bytes metric with initial data.
func (m *metricSparkExecutorShuffleReadBytes) init() {
	m.data.SetName("spark.executor.shuffle_read_bytes")
	m.data.SetDescription("Number of bytes read during shuffle operations for this executor.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorShuffleReadBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorShuffleReadBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorShuffleReadBytes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorShuffleReadBytes(settings MetricSettings) metricSparkExecutorShuffleReadBytes {
	m := metricSparkExecutorShuffleReadBytes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorShuffleWriteBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.shuffle_write_bytes metric with initial data.
func (m *metricSparkExecutorShuffleWriteBytes) init() {
	m.data.SetName("spark.executor.shuffle_write_bytes")
	m.data.SetDescription("Number of bytes written during shuffle operations for this executor.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorShuffleWriteBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorShuffleWriteBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorShuffleWriteBytes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorShuffleWriteBytes(settings MetricSettings) metricSparkExecutorShuffleWriteBytes {
	m := metricSparkExecutorShuffleWriteBytes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorTotalStorageMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.total_storage_memory metric with initial data.
func (m *metricSparkExecutorTotalStorageMemory) init() {
	m.data.SetName("spark.executor.total_storage_memory")
	m.data.SetDescription("Total memory that can be used for storage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorTotalStorageMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string, locationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
	dp.Attributes().PutStr("location", locationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorTotalStorageMemory) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorTotalStorageMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorTotalStorageMemory(settings MetricSettings) metricSparkExecutorTotalStorageMemory {
	m := metricSparkExecutorTotalStorageMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkExecutorUsedStorageMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.executor.used_storage_memory metric with initial data.
func (m *metricSparkExecutorUsedStorageMemory) init() {
	m.data.SetName("spark.executor.used_storage_memory")
	m.data.SetDescription("Amount of memory currently used for storage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkExecutorUsedStorageMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string, locationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutStr("executor_id", executorIDAttributeValue)
	dp.Attributes().PutStr("location", locationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkExecutorUsedStorageMemory) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkExecutorUsedStorageMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkExecutorUsedStorageMemory(settings MetricSettings) metricSparkExecutorUsedStorageMemory {
	m := metricSparkExecutorUsedStorageMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkJobActiveStages struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.job.active_stages metric with initial data.
func (m *metricSparkJobActiveStages) init() {
	m.data.SetName("spark.job.active_stages")
	m.data.SetDescription("Number of active stages in this job.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkJobActiveStages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("job_id", jobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkJobActiveStages) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkJobActiveStages) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkJobActiveStages(settings MetricSettings) metricSparkJobActiveStages {
	m := metricSparkJobActiveStages{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkJobActiveTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.job.active_tasks metric with initial data.
func (m *metricSparkJobActiveTasks) init() {
	m.data.SetName("spark.job.active_tasks")
	m.data.SetDescription("Number of active tasks in this job.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkJobActiveTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("job_id", jobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkJobActiveTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkJobActiveTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkJobActiveTasks(settings MetricSettings) metricSparkJobActiveTasks {
	m := metricSparkJobActiveTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkJobCompletedStages struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.job.completed_stages metric with initial data.
func (m *metricSparkJobCompletedStages) init() {
	m.data.SetName("spark.job.completed_stages")
	m.data.SetDescription("Number of completed stages in this job.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkJobCompletedStages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("job_id", jobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkJobCompletedStages) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkJobCompletedStages) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkJobCompletedStages(settings MetricSettings) metricSparkJobCompletedStages {
	m := metricSparkJobCompletedStages{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkJobCompletedTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.job.completed_tasks metric with initial data.
func (m *metricSparkJobCompletedTasks) init() {
	m.data.SetName("spark.job.completed_tasks")
	m.data.SetDescription("Number of completed tasks in this job.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkJobCompletedTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("job_id", jobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkJobCompletedTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkJobCompletedTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkJobCompletedTasks(settings MetricSettings) metricSparkJobCompletedTasks {
	m := metricSparkJobCompletedTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkJobFailedStages struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.job.failed_stages metric with initial data.
func (m *metricSparkJobFailedStages) init() {
	m.data.SetName("spark.job.failed_stages")
	m.data.SetDescription("Number of failed stages in this job.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkJobFailedStages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("job_id", jobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkJobFailedStages) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkJobFailedStages) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkJobFailedStages(settings MetricSettings) metricSparkJobFailedStages {
	m := metricSparkJobFailedStages{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkJobFailedTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.job.failed_tasks metric with initial data.
func (m *metricSparkJobFailedTasks) init() {
	m.data.SetName("spark.job.failed_tasks")
	m.data.SetDescription("Number of failed tasks in this job.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkJobFailedTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("job_id", jobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkJobFailedTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkJobFailedTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkJobFailedTasks(settings MetricSettings) metricSparkJobFailedTasks {
	m := metricSparkJobFailedTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkJobSkippedStages struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.job.skipped_stages metric with initial data.
func (m *metricSparkJobSkippedStages) init() {
	m.data.SetName("spark.job.skipped_stages")
	m.data.SetDescription("Number of skipped stages in this job.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkJobSkippedStages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("job_id", jobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkJobSkippedStages) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkJobSkippedStages) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkJobSkippedStages(settings MetricSettings) metricSparkJobSkippedStages {
	m := metricSparkJobSkippedStages{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkJobSkippedTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.job.skipped_tasks metric with initial data.
func (m *metricSparkJobSkippedTasks) init() {
	m.data.SetName("spark.job.skipped_tasks")
	m.data.SetDescription("Number of skipped tasks in this job.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkJobSkippedTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("job_id", jobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkJobSkippedTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkJobSkippedTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkJobSkippedTasks(settings MetricSettings) metricSparkJobSkippedTasks {
	m := metricSparkJobSkippedTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageActiveTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.active_tasks metric with initial data.
func (m *metricSparkStageActiveTasks) init() {
	m.data.SetName("spark.stage.active_tasks")
	m.data.SetDescription("Number of active tasks in this stage.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageActiveTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageActiveTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageActiveTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageActiveTasks(settings MetricSettings) metricSparkStageActiveTasks {
	m := metricSparkStageActiveTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageCompleteTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.complete_tasks metric with initial data.
func (m *metricSparkStageCompleteTasks) init() {
	m.data.SetName("spark.stage.complete_tasks")
	m.data.SetDescription("Number of complete tasks in this stage.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageCompleteTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageCompleteTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageCompleteTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageCompleteTasks(settings MetricSettings) metricSparkStageCompleteTasks {
	m := metricSparkStageCompleteTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageDiskSpaceSpilled struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.disk_space_spilled metric with initial data.
func (m *metricSparkStageDiskSpaceSpilled) init() {
	m.data.SetName("spark.stage.disk_space_spilled")
	m.data.SetDescription("The amount of disk space used for storing portions of overly large data chunks that couldn’t fit in memory in this stage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageDiskSpaceSpilled) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageDiskSpaceSpilled) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageDiskSpaceSpilled) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageDiskSpaceSpilled(settings MetricSettings) metricSparkStageDiskSpaceSpilled {
	m := metricSparkStageDiskSpaceSpilled{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageExecutorCPUTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.executor_cpu_time metric with initial data.
func (m *metricSparkStageExecutorCPUTime) init() {
	m.data.SetName("spark.stage.executor_cpu_time")
	m.data.SetDescription("CPU time spent by the executor in this stage.")
	m.data.SetUnit("ns")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageExecutorCPUTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageExecutorCPUTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageExecutorCPUTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageExecutorCPUTime(settings MetricSettings) metricSparkStageExecutorCPUTime {
	m := metricSparkStageExecutorCPUTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageExecutorRunTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.executor_run_time metric with initial data.
func (m *metricSparkStageExecutorRunTime) init() {
	m.data.SetName("spark.stage.executor_run_time")
	m.data.SetDescription("Amount of time spent by the executor in this stage.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageExecutorRunTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageExecutorRunTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageExecutorRunTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageExecutorRunTime(settings MetricSettings) metricSparkStageExecutorRunTime {
	m := metricSparkStageExecutorRunTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageFailedTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.failed_tasks metric with initial data.
func (m *metricSparkStageFailedTasks) init() {
	m.data.SetName("spark.stage.failed_tasks")
	m.data.SetDescription("Number of failed tasks in this stage.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageFailedTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageFailedTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageFailedTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageFailedTasks(settings MetricSettings) metricSparkStageFailedTasks {
	m := metricSparkStageFailedTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageInputBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.input_bytes metric with initial data.
func (m *metricSparkStageInputBytes) init() {
	m.data.SetName("spark.stage.input_bytes")
	m.data.SetDescription("Number of bytes read in this stage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageInputBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageInputBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageInputBytes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageInputBytes(settings MetricSettings) metricSparkStageInputBytes {
	m := metricSparkStageInputBytes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageInputRecords struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.input_records metric with initial data.
func (m *metricSparkStageInputRecords) init() {
	m.data.SetName("spark.stage.input_records")
	m.data.SetDescription("Number of records read in this stage.")
	m.data.SetUnit("{ records }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageInputRecords) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageInputRecords) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageInputRecords) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageInputRecords(settings MetricSettings) metricSparkStageInputRecords {
	m := metricSparkStageInputRecords{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageJvmGcTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.jvm_gc_time metric with initial data.
func (m *metricSparkStageJvmGcTime) init() {
	m.data.SetName("spark.stage.jvm_gc_time")
	m.data.SetDescription("The amount of time the JVM spent on garbage collection in this stage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageJvmGcTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageJvmGcTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageJvmGcTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageJvmGcTime(settings MetricSettings) metricSparkStageJvmGcTime {
	m := metricSparkStageJvmGcTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageKilledTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.killed_tasks metric with initial data.
func (m *metricSparkStageKilledTasks) init() {
	m.data.SetName("spark.stage.killed_tasks")
	m.data.SetDescription("Number of killed tasks in this stage.")
	m.data.SetUnit("{ tasks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageKilledTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageKilledTasks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageKilledTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageKilledTasks(settings MetricSettings) metricSparkStageKilledTasks {
	m := metricSparkStageKilledTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageMemorySpilled struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.memory_spilled metric with initial data.
func (m *metricSparkStageMemorySpilled) init() {
	m.data.SetName("spark.stage.memory_spilled")
	m.data.SetDescription("The amount of memory moved to disk due to size constraints (spilled) in this stage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageMemorySpilled) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageMemorySpilled) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageMemorySpilled) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageMemorySpilled(settings MetricSettings) metricSparkStageMemorySpilled {
	m := metricSparkStageMemorySpilled{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageOutputBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.output_bytes metric with initial data.
func (m *metricSparkStageOutputBytes) init() {
	m.data.SetName("spark.stage.output_bytes")
	m.data.SetDescription("Number of bytes written in this stage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageOutputBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageOutputBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageOutputBytes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageOutputBytes(settings MetricSettings) metricSparkStageOutputBytes {
	m := metricSparkStageOutputBytes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageOutputRecords struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.output_records metric with initial data.
func (m *metricSparkStageOutputRecords) init() {
	m.data.SetName("spark.stage.output_records")
	m.data.SetDescription("Number of records written in this stage.")
	m.data.SetUnit("{ records }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageOutputRecords) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageOutputRecords) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageOutputRecords) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageOutputRecords(settings MetricSettings) metricSparkStageOutputRecords {
	m := metricSparkStageOutputRecords{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStagePeakExecutionMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.peak_execution_memory metric with initial data.
func (m *metricSparkStagePeakExecutionMemory) init() {
	m.data.SetName("spark.stage.peak_execution_memory")
	m.data.SetDescription("Peak memory used by internal data structures created during shuffles, aggregations and joins in this stage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStagePeakExecutionMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStagePeakExecutionMemory) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStagePeakExecutionMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStagePeakExecutionMemory(settings MetricSettings) metricSparkStagePeakExecutionMemory {
	m := metricSparkStagePeakExecutionMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageResultSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.result_size metric with initial data.
func (m *metricSparkStageResultSize) init() {
	m.data.SetName("spark.stage.result_size")
	m.data.SetDescription("The sum of the bytes transmitted back to the driver by all the tasks in this stage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageResultSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageResultSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageResultSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageResultSize(settings MetricSettings) metricSparkStageResultSize {
	m := metricSparkStageResultSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageShuffleBlocksFetched struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.shuffle.blocks_fetched metric with initial data.
func (m *metricSparkStageShuffleBlocksFetched) init() {
	m.data.SetName("spark.stage.shuffle.blocks_fetched")
	m.data.SetDescription("Number of blocks fetched in shuffle operations in this stage.")
	m.data.SetUnit("{ blocks }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageShuffleBlocksFetched) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string, sourceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
	dp.Attributes().PutStr("source", sourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageShuffleBlocksFetched) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageShuffleBlocksFetched) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageShuffleBlocksFetched(settings MetricSettings) metricSparkStageShuffleBlocksFetched {
	m := metricSparkStageShuffleBlocksFetched{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageShuffleBytesRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.shuffle.bytes_read metric with initial data.
func (m *metricSparkStageShuffleBytesRead) init() {
	m.data.SetName("spark.stage.shuffle.bytes_read")
	m.data.SetDescription("Number of bytes read in shuffle operations in this stage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageShuffleBytesRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string, sourceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
	dp.Attributes().PutStr("source", sourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageShuffleBytesRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageShuffleBytesRead) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageShuffleBytesRead(settings MetricSettings) metricSparkStageShuffleBytesRead {
	m := metricSparkStageShuffleBytesRead{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageShuffleFetchWaitTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.shuffle.fetch_wait_time metric with initial data.
func (m *metricSparkStageShuffleFetchWaitTime) init() {
	m.data.SetName("spark.stage.shuffle.fetch_wait_time")
	m.data.SetDescription("Time spent in this stage waiting for remote shuffle blocks.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageShuffleFetchWaitTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageShuffleFetchWaitTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageShuffleFetchWaitTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageShuffleFetchWaitTime(settings MetricSettings) metricSparkStageShuffleFetchWaitTime {
	m := metricSparkStageShuffleFetchWaitTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageShuffleReadBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.shuffle.read_bytes metric with initial data.
func (m *metricSparkStageShuffleReadBytes) init() {
	m.data.SetName("spark.stage.shuffle.read_bytes")
	m.data.SetDescription("Number of bytes read in shuffle operations in this stage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageShuffleReadBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageShuffleReadBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageShuffleReadBytes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageShuffleReadBytes(settings MetricSettings) metricSparkStageShuffleReadBytes {
	m := metricSparkStageShuffleReadBytes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageShuffleReadRecords struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.shuffle.read_records metric with initial data.
func (m *metricSparkStageShuffleReadRecords) init() {
	m.data.SetName("spark.stage.shuffle.read_records")
	m.data.SetDescription("Number of records read in shuffle operations in this stage.")
	m.data.SetUnit("{ records }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageShuffleReadRecords) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageShuffleReadRecords) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageShuffleReadRecords) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageShuffleReadRecords(settings MetricSettings) metricSparkStageShuffleReadRecords {
	m := metricSparkStageShuffleReadRecords{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageShuffleRemoteBytesReadToDisk struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.shuffle.remote_bytes_read_to_disk metric with initial data.
func (m *metricSparkStageShuffleRemoteBytesReadToDisk) init() {
	m.data.SetName("spark.stage.shuffle.remote_bytes_read_to_disk")
	m.data.SetDescription("Number of remote bytes read to disk in shuffle operations (sometimes required for large blocks, as opposed to the default behavior of reading into memory).")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageShuffleRemoteBytesReadToDisk) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageShuffleRemoteBytesReadToDisk) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageShuffleRemoteBytesReadToDisk) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageShuffleRemoteBytesReadToDisk(settings MetricSettings) metricSparkStageShuffleRemoteBytesReadToDisk {
	m := metricSparkStageShuffleRemoteBytesReadToDisk{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageShuffleWriteBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.shuffle.write_bytes metric with initial data.
func (m *metricSparkStageShuffleWriteBytes) init() {
	m.data.SetName("spark.stage.shuffle.write_bytes")
	m.data.SetDescription("Number of bytes written in shuffle operations in this stage.")
	m.data.SetUnit("bytes")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageShuffleWriteBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageShuffleWriteBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageShuffleWriteBytes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageShuffleWriteBytes(settings MetricSettings) metricSparkStageShuffleWriteBytes {
	m := metricSparkStageShuffleWriteBytes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageShuffleWriteRecords struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.shuffle.write_records metric with initial data.
func (m *metricSparkStageShuffleWriteRecords) init() {
	m.data.SetName("spark.stage.shuffle.write_records")
	m.data.SetDescription("Number of records written in shuffle operations in this stage.")
	m.data.SetUnit("{ records }")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageShuffleWriteRecords) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageShuffleWriteRecords) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageShuffleWriteRecords) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageShuffleWriteRecords(settings MetricSettings) metricSparkStageShuffleWriteRecords {
	m := metricSparkStageShuffleWriteRecords{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSparkStageShuffleWriteTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills spark.stage.shuffle.write_time metric with initial data.
func (m *metricSparkStageShuffleWriteTime) init() {
	m.data.SetName("spark.stage.shuffle.write_time")
	m.data.SetDescription("Time spent blocking on writes to disk or buffer cache in this stage.")
	m.data.SetUnit("ns")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSparkStageShuffleWriteTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("application_id", applicationIDAttributeValue)
	dp.Attributes().PutStr("application_name", applicationNameAttributeValue)
	dp.Attributes().PutInt("stage_id", stageIDAttributeValue)
	dp.Attributes().PutInt("attempt_id", attemptIDAttributeValue)
	dp.Attributes().PutStr("stage_status", stageStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSparkStageShuffleWriteTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSparkStageShuffleWriteTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSparkStageShuffleWriteTime(settings MetricSettings) metricSparkStageShuffleWriteTime {
	m := metricSparkStageShuffleWriteTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilderConfig is a structural subset of an otherwise 1-1 copy of metadata.yaml
type MetricsBuilderConfig struct {
	Metrics            MetricsSettings            `mapstructure:"metrics"`
	ResourceAttributes ResourceAttributesSettings `mapstructure:"resource_attributes"`
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user settings.
type MetricsBuilder struct {
	startTime                                                     pcommon.Timestamp   // start time that will be applied to all recorded data points.
	metricsCapacity                                               int                 // maximum observed number of metrics per resource.
	resourceCapacity                                              int                 // maximum observed number of resource attributes.
	metricsBuffer                                                 pmetric.Metrics     // accumulates metrics data before emitting.
	buildInfo                                                     component.BuildInfo // contains version information
	resourceAttributesSettings                                    ResourceAttributesSettings
	metricSparkDriverBlockManagerDiskDiskSpaceUsed                metricSparkDriverBlockManagerDiskDiskSpaceUsed
	metricSparkDriverBlockManagerMemoryRemaining                  metricSparkDriverBlockManagerMemoryRemaining
	metricSparkDriverBlockManagerMemoryUsed                       metricSparkDriverBlockManagerMemoryUsed
	metricSparkDriverCodeGeneratorCompilationAverageTime          metricSparkDriverCodeGeneratorCompilationAverageTime
	metricSparkDriverCodeGeneratorCompilationCount                metricSparkDriverCodeGeneratorCompilationCount
	metricSparkDriverCodeGeneratorGeneratedClassAverageSize       metricSparkDriverCodeGeneratorGeneratedClassAverageSize
	metricSparkDriverCodeGeneratorGeneratedClassCount             metricSparkDriverCodeGeneratorGeneratedClassCount
	metricSparkDriverCodeGeneratorGeneratedMethodAverageSize      metricSparkDriverCodeGeneratorGeneratedMethodAverageSize
	metricSparkDriverCodeGeneratorGeneratedMethodCount            metricSparkDriverCodeGeneratorGeneratedMethodCount
	metricSparkDriverCodeGeneratorSourceCodeAverageSize           metricSparkDriverCodeGeneratorSourceCodeAverageSize
	metricSparkDriverCodeGeneratorSourceCodeCount                 metricSparkDriverCodeGeneratorSourceCodeCount
	metricSparkDriverDagSchedulerJobActiveJobs                    metricSparkDriverDagSchedulerJobActiveJobs
	metricSparkDriverDagSchedulerJobAllJobs                       metricSparkDriverDagSchedulerJobAllJobs
	metricSparkDriverDagSchedulerStageFailedStages                metricSparkDriverDagSchedulerStageFailedStages
	metricSparkDriverDagSchedulerStageRunningStages               metricSparkDriverDagSchedulerStageRunningStages
	metricSparkDriverDagSchedulerStageWaitingStages               metricSparkDriverDagSchedulerStageWaitingStages
	metricSparkDriverExecutorMetricsExecutionMemory               metricSparkDriverExecutorMetricsExecutionMemory
	metricSparkDriverExecutorMetricsGcCount                       metricSparkDriverExecutorMetricsGcCount
	metricSparkDriverExecutorMetricsGcTime                        metricSparkDriverExecutorMetricsGcTime
	metricSparkDriverExecutorMetricsJvmMemory                     metricSparkDriverExecutorMetricsJvmMemory
	metricSparkDriverExecutorMetricsPoolMemory                    metricSparkDriverExecutorMetricsPoolMemory
	metricSparkDriverExecutorMetricsStorageMemory                 metricSparkDriverExecutorMetricsStorageMemory
	metricSparkDriverHiveExternalCatalogFileCacheHits             metricSparkDriverHiveExternalCatalogFileCacheHits
	metricSparkDriverHiveExternalCatalogFilesDiscovered           metricSparkDriverHiveExternalCatalogFilesDiscovered
	metricSparkDriverHiveExternalCatalogHiveClientCalls           metricSparkDriverHiveExternalCatalogHiveClientCalls
	metricSparkDriverHiveExternalCatalogParallelListingJobs       metricSparkDriverHiveExternalCatalogParallelListingJobs
	metricSparkDriverHiveExternalCatalogPartitionsFetched         metricSparkDriverHiveExternalCatalogPartitionsFetched
	metricSparkDriverJvmCPUTime                                   metricSparkDriverJvmCPUTime
	metricSparkDriverLiveListenerBusEventsDropped                 metricSparkDriverLiveListenerBusEventsDropped
	metricSparkDriverLiveListenerBusEventsPosted                  metricSparkDriverLiveListenerBusEventsPosted
	metricSparkDriverLiveListenerBusListenerProcessingTimeAverage metricSparkDriverLiveListenerBusListenerProcessingTimeAverage
	metricSparkDriverLiveListenerBusQueueSize                     metricSparkDriverLiveListenerBusQueueSize
	metricSparkExecutorActiveTasks                                metricSparkExecutorActiveTasks
	metricSparkExecutorCompletedTasks                             metricSparkExecutorCompletedTasks
	metricSparkExecutorDiskUsed                                   metricSparkExecutorDiskUsed
	metricSparkExecutorDuration                                   metricSparkExecutorDuration
	metricSparkExecutorFailedTasks                                metricSparkExecutorFailedTasks
	metricSparkExecutorGcTime                                     metricSparkExecutorGcTime
	metricSparkExecutorInputBytes                                 metricSparkExecutorInputBytes
	metricSparkExecutorMaxTasks                                   metricSparkExecutorMaxTasks
	metricSparkExecutorMemoryUsed                                 metricSparkExecutorMemoryUsed
	metricSparkExecutorShuffleReadBytes                           metricSparkExecutorShuffleReadBytes
	metricSparkExecutorShuffleWriteBytes                          metricSparkExecutorShuffleWriteBytes
	metricSparkExecutorTotalStorageMemory                         metricSparkExecutorTotalStorageMemory
	metricSparkExecutorUsedStorageMemory                          metricSparkExecutorUsedStorageMemory
	metricSparkJobActiveStages                                    metricSparkJobActiveStages
	metricSparkJobActiveTasks                                     metricSparkJobActiveTasks
	metricSparkJobCompletedStages                                 metricSparkJobCompletedStages
	metricSparkJobCompletedTasks                                  metricSparkJobCompletedTasks
	metricSparkJobFailedStages                                    metricSparkJobFailedStages
	metricSparkJobFailedTasks                                     metricSparkJobFailedTasks
	metricSparkJobSkippedStages                                   metricSparkJobSkippedStages
	metricSparkJobSkippedTasks                                    metricSparkJobSkippedTasks
	metricSparkStageActiveTasks                                   metricSparkStageActiveTasks
	metricSparkStageCompleteTasks                                 metricSparkStageCompleteTasks
	metricSparkStageDiskSpaceSpilled                              metricSparkStageDiskSpaceSpilled
	metricSparkStageExecutorCPUTime                               metricSparkStageExecutorCPUTime
	metricSparkStageExecutorRunTime                               metricSparkStageExecutorRunTime
	metricSparkStageFailedTasks                                   metricSparkStageFailedTasks
	metricSparkStageInputBytes                                    metricSparkStageInputBytes
	metricSparkStageInputRecords                                  metricSparkStageInputRecords
	metricSparkStageJvmGcTime                                     metricSparkStageJvmGcTime
	metricSparkStageKilledTasks                                   metricSparkStageKilledTasks
	metricSparkStageMemorySpilled                                 metricSparkStageMemorySpilled
	metricSparkStageOutputBytes                                   metricSparkStageOutputBytes
	metricSparkStageOutputRecords                                 metricSparkStageOutputRecords
	metricSparkStagePeakExecutionMemory                           metricSparkStagePeakExecutionMemory
	metricSparkStageResultSize                                    metricSparkStageResultSize
	metricSparkStageShuffleBlocksFetched                          metricSparkStageShuffleBlocksFetched
	metricSparkStageShuffleBytesRead                              metricSparkStageShuffleBytesRead
	metricSparkStageShuffleFetchWaitTime                          metricSparkStageShuffleFetchWaitTime
	metricSparkStageShuffleReadBytes                              metricSparkStageShuffleReadBytes
	metricSparkStageShuffleReadRecords                            metricSparkStageShuffleReadRecords
	metricSparkStageShuffleRemoteBytesReadToDisk                  metricSparkStageShuffleRemoteBytesReadToDisk
	metricSparkStageShuffleWriteBytes                             metricSparkStageShuffleWriteBytes
	metricSparkStageShuffleWriteRecords                           metricSparkStageShuffleWriteRecords
	metricSparkStageShuffleWriteTime                              metricSparkStageShuffleWriteTime
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func DefaultMetricsBuilderConfig() MetricsBuilderConfig {
	return MetricsBuilderConfig{
		Metrics:            DefaultMetricsSettings(),
		ResourceAttributes: DefaultResourceAttributesSettings(),
	}
}

func NewMetricsBuilderConfig(ms MetricsSettings, ras ResourceAttributesSettings) MetricsBuilderConfig {
	return MetricsBuilderConfig{
		Metrics:            ms,
		ResourceAttributes: ras,
	}
}

func NewMetricsBuilder(mbc MetricsBuilderConfig, settings receiver.CreateSettings, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		startTime:                  pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:              pmetric.NewMetrics(),
		buildInfo:                  settings.BuildInfo,
		resourceAttributesSettings: mbc.ResourceAttributes,
		metricSparkDriverBlockManagerDiskDiskSpaceUsed:                newMetricSparkDriverBlockManagerDiskDiskSpaceUsed(mbc.Metrics.SparkDriverBlockManagerDiskDiskSpaceUsed),
		metricSparkDriverBlockManagerMemoryRemaining:                  newMetricSparkDriverBlockManagerMemoryRemaining(mbc.Metrics.SparkDriverBlockManagerMemoryRemaining),
		metricSparkDriverBlockManagerMemoryUsed:                       newMetricSparkDriverBlockManagerMemoryUsed(mbc.Metrics.SparkDriverBlockManagerMemoryUsed),
		metricSparkDriverCodeGeneratorCompilationAverageTime:          newMetricSparkDriverCodeGeneratorCompilationAverageTime(mbc.Metrics.SparkDriverCodeGeneratorCompilationAverageTime),
		metricSparkDriverCodeGeneratorCompilationCount:                newMetricSparkDriverCodeGeneratorCompilationCount(mbc.Metrics.SparkDriverCodeGeneratorCompilationCount),
		metricSparkDriverCodeGeneratorGeneratedClassAverageSize:       newMetricSparkDriverCodeGeneratorGeneratedClassAverageSize(mbc.Metrics.SparkDriverCodeGeneratorGeneratedClassAverageSize),
		metricSparkDriverCodeGeneratorGeneratedClassCount:             newMetricSparkDriverCodeGeneratorGeneratedClassCount(mbc.Metrics.SparkDriverCodeGeneratorGeneratedClassCount),
		metricSparkDriverCodeGeneratorGeneratedMethodAverageSize:      newMetricSparkDriverCodeGeneratorGeneratedMethodAverageSize(mbc.Metrics.SparkDriverCodeGeneratorGeneratedMethodAverageSize),
		metricSparkDriverCodeGeneratorGeneratedMethodCount:            newMetricSparkDriverCodeGeneratorGeneratedMethodCount(mbc.Metrics.SparkDriverCodeGeneratorGeneratedMethodCount),
		metricSparkDriverCodeGeneratorSourceCodeAverageSize:           newMetricSparkDriverCodeGeneratorSourceCodeAverageSize(mbc.Metrics.SparkDriverCodeGeneratorSourceCodeAverageSize),
		metricSparkDriverCodeGeneratorSourceCodeCount:                 newMetricSparkDriverCodeGeneratorSourceCodeCount(mbc.Metrics.SparkDriverCodeGeneratorSourceCodeCount),
		metricSparkDriverDagSchedulerJobActiveJobs:                    newMetricSparkDriverDagSchedulerJobActiveJobs(mbc.Metrics.SparkDriverDagSchedulerJobActiveJobs),
		metricSparkDriverDagSchedulerJobAllJobs:                       newMetricSparkDriverDagSchedulerJobAllJobs(mbc.Metrics.SparkDriverDagSchedulerJobAllJobs),
		metricSparkDriverDagSchedulerStageFailedStages:                newMetricSparkDriverDagSchedulerStageFailedStages(mbc.Metrics.SparkDriverDagSchedulerStageFailedStages),
		metricSparkDriverDagSchedulerStageRunningStages:               newMetricSparkDriverDagSchedulerStageRunningStages(mbc.Metrics.SparkDriverDagSchedulerStageRunningStages),
		metricSparkDriverDagSchedulerStageWaitingStages:               newMetricSparkDriverDagSchedulerStageWaitingStages(mbc.Metrics.SparkDriverDagSchedulerStageWaitingStages),
		metricSparkDriverExecutorMetricsExecutionMemory:               newMetricSparkDriverExecutorMetricsExecutionMemory(mbc.Metrics.SparkDriverExecutorMetricsExecutionMemory),
		metricSparkDriverExecutorMetricsGcCount:                       newMetricSparkDriverExecutorMetricsGcCount(mbc.Metrics.SparkDriverExecutorMetricsGcCount),
		metricSparkDriverExecutorMetricsGcTime:                        newMetricSparkDriverExecutorMetricsGcTime(mbc.Metrics.SparkDriverExecutorMetricsGcTime),
		metricSparkDriverExecutorMetricsJvmMemory:                     newMetricSparkDriverExecutorMetricsJvmMemory(mbc.Metrics.SparkDriverExecutorMetricsJvmMemory),
		metricSparkDriverExecutorMetricsPoolMemory:                    newMetricSparkDriverExecutorMetricsPoolMemory(mbc.Metrics.SparkDriverExecutorMetricsPoolMemory),
		metricSparkDriverExecutorMetricsStorageMemory:                 newMetricSparkDriverExecutorMetricsStorageMemory(mbc.Metrics.SparkDriverExecutorMetricsStorageMemory),
		metricSparkDriverHiveExternalCatalogFileCacheHits:             newMetricSparkDriverHiveExternalCatalogFileCacheHits(mbc.Metrics.SparkDriverHiveExternalCatalogFileCacheHits),
		metricSparkDriverHiveExternalCatalogFilesDiscovered:           newMetricSparkDriverHiveExternalCatalogFilesDiscovered(mbc.Metrics.SparkDriverHiveExternalCatalogFilesDiscovered),
		metricSparkDriverHiveExternalCatalogHiveClientCalls:           newMetricSparkDriverHiveExternalCatalogHiveClientCalls(mbc.Metrics.SparkDriverHiveExternalCatalogHiveClientCalls),
		metricSparkDriverHiveExternalCatalogParallelListingJobs:       newMetricSparkDriverHiveExternalCatalogParallelListingJobs(mbc.Metrics.SparkDriverHiveExternalCatalogParallelListingJobs),
		metricSparkDriverHiveExternalCatalogPartitionsFetched:         newMetricSparkDriverHiveExternalCatalogPartitionsFetched(mbc.Metrics.SparkDriverHiveExternalCatalogPartitionsFetched),
		metricSparkDriverJvmCPUTime:                                   newMetricSparkDriverJvmCPUTime(mbc.Metrics.SparkDriverJvmCPUTime),
		metricSparkDriverLiveListenerBusEventsDropped:                 newMetricSparkDriverLiveListenerBusEventsDropped(mbc.Metrics.SparkDriverLiveListenerBusEventsDropped),
		metricSparkDriverLiveListenerBusEventsPosted:                  newMetricSparkDriverLiveListenerBusEventsPosted(mbc.Metrics.SparkDriverLiveListenerBusEventsPosted),
		metricSparkDriverLiveListenerBusListenerProcessingTimeAverage: newMetricSparkDriverLiveListenerBusListenerProcessingTimeAverage(mbc.Metrics.SparkDriverLiveListenerBusListenerProcessingTimeAverage),
		metricSparkDriverLiveListenerBusQueueSize:                     newMetricSparkDriverLiveListenerBusQueueSize(mbc.Metrics.SparkDriverLiveListenerBusQueueSize),
		metricSparkExecutorActiveTasks:                                newMetricSparkExecutorActiveTasks(mbc.Metrics.SparkExecutorActiveTasks),
		metricSparkExecutorCompletedTasks:                             newMetricSparkExecutorCompletedTasks(mbc.Metrics.SparkExecutorCompletedTasks),
		metricSparkExecutorDiskUsed:                                   newMetricSparkExecutorDiskUsed(mbc.Metrics.SparkExecutorDiskUsed),
		metricSparkExecutorDuration:                                   newMetricSparkExecutorDuration(mbc.Metrics.SparkExecutorDuration),
		metricSparkExecutorFailedTasks:                                newMetricSparkExecutorFailedTasks(mbc.Metrics.SparkExecutorFailedTasks),
		metricSparkExecutorGcTime:                                     newMetricSparkExecutorGcTime(mbc.Metrics.SparkExecutorGcTime),
		metricSparkExecutorInputBytes:                                 newMetricSparkExecutorInputBytes(mbc.Metrics.SparkExecutorInputBytes),
		metricSparkExecutorMaxTasks:                                   newMetricSparkExecutorMaxTasks(mbc.Metrics.SparkExecutorMaxTasks),
		metricSparkExecutorMemoryUsed:                                 newMetricSparkExecutorMemoryUsed(mbc.Metrics.SparkExecutorMemoryUsed),
		metricSparkExecutorShuffleReadBytes:                           newMetricSparkExecutorShuffleReadBytes(mbc.Metrics.SparkExecutorShuffleReadBytes),
		metricSparkExecutorShuffleWriteBytes:                          newMetricSparkExecutorShuffleWriteBytes(mbc.Metrics.SparkExecutorShuffleWriteBytes),
		metricSparkExecutorTotalStorageMemory:                         newMetricSparkExecutorTotalStorageMemory(mbc.Metrics.SparkExecutorTotalStorageMemory),
		metricSparkExecutorUsedStorageMemory:                          newMetricSparkExecutorUsedStorageMemory(mbc.Metrics.SparkExecutorUsedStorageMemory),
		metricSparkJobActiveStages:                                    newMetricSparkJobActiveStages(mbc.Metrics.SparkJobActiveStages),
		metricSparkJobActiveTasks:                                     newMetricSparkJobActiveTasks(mbc.Metrics.SparkJobActiveTasks),
		metricSparkJobCompletedStages:                                 newMetricSparkJobCompletedStages(mbc.Metrics.SparkJobCompletedStages),
		metricSparkJobCompletedTasks:                                  newMetricSparkJobCompletedTasks(mbc.Metrics.SparkJobCompletedTasks),
		metricSparkJobFailedStages:                                    newMetricSparkJobFailedStages(mbc.Metrics.SparkJobFailedStages),
		metricSparkJobFailedTasks:                                     newMetricSparkJobFailedTasks(mbc.Metrics.SparkJobFailedTasks),
		metricSparkJobSkippedStages:                                   newMetricSparkJobSkippedStages(mbc.Metrics.SparkJobSkippedStages),
		metricSparkJobSkippedTasks:                                    newMetricSparkJobSkippedTasks(mbc.Metrics.SparkJobSkippedTasks),
		metricSparkStageActiveTasks:                                   newMetricSparkStageActiveTasks(mbc.Metrics.SparkStageActiveTasks),
		metricSparkStageCompleteTasks:                                 newMetricSparkStageCompleteTasks(mbc.Metrics.SparkStageCompleteTasks),
		metricSparkStageDiskSpaceSpilled:                              newMetricSparkStageDiskSpaceSpilled(mbc.Metrics.SparkStageDiskSpaceSpilled),
		metricSparkStageExecutorCPUTime:                               newMetricSparkStageExecutorCPUTime(mbc.Metrics.SparkStageExecutorCPUTime),
		metricSparkStageExecutorRunTime:                               newMetricSparkStageExecutorRunTime(mbc.Metrics.SparkStageExecutorRunTime),
		metricSparkStageFailedTasks:                                   newMetricSparkStageFailedTasks(mbc.Metrics.SparkStageFailedTasks),
		metricSparkStageInputBytes:                                    newMetricSparkStageInputBytes(mbc.Metrics.SparkStageInputBytes),
		metricSparkStageInputRecords:                                  newMetricSparkStageInputRecords(mbc.Metrics.SparkStageInputRecords),
		metricSparkStageJvmGcTime:                                     newMetricSparkStageJvmGcTime(mbc.Metrics.SparkStageJvmGcTime),
		metricSparkStageKilledTasks:                                   newMetricSparkStageKilledTasks(mbc.Metrics.SparkStageKilledTasks),
		metricSparkStageMemorySpilled:                                 newMetricSparkStageMemorySpilled(mbc.Metrics.SparkStageMemorySpilled),
		metricSparkStageOutputBytes:                                   newMetricSparkStageOutputBytes(mbc.Metrics.SparkStageOutputBytes),
		metricSparkStageOutputRecords:                                 newMetricSparkStageOutputRecords(mbc.Metrics.SparkStageOutputRecords),
		metricSparkStagePeakExecutionMemory:                           newMetricSparkStagePeakExecutionMemory(mbc.Metrics.SparkStagePeakExecutionMemory),
		metricSparkStageResultSize:                                    newMetricSparkStageResultSize(mbc.Metrics.SparkStageResultSize),
		metricSparkStageShuffleBlocksFetched:                          newMetricSparkStageShuffleBlocksFetched(mbc.Metrics.SparkStageShuffleBlocksFetched),
		metricSparkStageShuffleBytesRead:                              newMetricSparkStageShuffleBytesRead(mbc.Metrics.SparkStageShuffleBytesRead),
		metricSparkStageShuffleFetchWaitTime:                          newMetricSparkStageShuffleFetchWaitTime(mbc.Metrics.SparkStageShuffleFetchWaitTime),
		metricSparkStageShuffleReadBytes:                              newMetricSparkStageShuffleReadBytes(mbc.Metrics.SparkStageShuffleReadBytes),
		metricSparkStageShuffleReadRecords:                            newMetricSparkStageShuffleReadRecords(mbc.Metrics.SparkStageShuffleReadRecords),
		metricSparkStageShuffleRemoteBytesReadToDisk:                  newMetricSparkStageShuffleRemoteBytesReadToDisk(mbc.Metrics.SparkStageShuffleRemoteBytesReadToDisk),
		metricSparkStageShuffleWriteBytes:                             newMetricSparkStageShuffleWriteBytes(mbc.Metrics.SparkStageShuffleWriteBytes),
		metricSparkStageShuffleWriteRecords:                           newMetricSparkStageShuffleWriteRecords(mbc.Metrics.SparkStageShuffleWriteRecords),
		metricSparkStageShuffleWriteTime:                              newMetricSparkStageShuffleWriteTime(mbc.Metrics.SparkStageShuffleWriteTime),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
	if mb.resourceCapacity < rm.Resource().Attributes().Len() {
		mb.resourceCapacity = rm.Resource().Attributes().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption func(ResourceAttributesSettings, pmetric.ResourceMetrics)

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return func(ras ResourceAttributesSettings, rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	}
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(rmo ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	rm.Resource().Attributes().EnsureCapacity(mb.resourceCapacity)
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName("otelcol/apachesparkreceiver")
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricSparkDriverBlockManagerDiskDiskSpaceUsed.emit(ils.Metrics())
	mb.metricSparkDriverBlockManagerMemoryRemaining.emit(ils.Metrics())
	mb.metricSparkDriverBlockManagerMemoryUsed.emit(ils.Metrics())
	mb.metricSparkDriverCodeGeneratorCompilationAverageTime.emit(ils.Metrics())
	mb.metricSparkDriverCodeGeneratorCompilationCount.emit(ils.Metrics())
	mb.metricSparkDriverCodeGeneratorGeneratedClassAverageSize.emit(ils.Metrics())
	mb.metricSparkDriverCodeGeneratorGeneratedClassCount.emit(ils.Metrics())
	mb.metricSparkDriverCodeGeneratorGeneratedMethodAverageSize.emit(ils.Metrics())
	mb.metricSparkDriverCodeGeneratorGeneratedMethodCount.emit(ils.Metrics())
	mb.metricSparkDriverCodeGeneratorSourceCodeAverageSize.emit(ils.Metrics())
	mb.metricSparkDriverCodeGeneratorSourceCodeCount.emit(ils.Metrics())
	mb.metricSparkDriverDagSchedulerJobActiveJobs.emit(ils.Metrics())
	mb.metricSparkDriverDagSchedulerJobAllJobs.emit(ils.Metrics())
	mb.metricSparkDriverDagSchedulerStageFailedStages.emit(ils.Metrics())
	mb.metricSparkDriverDagSchedulerStageRunningStages.emit(ils.Metrics())
	mb.metricSparkDriverDagSchedulerStageWaitingStages.emit(ils.Metrics())
	mb.metricSparkDriverExecutorMetricsExecutionMemory.emit(ils.Metrics())
	mb.metricSparkDriverExecutorMetricsGcCount.emit(ils.Metrics())
	mb.metricSparkDriverExecutorMetricsGcTime.emit(ils.Metrics())
	mb.metricSparkDriverExecutorMetricsJvmMemory.emit(ils.Metrics())
	mb.metricSparkDriverExecutorMetricsPoolMemory.emit(ils.Metrics())
	mb.metricSparkDriverExecutorMetricsStorageMemory.emit(ils.Metrics())
	mb.metricSparkDriverHiveExternalCatalogFileCacheHits.emit(ils.Metrics())
	mb.metricSparkDriverHiveExternalCatalogFilesDiscovered.emit(ils.Metrics())
	mb.metricSparkDriverHiveExternalCatalogHiveClientCalls.emit(ils.Metrics())
	mb.metricSparkDriverHiveExternalCatalogParallelListingJobs.emit(ils.Metrics())
	mb.metricSparkDriverHiveExternalCatalogPartitionsFetched.emit(ils.Metrics())
	mb.metricSparkDriverJvmCPUTime.emit(ils.Metrics())
	mb.metricSparkDriverLiveListenerBusEventsDropped.emit(ils.Metrics())
	mb.metricSparkDriverLiveListenerBusEventsPosted.emit(ils.Metrics())
	mb.metricSparkDriverLiveListenerBusListenerProcessingTimeAverage.emit(ils.Metrics())
	mb.metricSparkDriverLiveListenerBusQueueSize.emit(ils.Metrics())
	mb.metricSparkExecutorActiveTasks.emit(ils.Metrics())
	mb.metricSparkExecutorCompletedTasks.emit(ils.Metrics())
	mb.metricSparkExecutorDiskUsed.emit(ils.Metrics())
	mb.metricSparkExecutorDuration.emit(ils.Metrics())
	mb.metricSparkExecutorFailedTasks.emit(ils.Metrics())
	mb.metricSparkExecutorGcTime.emit(ils.Metrics())
	mb.metricSparkExecutorInputBytes.emit(ils.Metrics())
	mb.metricSparkExecutorMaxTasks.emit(ils.Metrics())
	mb.metricSparkExecutorMemoryUsed.emit(ils.Metrics())
	mb.metricSparkExecutorShuffleReadBytes.emit(ils.Metrics())
	mb.metricSparkExecutorShuffleWriteBytes.emit(ils.Metrics())
	mb.metricSparkExecutorTotalStorageMemory.emit(ils.Metrics())
	mb.metricSparkExecutorUsedStorageMemory.emit(ils.Metrics())
	mb.metricSparkJobActiveStages.emit(ils.Metrics())
	mb.metricSparkJobActiveTasks.emit(ils.Metrics())
	mb.metricSparkJobCompletedStages.emit(ils.Metrics())
	mb.metricSparkJobCompletedTasks.emit(ils.Metrics())
	mb.metricSparkJobFailedStages.emit(ils.Metrics())
	mb.metricSparkJobFailedTasks.emit(ils.Metrics())
	mb.metricSparkJobSkippedStages.emit(ils.Metrics())
	mb.metricSparkJobSkippedTasks.emit(ils.Metrics())
	mb.metricSparkStageActiveTasks.emit(ils.Metrics())
	mb.metricSparkStageCompleteTasks.emit(ils.Metrics())
	mb.metricSparkStageDiskSpaceSpilled.emit(ils.Metrics())
	mb.metricSparkStageExecutorCPUTime.emit(ils.Metrics())
	mb.metricSparkStageExecutorRunTime.emit(ils.Metrics())
	mb.metricSparkStageFailedTasks.emit(ils.Metrics())
	mb.metricSparkStageInputBytes.emit(ils.Metrics())
	mb.metricSparkStageInputRecords.emit(ils.Metrics())
	mb.metricSparkStageJvmGcTime.emit(ils.Metrics())
	mb.metricSparkStageKilledTasks.emit(ils.Metrics())
	mb.metricSparkStageMemorySpilled.emit(ils.Metrics())
	mb.metricSparkStageOutputBytes.emit(ils.Metrics())
	mb.metricSparkStageOutputRecords.emit(ils.Metrics())
	mb.metricSparkStagePeakExecutionMemory.emit(ils.Metrics())
	mb.metricSparkStageResultSize.emit(ils.Metrics())
	mb.metricSparkStageShuffleBlocksFetched.emit(ils.Metrics())
	mb.metricSparkStageShuffleBytesRead.emit(ils.Metrics())
	mb.metricSparkStageShuffleFetchWaitTime.emit(ils.Metrics())
	mb.metricSparkStageShuffleReadBytes.emit(ils.Metrics())
	mb.metricSparkStageShuffleReadRecords.emit(ils.Metrics())
	mb.metricSparkStageShuffleRemoteBytesReadToDisk.emit(ils.Metrics())
	mb.metricSparkStageShuffleWriteBytes.emit(ils.Metrics())
	mb.metricSparkStageShuffleWriteRecords.emit(ils.Metrics())
	mb.metricSparkStageShuffleWriteTime.emit(ils.Metrics())

	for _, op := range rmo {
		op(mb.resourceAttributesSettings, rm)
	}
	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user settings, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(rmo ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(rmo...)
	metrics := mb.metricsBuffer
	mb.metricsBuffer = pmetric.NewMetrics()
	return metrics
}

// RecordSparkDriverBlockManagerDiskDiskSpaceUsedDataPoint adds a data point to spark.driver.block_manager.disk.diskSpaceUsed metric.
func (mb *MetricsBuilder) RecordSparkDriverBlockManagerDiskDiskSpaceUsedDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverBlockManagerDiskDiskSpaceUsed.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverBlockManagerMemoryRemainingDataPoint adds a data point to spark.driver.block_manager.memory.remaining metric.
func (mb *MetricsBuilder) RecordSparkDriverBlockManagerMemoryRemainingDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, locationAttributeValue AttributeLocation) {
	mb.metricSparkDriverBlockManagerMemoryRemaining.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, locationAttributeValue.String())
}

// RecordSparkDriverBlockManagerMemoryUsedDataPoint adds a data point to spark.driver.block_manager.memory.used metric.
func (mb *MetricsBuilder) RecordSparkDriverBlockManagerMemoryUsedDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, locationAttributeValue AttributeLocation) {
	mb.metricSparkDriverBlockManagerMemoryUsed.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, locationAttributeValue.String())
}

// RecordSparkDriverCodeGeneratorCompilationAverageTimeDataPoint adds a data point to spark.driver.code_generator.compilation.average_time metric.
func (mb *MetricsBuilder) RecordSparkDriverCodeGeneratorCompilationAverageTimeDataPoint(ts pcommon.Timestamp, val float64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverCodeGeneratorCompilationAverageTime.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverCodeGeneratorCompilationCountDataPoint adds a data point to spark.driver.code_generator.compilation.count metric.
func (mb *MetricsBuilder) RecordSparkDriverCodeGeneratorCompilationCountDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverCodeGeneratorCompilationCount.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverCodeGeneratorGeneratedClassAverageSizeDataPoint adds a data point to spark.driver.code_generator.generated_class.average_size metric.
func (mb *MetricsBuilder) RecordSparkDriverCodeGeneratorGeneratedClassAverageSizeDataPoint(ts pcommon.Timestamp, val float64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverCodeGeneratorGeneratedClassAverageSize.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverCodeGeneratorGeneratedClassCountDataPoint adds a data point to spark.driver.code_generator.generated_class.count metric.
func (mb *MetricsBuilder) RecordSparkDriverCodeGeneratorGeneratedClassCountDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverCodeGeneratorGeneratedClassCount.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverCodeGeneratorGeneratedMethodAverageSizeDataPoint adds a data point to spark.driver.code_generator.generated_method.average_size metric.
func (mb *MetricsBuilder) RecordSparkDriverCodeGeneratorGeneratedMethodAverageSizeDataPoint(ts pcommon.Timestamp, val float64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverCodeGeneratorGeneratedMethodAverageSize.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverCodeGeneratorGeneratedMethodCountDataPoint adds a data point to spark.driver.code_generator.generated_method.count metric.
func (mb *MetricsBuilder) RecordSparkDriverCodeGeneratorGeneratedMethodCountDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverCodeGeneratorGeneratedMethodCount.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverCodeGeneratorSourceCodeAverageSizeDataPoint adds a data point to spark.driver.code_generator.source_code.average_size metric.
func (mb *MetricsBuilder) RecordSparkDriverCodeGeneratorSourceCodeAverageSizeDataPoint(ts pcommon.Timestamp, val float64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverCodeGeneratorSourceCodeAverageSize.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverCodeGeneratorSourceCodeCountDataPoint adds a data point to spark.driver.code_generator.source_code.count metric.
func (mb *MetricsBuilder) RecordSparkDriverCodeGeneratorSourceCodeCountDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverCodeGeneratorSourceCodeCount.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverDagSchedulerJobActiveJobsDataPoint adds a data point to spark.driver.dag_scheduler.job.active_jobs metric.
func (mb *MetricsBuilder) RecordSparkDriverDagSchedulerJobActiveJobsDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverDagSchedulerJobActiveJobs.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverDagSchedulerJobAllJobsDataPoint adds a data point to spark.driver.dag_scheduler.job.all_jobs metric.
func (mb *MetricsBuilder) RecordSparkDriverDagSchedulerJobAllJobsDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverDagSchedulerJobAllJobs.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverDagSchedulerStageFailedStagesDataPoint adds a data point to spark.driver.dag_scheduler.stage.failed_stages metric.
func (mb *MetricsBuilder) RecordSparkDriverDagSchedulerStageFailedStagesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverDagSchedulerStageFailedStages.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverDagSchedulerStageRunningStagesDataPoint adds a data point to spark.driver.dag_scheduler.stage.running_stages metric.
func (mb *MetricsBuilder) RecordSparkDriverDagSchedulerStageRunningStagesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverDagSchedulerStageRunningStages.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverDagSchedulerStageWaitingStagesDataPoint adds a data point to spark.driver.dag_scheduler.stage.waiting_stages metric.
func (mb *MetricsBuilder) RecordSparkDriverDagSchedulerStageWaitingStagesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverDagSchedulerStageWaitingStages.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverExecutorMetricsExecutionMemoryDataPoint adds a data point to spark.driver.executor_metrics.execution_memory metric.
func (mb *MetricsBuilder) RecordSparkDriverExecutorMetricsExecutionMemoryDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, locationAttributeValue AttributeLocation) {
	mb.metricSparkDriverExecutorMetricsExecutionMemory.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, locationAttributeValue.String())
}

// RecordSparkDriverExecutorMetricsGcCountDataPoint adds a data point to spark.driver.executor_metrics.gc_count metric.
func (mb *MetricsBuilder) RecordSparkDriverExecutorMetricsGcCountDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, gcTypeAttributeValue AttributeGcType) {
	mb.metricSparkDriverExecutorMetricsGcCount.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, gcTypeAttributeValue.String())
}

// RecordSparkDriverExecutorMetricsGcTimeDataPoint adds a data point to spark.driver.executor_metrics.gc_time metric.
func (mb *MetricsBuilder) RecordSparkDriverExecutorMetricsGcTimeDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, gcTypeAttributeValue AttributeGcType) {
	mb.metricSparkDriverExecutorMetricsGcTime.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, gcTypeAttributeValue.String())
}

// RecordSparkDriverExecutorMetricsJvmMemoryDataPoint adds a data point to spark.driver.executor_metrics.jvm_memory metric.
func (mb *MetricsBuilder) RecordSparkDriverExecutorMetricsJvmMemoryDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, locationAttributeValue AttributeLocation) {
	mb.metricSparkDriverExecutorMetricsJvmMemory.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, locationAttributeValue.String())
}

// RecordSparkDriverExecutorMetricsPoolMemoryDataPoint adds a data point to spark.driver.executor_metrics.pool_memory metric.
func (mb *MetricsBuilder) RecordSparkDriverExecutorMetricsPoolMemoryDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, poolMemoryTypeAttributeValue AttributePoolMemoryType) {
	mb.metricSparkDriverExecutorMetricsPoolMemory.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, poolMemoryTypeAttributeValue.String())
}

// RecordSparkDriverExecutorMetricsStorageMemoryDataPoint adds a data point to spark.driver.executor_metrics.storage_memory metric.
func (mb *MetricsBuilder) RecordSparkDriverExecutorMetricsStorageMemoryDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, locationAttributeValue AttributeLocation) {
	mb.metricSparkDriverExecutorMetricsStorageMemory.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, locationAttributeValue.String())
}

// RecordSparkDriverHiveExternalCatalogFileCacheHitsDataPoint adds a data point to spark.driver.hive_external_catalog.file_cache_hits metric.
func (mb *MetricsBuilder) RecordSparkDriverHiveExternalCatalogFileCacheHitsDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverHiveExternalCatalogFileCacheHits.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverHiveExternalCatalogFilesDiscoveredDataPoint adds a data point to spark.driver.hive_external_catalog.files_discovered metric.
func (mb *MetricsBuilder) RecordSparkDriverHiveExternalCatalogFilesDiscoveredDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverHiveExternalCatalogFilesDiscovered.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverHiveExternalCatalogHiveClientCallsDataPoint adds a data point to spark.driver.hive_external_catalog.hive_client_calls metric.
func (mb *MetricsBuilder) RecordSparkDriverHiveExternalCatalogHiveClientCallsDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverHiveExternalCatalogHiveClientCalls.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverHiveExternalCatalogParallelListingJobsDataPoint adds a data point to spark.driver.hive_external_catalog.parallel_listing_jobs metric.
func (mb *MetricsBuilder) RecordSparkDriverHiveExternalCatalogParallelListingJobsDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverHiveExternalCatalogParallelListingJobs.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverHiveExternalCatalogPartitionsFetchedDataPoint adds a data point to spark.driver.hive_external_catalog.partitions_fetched metric.
func (mb *MetricsBuilder) RecordSparkDriverHiveExternalCatalogPartitionsFetchedDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverHiveExternalCatalogPartitionsFetched.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverJvmCPUTimeDataPoint adds a data point to spark.driver.jvm_cpu_time metric.
func (mb *MetricsBuilder) RecordSparkDriverJvmCPUTimeDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverJvmCPUTime.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverLiveListenerBusEventsDroppedDataPoint adds a data point to spark.driver.live_listener_bus.events_dropped metric.
func (mb *MetricsBuilder) RecordSparkDriverLiveListenerBusEventsDroppedDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverLiveListenerBusEventsDropped.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverLiveListenerBusEventsPostedDataPoint adds a data point to spark.driver.live_listener_bus.events_posted metric.
func (mb *MetricsBuilder) RecordSparkDriverLiveListenerBusEventsPostedDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverLiveListenerBusEventsPosted.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverLiveListenerBusListenerProcessingTimeAverageDataPoint adds a data point to spark.driver.live_listener_bus.listener_processing_time.average metric.
func (mb *MetricsBuilder) RecordSparkDriverLiveListenerBusListenerProcessingTimeAverageDataPoint(ts pcommon.Timestamp, val float64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverLiveListenerBusListenerProcessingTimeAverage.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkDriverLiveListenerBusQueueSizeDataPoint adds a data point to spark.driver.live_listener_bus.queue_size metric.
func (mb *MetricsBuilder) RecordSparkDriverLiveListenerBusQueueSizeDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string) {
	mb.metricSparkDriverLiveListenerBusQueueSize.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue)
}

// RecordSparkExecutorActiveTasksDataPoint adds a data point to spark.executor.active_tasks metric.
func (mb *MetricsBuilder) RecordSparkExecutorActiveTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	mb.metricSparkExecutorActiveTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue)
}

// RecordSparkExecutorCompletedTasksDataPoint adds a data point to spark.executor.completed_tasks metric.
func (mb *MetricsBuilder) RecordSparkExecutorCompletedTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	mb.metricSparkExecutorCompletedTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue)
}

// RecordSparkExecutorDiskUsedDataPoint adds a data point to spark.executor.disk_used metric.
func (mb *MetricsBuilder) RecordSparkExecutorDiskUsedDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	mb.metricSparkExecutorDiskUsed.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue)
}

// RecordSparkExecutorDurationDataPoint adds a data point to spark.executor.duration metric.
func (mb *MetricsBuilder) RecordSparkExecutorDurationDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	mb.metricSparkExecutorDuration.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue)
}

// RecordSparkExecutorFailedTasksDataPoint adds a data point to spark.executor.failed_tasks metric.
func (mb *MetricsBuilder) RecordSparkExecutorFailedTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	mb.metricSparkExecutorFailedTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue)
}

// RecordSparkExecutorGcTimeDataPoint adds a data point to spark.executor.gc_time metric.
func (mb *MetricsBuilder) RecordSparkExecutorGcTimeDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	mb.metricSparkExecutorGcTime.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue)
}

// RecordSparkExecutorInputBytesDataPoint adds a data point to spark.executor.input_bytes metric.
func (mb *MetricsBuilder) RecordSparkExecutorInputBytesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	mb.metricSparkExecutorInputBytes.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue)
}

// RecordSparkExecutorMaxTasksDataPoint adds a data point to spark.executor.max_tasks metric.
func (mb *MetricsBuilder) RecordSparkExecutorMaxTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	mb.metricSparkExecutorMaxTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue)
}

// RecordSparkExecutorMemoryUsedDataPoint adds a data point to spark.executor.memory_used metric.
func (mb *MetricsBuilder) RecordSparkExecutorMemoryUsedDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	mb.metricSparkExecutorMemoryUsed.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue)
}

// RecordSparkExecutorShuffleReadBytesDataPoint adds a data point to spark.executor.shuffle_read_bytes metric.
func (mb *MetricsBuilder) RecordSparkExecutorShuffleReadBytesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	mb.metricSparkExecutorShuffleReadBytes.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue)
}

// RecordSparkExecutorShuffleWriteBytesDataPoint adds a data point to spark.executor.shuffle_write_bytes metric.
func (mb *MetricsBuilder) RecordSparkExecutorShuffleWriteBytesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string) {
	mb.metricSparkExecutorShuffleWriteBytes.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue)
}

// RecordSparkExecutorTotalStorageMemoryDataPoint adds a data point to spark.executor.total_storage_memory metric.
func (mb *MetricsBuilder) RecordSparkExecutorTotalStorageMemoryDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string, locationAttributeValue AttributeLocation) {
	mb.metricSparkExecutorTotalStorageMemory.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue, locationAttributeValue.String())
}

// RecordSparkExecutorUsedStorageMemoryDataPoint adds a data point to spark.executor.used_storage_memory metric.
func (mb *MetricsBuilder) RecordSparkExecutorUsedStorageMemoryDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, executorIDAttributeValue string, locationAttributeValue AttributeLocation) {
	mb.metricSparkExecutorUsedStorageMemory.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, executorIDAttributeValue, locationAttributeValue.String())
}

// RecordSparkJobActiveStagesDataPoint adds a data point to spark.job.active_stages metric.
func (mb *MetricsBuilder) RecordSparkJobActiveStagesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	mb.metricSparkJobActiveStages.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, jobIDAttributeValue)
}

// RecordSparkJobActiveTasksDataPoint adds a data point to spark.job.active_tasks metric.
func (mb *MetricsBuilder) RecordSparkJobActiveTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	mb.metricSparkJobActiveTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, jobIDAttributeValue)
}

// RecordSparkJobCompletedStagesDataPoint adds a data point to spark.job.completed_stages metric.
func (mb *MetricsBuilder) RecordSparkJobCompletedStagesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	mb.metricSparkJobCompletedStages.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, jobIDAttributeValue)
}

// RecordSparkJobCompletedTasksDataPoint adds a data point to spark.job.completed_tasks metric.
func (mb *MetricsBuilder) RecordSparkJobCompletedTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	mb.metricSparkJobCompletedTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, jobIDAttributeValue)
}

// RecordSparkJobFailedStagesDataPoint adds a data point to spark.job.failed_stages metric.
func (mb *MetricsBuilder) RecordSparkJobFailedStagesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	mb.metricSparkJobFailedStages.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, jobIDAttributeValue)
}

// RecordSparkJobFailedTasksDataPoint adds a data point to spark.job.failed_tasks metric.
func (mb *MetricsBuilder) RecordSparkJobFailedTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	mb.metricSparkJobFailedTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, jobIDAttributeValue)
}

// RecordSparkJobSkippedStagesDataPoint adds a data point to spark.job.skipped_stages metric.
func (mb *MetricsBuilder) RecordSparkJobSkippedStagesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	mb.metricSparkJobSkippedStages.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, jobIDAttributeValue)
}

// RecordSparkJobSkippedTasksDataPoint adds a data point to spark.job.skipped_tasks metric.
func (mb *MetricsBuilder) RecordSparkJobSkippedTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, jobIDAttributeValue int64) {
	mb.metricSparkJobSkippedTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, jobIDAttributeValue)
}

// RecordSparkStageActiveTasksDataPoint adds a data point to spark.stage.active_tasks metric.
func (mb *MetricsBuilder) RecordSparkStageActiveTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageActiveTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageCompleteTasksDataPoint adds a data point to spark.stage.complete_tasks metric.
func (mb *MetricsBuilder) RecordSparkStageCompleteTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageCompleteTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageDiskSpaceSpilledDataPoint adds a data point to spark.stage.disk_space_spilled metric.
func (mb *MetricsBuilder) RecordSparkStageDiskSpaceSpilledDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageDiskSpaceSpilled.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageExecutorCPUTimeDataPoint adds a data point to spark.stage.executor_cpu_time metric.
func (mb *MetricsBuilder) RecordSparkStageExecutorCPUTimeDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageExecutorCPUTime.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageExecutorRunTimeDataPoint adds a data point to spark.stage.executor_run_time metric.
func (mb *MetricsBuilder) RecordSparkStageExecutorRunTimeDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageExecutorRunTime.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageFailedTasksDataPoint adds a data point to spark.stage.failed_tasks metric.
func (mb *MetricsBuilder) RecordSparkStageFailedTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageFailedTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageInputBytesDataPoint adds a data point to spark.stage.input_bytes metric.
func (mb *MetricsBuilder) RecordSparkStageInputBytesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageInputBytes.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageInputRecordsDataPoint adds a data point to spark.stage.input_records metric.
func (mb *MetricsBuilder) RecordSparkStageInputRecordsDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageInputRecords.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageJvmGcTimeDataPoint adds a data point to spark.stage.jvm_gc_time metric.
func (mb *MetricsBuilder) RecordSparkStageJvmGcTimeDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageJvmGcTime.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageKilledTasksDataPoint adds a data point to spark.stage.killed_tasks metric.
func (mb *MetricsBuilder) RecordSparkStageKilledTasksDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageKilledTasks.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageMemorySpilledDataPoint adds a data point to spark.stage.memory_spilled metric.
func (mb *MetricsBuilder) RecordSparkStageMemorySpilledDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageMemorySpilled.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageOutputBytesDataPoint adds a data point to spark.stage.output_bytes metric.
func (mb *MetricsBuilder) RecordSparkStageOutputBytesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageOutputBytes.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageOutputRecordsDataPoint adds a data point to spark.stage.output_records metric.
func (mb *MetricsBuilder) RecordSparkStageOutputRecordsDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageOutputRecords.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStagePeakExecutionMemoryDataPoint adds a data point to spark.stage.peak_execution_memory metric.
func (mb *MetricsBuilder) RecordSparkStagePeakExecutionMemoryDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStagePeakExecutionMemory.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageResultSizeDataPoint adds a data point to spark.stage.result_size metric.
func (mb *MetricsBuilder) RecordSparkStageResultSizeDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageResultSize.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageShuffleBlocksFetchedDataPoint adds a data point to spark.stage.shuffle.blocks_fetched metric.
func (mb *MetricsBuilder) RecordSparkStageShuffleBlocksFetchedDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus, sourceAttributeValue AttributeSource) {
	mb.metricSparkStageShuffleBlocksFetched.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String(), sourceAttributeValue.String())
}

// RecordSparkStageShuffleBytesReadDataPoint adds a data point to spark.stage.shuffle.bytes_read metric.
func (mb *MetricsBuilder) RecordSparkStageShuffleBytesReadDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus, sourceAttributeValue AttributeSource) {
	mb.metricSparkStageShuffleBytesRead.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String(), sourceAttributeValue.String())
}

// RecordSparkStageShuffleFetchWaitTimeDataPoint adds a data point to spark.stage.shuffle.fetch_wait_time metric.
func (mb *MetricsBuilder) RecordSparkStageShuffleFetchWaitTimeDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageShuffleFetchWaitTime.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageShuffleReadBytesDataPoint adds a data point to spark.stage.shuffle.read_bytes metric.
func (mb *MetricsBuilder) RecordSparkStageShuffleReadBytesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageShuffleReadBytes.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageShuffleReadRecordsDataPoint adds a data point to spark.stage.shuffle.read_records metric.
func (mb *MetricsBuilder) RecordSparkStageShuffleReadRecordsDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageShuffleReadRecords.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageShuffleRemoteBytesReadToDiskDataPoint adds a data point to spark.stage.shuffle.remote_bytes_read_to_disk metric.
func (mb *MetricsBuilder) RecordSparkStageShuffleRemoteBytesReadToDiskDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageShuffleRemoteBytesReadToDisk.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageShuffleWriteBytesDataPoint adds a data point to spark.stage.shuffle.write_bytes metric.
func (mb *MetricsBuilder) RecordSparkStageShuffleWriteBytesDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageShuffleWriteBytes.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageShuffleWriteRecordsDataPoint adds a data point to spark.stage.shuffle.write_records metric.
func (mb *MetricsBuilder) RecordSparkStageShuffleWriteRecordsDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageShuffleWriteRecords.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// RecordSparkStageShuffleWriteTimeDataPoint adds a data point to spark.stage.shuffle.write_time metric.
func (mb *MetricsBuilder) RecordSparkStageShuffleWriteTimeDataPoint(ts pcommon.Timestamp, val int64, applicationIDAttributeValue string, applicationNameAttributeValue string, stageIDAttributeValue int64, attemptIDAttributeValue int64, stageStatusAttributeValue AttributeStageStatus) {
	mb.metricSparkStageShuffleWriteTime.recordDataPoint(mb.startTime, ts, val, applicationIDAttributeValue, applicationNameAttributeValue, stageIDAttributeValue, attemptIDAttributeValue, stageStatusAttributeValue.String())
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}
