type: apachespark

status:
  class: receiver
  stability: 
    development: [metrics]
  distributions: [contrib]

resource_attributes:
  spark.application.id:
    description: The ID of the application for which the metric was recorded.
    type: string
    enabled: true
  spark.application.name:
    description: The name of the application for which the metric was recorded.
    type: string
    enabled: true
  spark.stage.id:
    description: The ID of the application stage for which the metric was recorded.
    type: int
    enabled: true
  spark.executor.id:
    description: The ID of the executor for which the metric was recorded.
    type: string
    enabled: true
  spark.job.id:
    description: The ID of the job for which the metric was recorded.
    type: int
    enabled: true

attributes:
  attempt_id:
    description: The ID of the stage attempt for which the metric was recorded.
    type: int
  stage_active:
    description: Whether the stage for which the metric was recorded is active.
    type: boolean
  stage_complete:
    description: Whether the stage for which the metric was recorded is complete.
    type: boolean
  stage_pending:
    description: Whether the stage for which the metric was recorded is pending.
    type: boolean
  stage_failed:
    description: Whether the stage for which the metric was recorded is failed.
    type: boolean
  stage_task_result:
    description: The result of the stage tasks for which the metric was recorded.
    type: string
    enum:
      - completed
      - failed
      - killed
  executor_task_result:
    description: The result of the executor tasks for which the metric was recorded.
    type: string
    enum:
      - completed
      - failed
  job_task_result:
    description: The result of the job tasks for which the metric was recorded.
    type: string
    enum:
      - completed
      - failed
      - skipped
  job_stage_result:
    description: The result of the job stages for which the metric was recorded.
    type: string
    enum:
      - completed
      - failed
      - skipped
  direction:
    description: Whether the metric is in regards to input or output operations.
    type: string
    enum:
      - in
      - out
  source:
    description: The source from which data was fetched for the metric.
    type: string
    enum:
      - local
      - remote
  location:
    description: The location of the memory which is quantified in the metric.
    type: string
    enum:
      - on_heap
      - off_heap
  pool_memory_type:
    description: The type of pool memory for which the metric was recorded.
    type: string
    enum:
      - direct
      - mapped
  gc_type:
    description: The type of the garbage collection performed for the metric.
    type: string
    enum:
      - major
      - minor

metrics:
  #stage
  spark.stage.task.active:
    description: Number of active tasks in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: "{ tasks }"
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed]
  spark.stage.task.results:
    description: Number of tasks with a specific result in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ tasks }"
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed, stage_task_result]
  spark.stage.executor.run_time:
    description: Amount of time spent by the executor in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: ms
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed]
  spark.stage.executor.cpu_time:
    description: CPU time spent by the executor in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: ns
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed]
  spark.stage.task.result_size:
    description: The amount of data transmitted back to the driver by all the tasks in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: bytes
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed]
  spark.stage.jvm_gc_time:
    description: The amount of time the JVM spent on garbage collection in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: ms
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed]
  spark.stage.memory_spilled:
    description: The amount of memory moved to disk due to size constraints (spilled) in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: bytes
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed]
  spark.stage.disk_space_spilled:
    description: The amount of disk space used for storing portions of overly large data chunks that couldnâ€™t fit in memory in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: bytes
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed]
  spark.stage.peak_execution_memory:
    description: Peak memory used by internal data structures created during shuffles, aggregations and joins in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: bytes
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed]
  spark.stage.io.size:
    description: Amount of data written and read at this stage.
    enabled: true
    sum: 
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: bytes
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed, direction]
  spark.stage.io.records:
    description: Number of records written and read in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ records }"
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed, direction]
  spark.stage.shuffle.blocks_fetched:
    description: Number of blocks fetched in shuffle operations in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ blocks }"
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed, source]
  spark.stage.shuffle.fetch_wait_time:
    description: Time spent in this stage waiting for remote shuffle blocks.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: ms
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed]
  spark.stage.shuffle.remote_data_read_to_disk:
    description: Amount of data read to disk in shuffle operations (sometimes required for large blocks, as opposed to the default behavior of reading into memory).
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: bytes
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed]
  spark.stage.shuffle.io.size:
    description: Amount of data written or read in shuffle operations in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: bytes
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed, source, direction]
  spark.stage.shuffle.io.records:
    description: Number of records written or read in shuffle operations in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ records }"
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed, direction]
  spark.stage.shuffle.write_time:
    description: Time spent blocking on writes to disk or buffer cache in this stage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: ns
    attributes:
      [attempt_id, stage_active, stage_complete, stage_pending, stage_failed]
  #executor
  spark.executor.memory_used:
    description: Storage memory used by this executor.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: bytes
    attributes: []
  spark.executor.disk_used:
    description: Disk space used by this executor for RDD storage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: bytes
    attributes: []
  spark.executor.tasks.max:
    description: Maximum number of tasks that can run concurrently in this executor.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: "{ tasks }"
    attributes: []
  spark.executor.tasks.active:
    description: Number of tasks currently running in this executor.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: "{ tasks }"
    attributes: []
  spark.executor.tasks.results:
    description: Number of tasks with a specific result in this executor.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ tasks }"
    attributes: [executor_task_result]
  spark.executor.duration:
    description: Elapsed time the JVM spent executing tasks in this executor.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: ms
    attributes: []
  spark.executor.gc_time:
    description: Elapsed time the JVM spent in garbage collection in this executor.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: ms
    attributes: []
  spark.executor.input_size:
    description: Amount of data input for this executor.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: bytes
    attributes: []
  spark.executor.shuffle.io.size:
    description: Amount of data written and read during shuffle operations for this executor.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: bytes
    attributes: [direction]
  spark.executor.storage_memory.used:
    description: Amount of memory currently used for storage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: bytes
    attributes: [location]
  spark.executor.storage_memory.total:
    description: Total memory that can be used for storage.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: bytes
    attributes: [location]
  #job
  spark.job.tasks.active:
    description: Number of active tasks in this job.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: "{ tasks }"
    attributes: []
  spark.job.tasks.results:
    description: Number of tasks with a specific result in this job.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ tasks }"
    attributes: [job_task_result]
  spark.job.stages.active:
    description: Number of active stages in this job.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: "{ tasks }"
    attributes: []
  spark.job.stages.results:
    description: Number of stages with a specific result in this job.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ tasks }"
    attributes: [job_stage_result]
  # metrics
  spark.driver.block_manager.disk.space_used:
    description: Disk space used by the BlockManager.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: mb
    attributes: []
  spark.driver.block_manager.memory.used:
    description: Memory used by the BlockManager.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: mb
    attributes: [location]
  spark.driver.block_manager.memory.remaining:
    description: Memory remaining for the BlockManager.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: mb
    attributes: [location]
  spark.driver.hive_external_catalog.file_cache_hits:
    description: Number of file cache hits on the HiveExternalCatalog.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ hits }"
    attributes: []
  spark.driver.hive_external_catalog.files_discovered:
    description: Number of files discovered while listing the partitions of a table in the Hive metastore
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ files }"
    attributes: []
  spark.driver.hive_external_catalog.hive_client_calls:
    description: Number of calls to the underlying Hive Metastore client made by the Spark application.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ calls }"
    attributes: []
  spark.driver.hive_external_catalog.parallel_listing_jobs:
    description: Number of parallel listing jobs initiated by the HiveExternalCatalog when listing partitions of a table.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ listing_jobs }"
    attributes: []
  spark.driver.hive_external_catalog.partitions_fetched:
    description: Table partitions fetched by the HiveExternalCatalog.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ partitions }"
    attributes: []
  spark.driver.code_generator.compilation.count:
    description: Number of source code compilation operations performed by the CodeGenerator.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ compilations }"
    attributes: []
  spark.driver.code_generator.compilation.average_time:
    description: Average time spent during CodeGenerator source code compilation operations.
    enabled: true
    gauge:
      value_type: double
    unit: ms
    attributes: []
  spark.driver.code_generator.generated_class.count:
    description: Number of classes generated by the CodeGenerator.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ classes }"
    attributes: []
  spark.driver.code_generator.generated_class.average_size:
    description: Average class size of the classes generated by the CodeGenerator.
    enabled: true
    gauge:
      value_type: double
    unit: bytes
    attributes: []
  spark.driver.code_generator.generated_method.count:
    description: Number of methods generated by the CodeGenerator.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ methods }"
    attributes: []
  spark.driver.code_generator.generated_method.average_size:
    description: Average method size of the classes generated by the CodeGenerator.
    enabled: true
    gauge:
      value_type: double
    unit: bytes
    attributes: []
  spark.driver.code_generator.source_code.count:
    description: Number of source code generation operations performed by the CodeGenerator.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ operations }"
    attributes: []
  spark.driver.code_generator.source_code.average_size:
    description: Average size of the source code generated by a CodeGenerator code generation operation.
    enabled: true
    gauge:
      value_type: double
    unit: bytes
    attributes: []
  spark.driver.dag_scheduler.jobs.active:
    description: Number of active jobs currently being processed by the DAGScheduler.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: "{ jobs }"
    attributes: []
  spark.driver.dag_scheduler.jobs.all:
    description: Number of jobs that have been submitted to the DAGScheduler.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ jobs }"
    attributes: []
  spark.driver.dag_scheduler.stages.failed:
    description: Number of failed stages run by the DAGScheduler.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ stages }"
    attributes: []
  spark.driver.dag_scheduler.stages.running:
    description: Number of stages the DAGScheduler is currently running.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: "{ stages }"
    attributes: []
  spark.driver.dag_scheduler.stages.waiting:
    description: Number of stages waiting to be run by the DAGScheduler.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: "{ stages }"
    attributes: []
  spark.driver.live_listener_bus.events_posted:
    description: Number of events that have been posted on the LiveListenerBus.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ events }"
    attributes: []
  spark.driver.live_listener_bus.listener_processing_time.average:
    description: Average time taken for the LiveListenerBus to process an event posted to it.
    enabled: true
    gauge:
      value_type: double
    unit: ms
    attributes: []
  spark.driver.live_listener_bus.events_dropped:
    description: Number of events that have been dropped by the LiveListenerBus.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ events }"
    attributes: []
  spark.driver.live_listener_bus.queue_size:
    description: Number of events currently waiting to be processed by the LiveListenerBus.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: "{ events }"
    attributes: []
  spark.driver.jvm_cpu_time:
    description: Current CPU time taken by the Spark driver.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: ns
    attributes: []
  spark.driver.executor_metrics.jvm_memory:
    description: Amount of memory used by the driver JVM.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: bytes
    attributes: [location]
  spark.driver.executor_metrics.memory.execution:
    description: Amount of execution memory currently used by the driver.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: bytes
    attributes: [location]
  spark.driver.executor_metrics.memory.storage:
    description: Amount of storage memory currently used by the driver.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: bytes
    attributes: [location]
  spark.driver.executor_metrics.memory.pool:
    description: Amount of pool memory currently used by the driver.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: false
      value_type: int
    unit: bytes
    attributes: [pool_memory_type]
  spark.driver.executor_metrics.gc.count:
    description: Number of garbage collection operations performed.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: "{ gc_operations }"
    attributes: [gc_type]
  spark.driver.executor_metrics.gc.time:
    description: Total elapsed time during garbage collection operations.
    enabled: true
    sum:
      aggregation: cumulative
      monotonic: true
      value_type: int
    unit: ms
    attributes: [gc_type]
