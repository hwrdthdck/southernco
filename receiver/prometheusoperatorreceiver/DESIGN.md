## Design Goals

### Vision
Provide a nearly zero-config migration path from PrometheusOperator defined scraping configs.  

### Scope
Only scrape config relevant CRDs (CustomResourceDefinition) are in scope of this receiver.    
As of this writing the relevant CRDs are: 
- ServiceMonitor
- PodMonitor

The receiver should be able to query this CRDs from Kubernetes, attach watchers and reconfigure its scraping config 
based on existing CRs (CustomResource) in Kubernetes.

Changes as well as creations or deletions of said CRs should trigger reconciliation of the scrape config. 
Active querying of the API should only occur during startup and in a defined interval of the receiver. 

Namespace limitations should be possible for shared cluster scenarios.     
This should be implemented using industry standard methods. 


### Out of scope
Excluded are other relevant CRDs such as: 
- PrometheusRules
- ThanosRuler
- Probes
- Alertmanagers
- AlertmanagerConfigs

This includes concepts like alerting and black box probing of defined targets. 


## PrometheusOperator CR watcher
The CR watcher is the component responsible for querying CRs from Kubernetes API and triggering a reconciliation.

### Major components of CR watcher
- **[ListenerFactory](https://github.com/prometheus-operator/prometheus-operator/blob/main/pkg/informers/monitoring.go):**
  the component which creates listeners on the CR
- **[APIClient](https://github.com/prometheus-operator/prometheus-operator/tree/main/pkg/client):**
  PrometheusOperator API client

## Config generator
The config generator is triggered by the watcher and generates a Prometheus config as a byte array.   
Instead of writing it onto a disk the configuration is unmarshalled using the Prometheus config loader, 
which is already in use by the `prometheusreceiver` resulting in a full Prometheus config. 

In case of an [Agent](#Collector vs Agent deployment) deployment, which is signaled with the `limit_to_node` option, 
only local endpoints will be fetched, endpoints should be filtered so that only pods are scraped which are scheduled 
on the current node.
To achieve this, additional relabeling rules are being added to all scrape configs generated by the `ConfigGenerator`

Here an example of a scrape config with the additional rules:
```yaml
scrape_configs:
- job_name: serviceMonitor/ingress/ingress-nginx-controller/0
  honor_timestamps: true
  scrape_interval: 30s
  scrape_timeout: 10s
  metrics_path: /metrics
  scheme: http
  follow_redirects: true
  relabel_configs: # shortened
### the autogenerated rules have been removed 
    # save in case node ports
  - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]
    regex: "Node;(.+)"
    replacement: $1
    target_label: __tmp_node_name
    action: replace
    # save in case pods
  - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_node_name]
    regex: "Pod;(.+)"
    replacement: $1
    target_label: __tmp_node_name
    action: replace
    # keep only targets on the defined node
  - source_labels: [__tmp_node_name]
    regex: "our-node-name" # node name we have extracted from the environment
    action: keep
  kubernetes_sd_configs:
  - role: endpoints
    kubeconfig_file: ""
    follow_redirects: true
    namespaces:
      names:
      - ingress
```

This config is then used to create a `prometheusreceiver` `Config` struct.  

### Major components of the config generation
- **[ConfigGenerator](https://github.com/prometheus-operator/prometheus-operator/blob/main/pkg/prometheus/promcfg.go#L304):**
  PrometheusOperator component which generates a valid Prometheus config marshalled to a byte array
- **[ConfigLoader](https://github.com/prometheus/prometheus/blob/main/config/config.go#L68):**
  Prometheus configuration loader which unmarshalls the config to a `prometheusreceiver` usable object 


## Processing config change events
The Prometheus config is at first compared against the currently applied one. Should there be any change, a new 
`prometheusreceiver` is started using the generated configuration. 
If the startup is successful the old instance of `prometheusreceiver` is shutdown. 
Should any error occur during startup the old instance will keep running

## Collector vs Agent deployment
The receiver should support both reference architectures of the collector. 

### Collector
If running as collector the Prometheus config provided by PrometheusOperator can be reused without a change.    
Should multiple instances with the same config run in the same cluster, they will act like a 
high availability pair of Prometheus. Therefore, all targets will be scraped multiple times and telemetry 
will have to deduplicated/compacted later on. 

```yaml
receivers:
  prometheus_operator:
    namespaces: []
    monitor_selector:
      match_labels:
        prometheus-operator-instance: a-instance
```

### Agent
In this case the collector is deployed as agent. The receiver can be limited to workloads on a single node
using the `limit_to_node` option and adding the node name as an environment variable.

```yaml
env:
  - name: K8S_NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName
```

```yaml
receivers:
  prometheus_operator:
    namespaces: []
    limit_to_node: ${K8S_NODE_NAME}
    monitor_selector:
      match_labels:
        prometheus-operator-instance: a-instance
```
