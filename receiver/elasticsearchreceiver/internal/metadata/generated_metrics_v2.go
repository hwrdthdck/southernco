// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
)

// MetricSettings provides common settings for a particular metric.
type MetricSettings struct {
	Enabled bool `mapstructure:"enabled"`
}

// MetricsSettings provides settings for elasticsearchreceiver metrics.
type MetricsSettings struct {
	ElasticsearchBreakerMemoryEstimated                        MetricSettings `mapstructure:"elasticsearch.breaker.memory.estimated"`
	ElasticsearchBreakerMemoryLimit                            MetricSettings `mapstructure:"elasticsearch.breaker.memory.limit"`
	ElasticsearchBreakerTripped                                MetricSettings `mapstructure:"elasticsearch.breaker.tripped"`
	ElasticsearchClusterDataNodes                              MetricSettings `mapstructure:"elasticsearch.cluster.data_nodes"`
	ElasticsearchClusterHealth                                 MetricSettings `mapstructure:"elasticsearch.cluster.health"`
	ElasticsearchClusterNodes                                  MetricSettings `mapstructure:"elasticsearch.cluster.nodes"`
	ElasticsearchClusterShards                                 MetricSettings `mapstructure:"elasticsearch.cluster.shards"`
	ElasticsearchIndexingPressureMemoryCoordinating            MetricSettings `mapstructure:"elasticsearch.indexing_pressure.memory.coordinating"`
	ElasticsearchIndexingPressureMemoryLimit                   MetricSettings `mapstructure:"elasticsearch.indexing_pressure.memory.limit"`
	ElasticsearchIndexingPressureMemoryPrimary                 MetricSettings `mapstructure:"elasticsearch.indexing_pressure.memory.primary"`
	ElasticsearchIndexingPressureMemoryReplica                 MetricSettings `mapstructure:"elasticsearch.indexing_pressure.memory.replica"`
	ElasticsearchIndexingPressureMemoryTotalPrimaryRejections  MetricSettings `mapstructure:"elasticsearch.indexing_pressure.memory.total.primary_rejections"`
	ElasticsearchIndexingPressureMemoryTotalReplicaRejections  MetricSettings `mapstructure:"elasticsearch.indexing_pressure.memory.total.replica_rejections"`
	ElasticsearchNodeCacheEvictions                            MetricSettings `mapstructure:"elasticsearch.node.cache.evictions"`
	ElasticsearchNodeCacheMemoryUsage                          MetricSettings `mapstructure:"elasticsearch.node.cache.memory.usage"`
	ElasticsearchNodeClusterConnections                        MetricSettings `mapstructure:"elasticsearch.node.cluster.connections"`
	ElasticsearchNodeClusterIo                                 MetricSettings `mapstructure:"elasticsearch.node.cluster.io"`
	ElasticsearchNodeClusterPublishedStatesCompatible          MetricSettings `mapstructure:"elasticsearch.node.cluster.published_states.compatible"`
	ElasticsearchNodeClusterPublishedStatesFull                MetricSettings `mapstructure:"elasticsearch.node.cluster.published_states.full"`
	ElasticsearchNodeClusterPublishedStatesIncompatible        MetricSettings `mapstructure:"elasticsearch.node.cluster.published_states.incompatible"`
	ElasticsearchNodeClusterStateQueueCommitted                MetricSettings `mapstructure:"elasticsearch.node.cluster.state_queue.committed"`
	ElasticsearchNodeClusterStateQueuePending                  MetricSettings `mapstructure:"elasticsearch.node.cluster.state_queue.pending"`
	ElasticsearchNodeClusterStateUpdateCommitTime              MetricSettings `mapstructure:"elasticsearch.node.cluster.state_update.commit_time"`
	ElasticsearchNodeClusterStateUpdateCompletionTime          MetricSettings `mapstructure:"elasticsearch.node.cluster.state_update.completion_time"`
	ElasticsearchNodeClusterStateUpdateComputationTime         MetricSettings `mapstructure:"elasticsearch.node.cluster.state_update.computation_time"`
	ElasticsearchNodeClusterStateUpdateContextConstructionTime MetricSettings `mapstructure:"elasticsearch.node.cluster.state_update.context_construction_time"`
	ElasticsearchNodeClusterStateUpdateCount                   MetricSettings `mapstructure:"elasticsearch.node.cluster.state_update.count"`
	ElasticsearchNodeClusterStateUpdateMasterApplyTime         MetricSettings `mapstructure:"elasticsearch.node.cluster.state_update.master_apply_time"`
	ElasticsearchNodeClusterStateUpdateNotificationTime        MetricSettings `mapstructure:"elasticsearch.node.cluster.state_update.notification_time"`
	ElasticsearchNodeDiskIoRead                                MetricSettings `mapstructure:"elasticsearch.node.disk.io.read"`
	ElasticsearchNodeDiskIoWrite                               MetricSettings `mapstructure:"elasticsearch.node.disk.io.write"`
	ElasticsearchNodeDocuments                                 MetricSettings `mapstructure:"elasticsearch.node.documents"`
	ElasticsearchNodeFsDiskAvailable                           MetricSettings `mapstructure:"elasticsearch.node.fs.disk.available"`
	ElasticsearchNodeHTTPConnections                           MetricSettings `mapstructure:"elasticsearch.node.http.connections"`
	ElasticsearchNodeIngestCount                               MetricSettings `mapstructure:"elasticsearch.node.ingest.count"`
	ElasticsearchNodeIngestCurrent                             MetricSettings `mapstructure:"elasticsearch.node.ingest.current"`
	ElasticsearchNodeIngestFailed                              MetricSettings `mapstructure:"elasticsearch.node.ingest.failed"`
	ElasticsearchNodeIngestPipelineCount                       MetricSettings `mapstructure:"elasticsearch.node.ingest.pipeline.count"`
	ElasticsearchNodeIngestPipelineCurrent                     MetricSettings `mapstructure:"elasticsearch.node.ingest.pipeline.current"`
	ElasticsearchNodeIngestPipelineFailed                      MetricSettings `mapstructure:"elasticsearch.node.ingest.pipeline.failed"`
	ElasticsearchNodeOpenFiles                                 MetricSettings `mapstructure:"elasticsearch.node.open_files"`
	ElasticsearchNodeOperationsCompleted                       MetricSettings `mapstructure:"elasticsearch.node.operations.completed"`
	ElasticsearchNodeOperationsTime                            MetricSettings `mapstructure:"elasticsearch.node.operations.time"`
	ElasticsearchNodeScriptCacheEvictions                      MetricSettings `mapstructure:"elasticsearch.node.script.cache_evictions"`
	ElasticsearchNodeScriptCompilationLimitTriggered           MetricSettings `mapstructure:"elasticsearch.node.script.compilation_limit_triggered"`
	ElasticsearchNodeScriptCompilations                        MetricSettings `mapstructure:"elasticsearch.node.script.compilations"`
	ElasticsearchNodeShardsDataSetSize                         MetricSettings `mapstructure:"elasticsearch.node.shards.data_set.size"`
	ElasticsearchNodeShardsReservedSize                        MetricSettings `mapstructure:"elasticsearch.node.shards.reserved.size"`
	ElasticsearchNodeShardsSize                                MetricSettings `mapstructure:"elasticsearch.node.shards.size"`
	ElasticsearchNodeThreadPoolTasksFinished                   MetricSettings `mapstructure:"elasticsearch.node.thread_pool.tasks.finished"`
	ElasticsearchNodeThreadPoolTasksQueued                     MetricSettings `mapstructure:"elasticsearch.node.thread_pool.tasks.queued"`
	ElasticsearchNodeThreadPoolThreads                         MetricSettings `mapstructure:"elasticsearch.node.thread_pool.threads"`
	ElasticsearchNodeTranslogOperations                        MetricSettings `mapstructure:"elasticsearch.node.translog.operations"`
	ElasticsearchNodeTranslogSize                              MetricSettings `mapstructure:"elasticsearch.node.translog.size"`
	ElasticsearchNodeTranslogUncommittedSize                   MetricSettings `mapstructure:"elasticsearch.node.translog.uncommitted.size"`
	ElasticsearchOsCPULoadAvg15m                               MetricSettings `mapstructure:"elasticsearch.os.cpu.load_avg.15m"`
	ElasticsearchOsCPULoadAvg1m                                MetricSettings `mapstructure:"elasticsearch.os.cpu.load_avg.1m"`
	ElasticsearchOsCPULoadAvg5m                                MetricSettings `mapstructure:"elasticsearch.os.cpu.load_avg.5m"`
	ElasticsearchOsCPUUsage                                    MetricSettings `mapstructure:"elasticsearch.os.cpu.usage"`
	ElasticsearchOsMemory                                      MetricSettings `mapstructure:"elasticsearch.os.memory"`
	JvmClassesLoaded                                           MetricSettings `mapstructure:"jvm.classes.loaded"`
	JvmGcCollectionsCount                                      MetricSettings `mapstructure:"jvm.gc.collections.count"`
	JvmGcCollectionsElapsed                                    MetricSettings `mapstructure:"jvm.gc.collections.elapsed"`
	JvmMemoryHeapCommitted                                     MetricSettings `mapstructure:"jvm.memory.heap.committed"`
	JvmMemoryHeapMax                                           MetricSettings `mapstructure:"jvm.memory.heap.max"`
	JvmMemoryHeapUsed                                          MetricSettings `mapstructure:"jvm.memory.heap.used"`
	JvmMemoryNonheapCommitted                                  MetricSettings `mapstructure:"jvm.memory.nonheap.committed"`
	JvmMemoryNonheapUsed                                       MetricSettings `mapstructure:"jvm.memory.nonheap.used"`
	JvmMemoryPoolMax                                           MetricSettings `mapstructure:"jvm.memory.pool.max"`
	JvmMemoryPoolUsed                                          MetricSettings `mapstructure:"jvm.memory.pool.used"`
	JvmThreadsCount                                            MetricSettings `mapstructure:"jvm.threads.count"`
}

func DefaultMetricsSettings() MetricsSettings {
	return MetricsSettings{
		ElasticsearchBreakerMemoryEstimated: MetricSettings{
			Enabled: true,
		},
		ElasticsearchBreakerMemoryLimit: MetricSettings{
			Enabled: true,
		},
		ElasticsearchBreakerTripped: MetricSettings{
			Enabled: true,
		},
		ElasticsearchClusterDataNodes: MetricSettings{
			Enabled: true,
		},
		ElasticsearchClusterHealth: MetricSettings{
			Enabled: true,
		},
		ElasticsearchClusterNodes: MetricSettings{
			Enabled: true,
		},
		ElasticsearchClusterShards: MetricSettings{
			Enabled: true,
		},
		ElasticsearchIndexingPressureMemoryCoordinating: MetricSettings{
			Enabled: true,
		},
		ElasticsearchIndexingPressureMemoryLimit: MetricSettings{
			Enabled: true,
		},
		ElasticsearchIndexingPressureMemoryPrimary: MetricSettings{
			Enabled: true,
		},
		ElasticsearchIndexingPressureMemoryReplica: MetricSettings{
			Enabled: true,
		},
		ElasticsearchIndexingPressureMemoryTotalPrimaryRejections: MetricSettings{
			Enabled: true,
		},
		ElasticsearchIndexingPressureMemoryTotalReplicaRejections: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeCacheEvictions: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeCacheMemoryUsage: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterConnections: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterIo: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterPublishedStatesCompatible: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterPublishedStatesFull: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterPublishedStatesIncompatible: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterStateQueueCommitted: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterStateQueuePending: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterStateUpdateCommitTime: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterStateUpdateCompletionTime: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterStateUpdateComputationTime: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterStateUpdateContextConstructionTime: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterStateUpdateCount: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterStateUpdateMasterApplyTime: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterStateUpdateNotificationTime: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeDiskIoRead: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeDiskIoWrite: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeDocuments: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeFsDiskAvailable: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeHTTPConnections: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeIngestCount: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeIngestCurrent: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeIngestFailed: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeIngestPipelineCount: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeIngestPipelineCurrent: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeIngestPipelineFailed: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeOpenFiles: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeOperationsCompleted: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeOperationsTime: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeScriptCacheEvictions: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeScriptCompilationLimitTriggered: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeScriptCompilations: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeShardsDataSetSize: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeShardsReservedSize: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeShardsSize: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeThreadPoolTasksFinished: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeThreadPoolTasksQueued: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeThreadPoolThreads: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeTranslogOperations: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeTranslogSize: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeTranslogUncommittedSize: MetricSettings{
			Enabled: true,
		},
		ElasticsearchOsCPULoadAvg15m: MetricSettings{
			Enabled: true,
		},
		ElasticsearchOsCPULoadAvg1m: MetricSettings{
			Enabled: true,
		},
		ElasticsearchOsCPULoadAvg5m: MetricSettings{
			Enabled: true,
		},
		ElasticsearchOsCPUUsage: MetricSettings{
			Enabled: true,
		},
		ElasticsearchOsMemory: MetricSettings{
			Enabled: true,
		},
		JvmClassesLoaded: MetricSettings{
			Enabled: true,
		},
		JvmGcCollectionsCount: MetricSettings{
			Enabled: true,
		},
		JvmGcCollectionsElapsed: MetricSettings{
			Enabled: true,
		},
		JvmMemoryHeapCommitted: MetricSettings{
			Enabled: true,
		},
		JvmMemoryHeapMax: MetricSettings{
			Enabled: true,
		},
		JvmMemoryHeapUsed: MetricSettings{
			Enabled: true,
		},
		JvmMemoryNonheapCommitted: MetricSettings{
			Enabled: true,
		},
		JvmMemoryNonheapUsed: MetricSettings{
			Enabled: true,
		},
		JvmMemoryPoolMax: MetricSettings{
			Enabled: true,
		},
		JvmMemoryPoolUsed: MetricSettings{
			Enabled: true,
		},
		JvmThreadsCount: MetricSettings{
			Enabled: true,
		},
	}
}

// AttributeCacheName specifies the a value cache_name attribute.
type AttributeCacheName int

const (
	_ AttributeCacheName = iota
	AttributeCacheNameFielddata
	AttributeCacheNameQuery
)

// String returns the string representation of the AttributeCacheName.
func (av AttributeCacheName) String() string {
	switch av {
	case AttributeCacheNameFielddata:
		return "fielddata"
	case AttributeCacheNameQuery:
		return "query"
	}
	return ""
}

// MapAttributeCacheName is a helper map of string to AttributeCacheName attribute value.
var MapAttributeCacheName = map[string]AttributeCacheName{
	"fielddata": AttributeCacheNameFielddata,
	"query":     AttributeCacheNameQuery,
}

// AttributeClusterStateUpdateType specifies the a value cluster_state_update_type attribute.
type AttributeClusterStateUpdateType int

const (
	_ AttributeClusterStateUpdateType = iota
	AttributeClusterStateUpdateTypeUnchanged
	AttributeClusterStateUpdateTypeSuccess
	AttributeClusterStateUpdateTypeFailure
)

// String returns the string representation of the AttributeClusterStateUpdateType.
func (av AttributeClusterStateUpdateType) String() string {
	switch av {
	case AttributeClusterStateUpdateTypeUnchanged:
		return "unchanged"
	case AttributeClusterStateUpdateTypeSuccess:
		return "success"
	case AttributeClusterStateUpdateTypeFailure:
		return "failure"
	}
	return ""
}

// MapAttributeClusterStateUpdateType is a helper map of string to AttributeClusterStateUpdateType attribute value.
var MapAttributeClusterStateUpdateType = map[string]AttributeClusterStateUpdateType{
	"unchanged": AttributeClusterStateUpdateTypeUnchanged,
	"success":   AttributeClusterStateUpdateTypeSuccess,
	"failure":   AttributeClusterStateUpdateTypeFailure,
}

// AttributeDirection specifies the a value direction attribute.
type AttributeDirection int

const (
	_ AttributeDirection = iota
	AttributeDirectionReceived
	AttributeDirectionSent
)

// String returns the string representation of the AttributeDirection.
func (av AttributeDirection) String() string {
	switch av {
	case AttributeDirectionReceived:
		return "received"
	case AttributeDirectionSent:
		return "sent"
	}
	return ""
}

// MapAttributeDirection is a helper map of string to AttributeDirection attribute value.
var MapAttributeDirection = map[string]AttributeDirection{
	"received": AttributeDirectionReceived,
	"sent":     AttributeDirectionSent,
}

// AttributeDocumentState specifies the a value document_state attribute.
type AttributeDocumentState int

const (
	_ AttributeDocumentState = iota
	AttributeDocumentStateActive
	AttributeDocumentStateDeleted
)

// String returns the string representation of the AttributeDocumentState.
func (av AttributeDocumentState) String() string {
	switch av {
	case AttributeDocumentStateActive:
		return "active"
	case AttributeDocumentStateDeleted:
		return "deleted"
	}
	return ""
}

// MapAttributeDocumentState is a helper map of string to AttributeDocumentState attribute value.
var MapAttributeDocumentState = map[string]AttributeDocumentState{
	"active":  AttributeDocumentStateActive,
	"deleted": AttributeDocumentStateDeleted,
}

// AttributeFsDirection specifies the a value fs_direction attribute.
type AttributeFsDirection int

const (
	_ AttributeFsDirection = iota
	AttributeFsDirectionRead
	AttributeFsDirectionWrite
)

// String returns the string representation of the AttributeFsDirection.
func (av AttributeFsDirection) String() string {
	switch av {
	case AttributeFsDirectionRead:
		return "read"
	case AttributeFsDirectionWrite:
		return "write"
	}
	return ""
}

// MapAttributeFsDirection is a helper map of string to AttributeFsDirection attribute value.
var MapAttributeFsDirection = map[string]AttributeFsDirection{
	"read":  AttributeFsDirectionRead,
	"write": AttributeFsDirectionWrite,
}

// AttributeHealthStatus specifies the a value health_status attribute.
type AttributeHealthStatus int

const (
	_ AttributeHealthStatus = iota
	AttributeHealthStatusGreen
	AttributeHealthStatusYellow
	AttributeHealthStatusRed
)

// String returns the string representation of the AttributeHealthStatus.
func (av AttributeHealthStatus) String() string {
	switch av {
	case AttributeHealthStatusGreen:
		return "green"
	case AttributeHealthStatusYellow:
		return "yellow"
	case AttributeHealthStatusRed:
		return "red"
	}
	return ""
}

// MapAttributeHealthStatus is a helper map of string to AttributeHealthStatus attribute value.
var MapAttributeHealthStatus = map[string]AttributeHealthStatus{
	"green":  AttributeHealthStatusGreen,
	"yellow": AttributeHealthStatusYellow,
	"red":    AttributeHealthStatusRed,
}

// AttributeIndexingMemoryState specifies the a value indexing_memory_state attribute.
type AttributeIndexingMemoryState int

const (
	_ AttributeIndexingMemoryState = iota
	AttributeIndexingMemoryStateCurrent
	AttributeIndexingMemoryStateTotal
)

// String returns the string representation of the AttributeIndexingMemoryState.
func (av AttributeIndexingMemoryState) String() string {
	switch av {
	case AttributeIndexingMemoryStateCurrent:
		return "current"
	case AttributeIndexingMemoryStateTotal:
		return "total"
	}
	return ""
}

// MapAttributeIndexingMemoryState is a helper map of string to AttributeIndexingMemoryState attribute value.
var MapAttributeIndexingMemoryState = map[string]AttributeIndexingMemoryState{
	"current": AttributeIndexingMemoryStateCurrent,
	"total":   AttributeIndexingMemoryStateTotal,
}

// AttributeMemoryState specifies the a value memory_state attribute.
type AttributeMemoryState int

const (
	_ AttributeMemoryState = iota
	AttributeMemoryStateFree
	AttributeMemoryStateUsed
)

// String returns the string representation of the AttributeMemoryState.
func (av AttributeMemoryState) String() string {
	switch av {
	case AttributeMemoryStateFree:
		return "free"
	case AttributeMemoryStateUsed:
		return "used"
	}
	return ""
}

// MapAttributeMemoryState is a helper map of string to AttributeMemoryState attribute value.
var MapAttributeMemoryState = map[string]AttributeMemoryState{
	"free": AttributeMemoryStateFree,
	"used": AttributeMemoryStateUsed,
}

// AttributeOperation specifies the a value operation attribute.
type AttributeOperation int

const (
	_ AttributeOperation = iota
	AttributeOperationIndex
	AttributeOperationDelete
	AttributeOperationGet
	AttributeOperationQuery
	AttributeOperationFetch
	AttributeOperationScroll
	AttributeOperationSuggest
	AttributeOperationMerge
	AttributeOperationRefresh
	AttributeOperationFlush
	AttributeOperationWarmer
)

// String returns the string representation of the AttributeOperation.
func (av AttributeOperation) String() string {
	switch av {
	case AttributeOperationIndex:
		return "index"
	case AttributeOperationDelete:
		return "delete"
	case AttributeOperationGet:
		return "get"
	case AttributeOperationQuery:
		return "query"
	case AttributeOperationFetch:
		return "fetch"
	case AttributeOperationScroll:
		return "scroll"
	case AttributeOperationSuggest:
		return "suggest"
	case AttributeOperationMerge:
		return "merge"
	case AttributeOperationRefresh:
		return "refresh"
	case AttributeOperationFlush:
		return "flush"
	case AttributeOperationWarmer:
		return "warmer"
	}
	return ""
}

// MapAttributeOperation is a helper map of string to AttributeOperation attribute value.
var MapAttributeOperation = map[string]AttributeOperation{
	"index":   AttributeOperationIndex,
	"delete":  AttributeOperationDelete,
	"get":     AttributeOperationGet,
	"query":   AttributeOperationQuery,
	"fetch":   AttributeOperationFetch,
	"scroll":  AttributeOperationScroll,
	"suggest": AttributeOperationSuggest,
	"merge":   AttributeOperationMerge,
	"refresh": AttributeOperationRefresh,
	"flush":   AttributeOperationFlush,
	"warmer":  AttributeOperationWarmer,
}

// AttributeShardState specifies the a value shard_state attribute.
type AttributeShardState int

const (
	_ AttributeShardState = iota
	AttributeShardStateActive
	AttributeShardStateRelocating
	AttributeShardStateInitializing
	AttributeShardStateUnassigned
)

// String returns the string representation of the AttributeShardState.
func (av AttributeShardState) String() string {
	switch av {
	case AttributeShardStateActive:
		return "active"
	case AttributeShardStateRelocating:
		return "relocating"
	case AttributeShardStateInitializing:
		return "initializing"
	case AttributeShardStateUnassigned:
		return "unassigned"
	}
	return ""
}

// MapAttributeShardState is a helper map of string to AttributeShardState attribute value.
var MapAttributeShardState = map[string]AttributeShardState{
	"active":       AttributeShardStateActive,
	"relocating":   AttributeShardStateRelocating,
	"initializing": AttributeShardStateInitializing,
	"unassigned":   AttributeShardStateUnassigned,
}

// AttributeTaskState specifies the a value task_state attribute.
type AttributeTaskState int

const (
	_ AttributeTaskState = iota
	AttributeTaskStateRejected
	AttributeTaskStateCompleted
)

// String returns the string representation of the AttributeTaskState.
func (av AttributeTaskState) String() string {
	switch av {
	case AttributeTaskStateRejected:
		return "rejected"
	case AttributeTaskStateCompleted:
		return "completed"
	}
	return ""
}

// MapAttributeTaskState is a helper map of string to AttributeTaskState attribute value.
var MapAttributeTaskState = map[string]AttributeTaskState{
	"rejected":  AttributeTaskStateRejected,
	"completed": AttributeTaskStateCompleted,
}

// AttributeThreadState specifies the a value thread_state attribute.
type AttributeThreadState int

const (
	_ AttributeThreadState = iota
	AttributeThreadStateActive
	AttributeThreadStateIdle
)

// String returns the string representation of the AttributeThreadState.
func (av AttributeThreadState) String() string {
	switch av {
	case AttributeThreadStateActive:
		return "active"
	case AttributeThreadStateIdle:
		return "idle"
	}
	return ""
}

// MapAttributeThreadState is a helper map of string to AttributeThreadState attribute value.
var MapAttributeThreadState = map[string]AttributeThreadState{
	"active": AttributeThreadStateActive,
	"idle":   AttributeThreadStateIdle,
}

type metricElasticsearchBreakerMemoryEstimated struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.breaker.memory.estimated metric with initial data.
func (m *metricElasticsearchBreakerMemoryEstimated) init() {
	m.data.SetName("elasticsearch.breaker.memory.estimated")
	m.data.SetDescription("Estimated memory used for the operation.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchBreakerMemoryEstimated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, circuitBreakerNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("name", circuitBreakerNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchBreakerMemoryEstimated) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchBreakerMemoryEstimated) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchBreakerMemoryEstimated(settings MetricSettings) metricElasticsearchBreakerMemoryEstimated {
	m := metricElasticsearchBreakerMemoryEstimated{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchBreakerMemoryLimit struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.breaker.memory.limit metric with initial data.
func (m *metricElasticsearchBreakerMemoryLimit) init() {
	m.data.SetName("elasticsearch.breaker.memory.limit")
	m.data.SetDescription("Memory limit for the circuit breaker.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchBreakerMemoryLimit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, circuitBreakerNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("name", circuitBreakerNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchBreakerMemoryLimit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchBreakerMemoryLimit) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchBreakerMemoryLimit(settings MetricSettings) metricElasticsearchBreakerMemoryLimit {
	m := metricElasticsearchBreakerMemoryLimit{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchBreakerTripped struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.breaker.tripped metric with initial data.
func (m *metricElasticsearchBreakerTripped) init() {
	m.data.SetName("elasticsearch.breaker.tripped")
	m.data.SetDescription("Total number of times the circuit breaker has been triggered and prevented an out of memory error.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchBreakerTripped) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, circuitBreakerNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("name", circuitBreakerNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchBreakerTripped) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchBreakerTripped) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchBreakerTripped(settings MetricSettings) metricElasticsearchBreakerTripped {
	m := metricElasticsearchBreakerTripped{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchClusterDataNodes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.cluster.data_nodes metric with initial data.
func (m *metricElasticsearchClusterDataNodes) init() {
	m.data.SetName("elasticsearch.cluster.data_nodes")
	m.data.SetDescription("The number of data nodes in the cluster.")
	m.data.SetUnit("{nodes}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchClusterDataNodes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchClusterDataNodes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchClusterDataNodes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchClusterDataNodes(settings MetricSettings) metricElasticsearchClusterDataNodes {
	m := metricElasticsearchClusterDataNodes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchClusterHealth struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.cluster.health metric with initial data.
func (m *metricElasticsearchClusterHealth) init() {
	m.data.SetName("elasticsearch.cluster.health")
	m.data.SetDescription("The health status of the cluster.")
	m.data.SetUnit("{status}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchClusterHealth) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, healthStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("status", healthStatusAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchClusterHealth) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchClusterHealth) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchClusterHealth(settings MetricSettings) metricElasticsearchClusterHealth {
	m := metricElasticsearchClusterHealth{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchClusterNodes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.cluster.nodes metric with initial data.
func (m *metricElasticsearchClusterNodes) init() {
	m.data.SetName("elasticsearch.cluster.nodes")
	m.data.SetDescription("The total number of nodes in the cluster.")
	m.data.SetUnit("{nodes}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchClusterNodes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchClusterNodes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchClusterNodes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchClusterNodes(settings MetricSettings) metricElasticsearchClusterNodes {
	m := metricElasticsearchClusterNodes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchClusterShards struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.cluster.shards metric with initial data.
func (m *metricElasticsearchClusterShards) init() {
	m.data.SetName("elasticsearch.cluster.shards")
	m.data.SetDescription("The number of shards in the cluster.")
	m.data.SetUnit("{shards}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchClusterShards) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, shardStateAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("state", shardStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchClusterShards) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchClusterShards) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchClusterShards(settings MetricSettings) metricElasticsearchClusterShards {
	m := metricElasticsearchClusterShards{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchIndexingPressureMemoryCoordinating struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.indexing_pressure.memory.coordinating metric with initial data.
func (m *metricElasticsearchIndexingPressureMemoryCoordinating) init() {
	m.data.SetName("elasticsearch.indexing_pressure.memory.coordinating")
	m.data.SetDescription("Memory consumed, in bytes, by indexing requests in the coordinating stage.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchIndexingPressureMemoryCoordinating) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchIndexingPressureMemoryCoordinating) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchIndexingPressureMemoryCoordinating) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchIndexingPressureMemoryCoordinating(settings MetricSettings) metricElasticsearchIndexingPressureMemoryCoordinating {
	m := metricElasticsearchIndexingPressureMemoryCoordinating{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchIndexingPressureMemoryLimit struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.indexing_pressure.memory.limit metric with initial data.
func (m *metricElasticsearchIndexingPressureMemoryLimit) init() {
	m.data.SetName("elasticsearch.indexing_pressure.memory.limit")
	m.data.SetDescription("Configured memory limit, in bytes, for the indexing requests.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchIndexingPressureMemoryLimit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchIndexingPressureMemoryLimit) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchIndexingPressureMemoryLimit) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchIndexingPressureMemoryLimit(settings MetricSettings) metricElasticsearchIndexingPressureMemoryLimit {
	m := metricElasticsearchIndexingPressureMemoryLimit{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchIndexingPressureMemoryPrimary struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.indexing_pressure.memory.primary metric with initial data.
func (m *metricElasticsearchIndexingPressureMemoryPrimary) init() {
	m.data.SetName("elasticsearch.indexing_pressure.memory.primary")
	m.data.SetDescription("Memory consumed, in bytes, by indexing requests in the primary stage.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchIndexingPressureMemoryPrimary) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchIndexingPressureMemoryPrimary) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchIndexingPressureMemoryPrimary) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchIndexingPressureMemoryPrimary(settings MetricSettings) metricElasticsearchIndexingPressureMemoryPrimary {
	m := metricElasticsearchIndexingPressureMemoryPrimary{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchIndexingPressureMemoryReplica struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.indexing_pressure.memory.replica metric with initial data.
func (m *metricElasticsearchIndexingPressureMemoryReplica) init() {
	m.data.SetName("elasticsearch.indexing_pressure.memory.replica")
	m.data.SetDescription("Memory consumed, in bytes, by indexing requests in the primary stage.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchIndexingPressureMemoryReplica) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchIndexingPressureMemoryReplica) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchIndexingPressureMemoryReplica) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchIndexingPressureMemoryReplica(settings MetricSettings) metricElasticsearchIndexingPressureMemoryReplica {
	m := metricElasticsearchIndexingPressureMemoryReplica{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.indexing_pressure.memory.total.primary_rejections metric with initial data.
func (m *metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections) init() {
	m.data.SetName("elasticsearch.indexing_pressure.memory.total.primary_rejections")
	m.data.SetDescription("Cumulative number of indexing requests rejected in the primary stage.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchIndexingPressureMemoryTotalPrimaryRejections(settings MetricSettings) metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections {
	m := metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchIndexingPressureMemoryTotalReplicaRejections struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.indexing_pressure.memory.total.replica_rejections metric with initial data.
func (m *metricElasticsearchIndexingPressureMemoryTotalReplicaRejections) init() {
	m.data.SetName("elasticsearch.indexing_pressure.memory.total.replica_rejections")
	m.data.SetDescription("Number of indexing requests rejected in the replica stage.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchIndexingPressureMemoryTotalReplicaRejections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchIndexingPressureMemoryTotalReplicaRejections) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchIndexingPressureMemoryTotalReplicaRejections) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchIndexingPressureMemoryTotalReplicaRejections(settings MetricSettings) metricElasticsearchIndexingPressureMemoryTotalReplicaRejections {
	m := metricElasticsearchIndexingPressureMemoryTotalReplicaRejections{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeCacheEvictions struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cache.evictions metric with initial data.
func (m *metricElasticsearchNodeCacheEvictions) init() {
	m.data.SetName("elasticsearch.node.cache.evictions")
	m.data.SetDescription("The number of evictions from the cache.")
	m.data.SetUnit("{evictions}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeCacheEvictions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, cacheNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("cache_name", cacheNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeCacheEvictions) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeCacheEvictions) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeCacheEvictions(settings MetricSettings) metricElasticsearchNodeCacheEvictions {
	m := metricElasticsearchNodeCacheEvictions{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeCacheMemoryUsage struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cache.memory.usage metric with initial data.
func (m *metricElasticsearchNodeCacheMemoryUsage) init() {
	m.data.SetName("elasticsearch.node.cache.memory.usage")
	m.data.SetDescription("The size in bytes of the cache.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeCacheMemoryUsage) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, cacheNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("cache_name", cacheNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeCacheMemoryUsage) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeCacheMemoryUsage) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeCacheMemoryUsage(settings MetricSettings) metricElasticsearchNodeCacheMemoryUsage {
	m := metricElasticsearchNodeCacheMemoryUsage{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.connections metric with initial data.
func (m *metricElasticsearchNodeClusterConnections) init() {
	m.data.SetName("elasticsearch.node.cluster.connections")
	m.data.SetDescription("The number of open tcp connections for internal cluster communication.")
	m.data.SetUnit("{connections}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeClusterConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterConnections) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterConnections) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterConnections(settings MetricSettings) metricElasticsearchNodeClusterConnections {
	m := metricElasticsearchNodeClusterConnections{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterIo struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.io metric with initial data.
func (m *metricElasticsearchNodeClusterIo) init() {
	m.data.SetName("elasticsearch.node.cluster.io")
	m.data.SetDescription("The number of bytes sent and received on the network for internal cluster communication.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeClusterIo) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, directionAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("direction", directionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterIo) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterIo) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterIo(settings MetricSettings) metricElasticsearchNodeClusterIo {
	m := metricElasticsearchNodeClusterIo{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterPublishedStatesCompatible struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.published_states.compatible metric with initial data.
func (m *metricElasticsearchNodeClusterPublishedStatesCompatible) init() {
	m.data.SetName("elasticsearch.node.cluster.published_states.compatible")
	m.data.SetDescription("Number of compatible differences between published cluster states.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchNodeClusterPublishedStatesCompatible) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterPublishedStatesCompatible) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterPublishedStatesCompatible) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterPublishedStatesCompatible(settings MetricSettings) metricElasticsearchNodeClusterPublishedStatesCompatible {
	m := metricElasticsearchNodeClusterPublishedStatesCompatible{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterPublishedStatesFull struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.published_states.full metric with initial data.
func (m *metricElasticsearchNodeClusterPublishedStatesFull) init() {
	m.data.SetName("elasticsearch.node.cluster.published_states.full")
	m.data.SetDescription("Number of published cluster states.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchNodeClusterPublishedStatesFull) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterPublishedStatesFull) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterPublishedStatesFull) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterPublishedStatesFull(settings MetricSettings) metricElasticsearchNodeClusterPublishedStatesFull {
	m := metricElasticsearchNodeClusterPublishedStatesFull{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterPublishedStatesIncompatible struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.published_states.incompatible metric with initial data.
func (m *metricElasticsearchNodeClusterPublishedStatesIncompatible) init() {
	m.data.SetName("elasticsearch.node.cluster.published_states.incompatible")
	m.data.SetDescription("Number of incompatible differences between published cluster states.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchNodeClusterPublishedStatesIncompatible) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterPublishedStatesIncompatible) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterPublishedStatesIncompatible) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterPublishedStatesIncompatible(settings MetricSettings) metricElasticsearchNodeClusterPublishedStatesIncompatible {
	m := metricElasticsearchNodeClusterPublishedStatesIncompatible{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterStateQueueCommitted struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.state_queue.committed metric with initial data.
func (m *metricElasticsearchNodeClusterStateQueueCommitted) init() {
	m.data.SetName("elasticsearch.node.cluster.state_queue.committed")
	m.data.SetDescription("Number of pending cluster states in queue.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchNodeClusterStateQueueCommitted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterStateQueueCommitted) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterStateQueueCommitted) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterStateQueueCommitted(settings MetricSettings) metricElasticsearchNodeClusterStateQueueCommitted {
	m := metricElasticsearchNodeClusterStateQueueCommitted{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterStateQueuePending struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.state_queue.pending metric with initial data.
func (m *metricElasticsearchNodeClusterStateQueuePending) init() {
	m.data.SetName("elasticsearch.node.cluster.state_queue.pending")
	m.data.SetDescription("Number of pending cluster states in queue.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchNodeClusterStateQueuePending) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterStateQueuePending) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterStateQueuePending) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterStateQueuePending(settings MetricSettings) metricElasticsearchNodeClusterStateQueuePending {
	m := metricElasticsearchNodeClusterStateQueuePending{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterStateUpdateCommitTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.state_update.commit_time metric with initial data.
func (m *metricElasticsearchNodeClusterStateUpdateCommitTime) init() {
	m.data.SetName("elasticsearch.node.cluster.state_update.commit_time")
	m.data.SetDescription("The cumulative amount of time spent waiting for a cluster state update to commit.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeClusterStateUpdateCommitTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("status", clusterStateUpdateTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterStateUpdateCommitTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterStateUpdateCommitTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterStateUpdateCommitTime(settings MetricSettings) metricElasticsearchNodeClusterStateUpdateCommitTime {
	m := metricElasticsearchNodeClusterStateUpdateCommitTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterStateUpdateCompletionTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.state_update.completion_time metric with initial data.
func (m *metricElasticsearchNodeClusterStateUpdateCompletionTime) init() {
	m.data.SetName("elasticsearch.node.cluster.state_update.completion_time")
	m.data.SetDescription("The cumulative amount of time spent waiting for a cluster state update to complete.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeClusterStateUpdateCompletionTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("status", clusterStateUpdateTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterStateUpdateCompletionTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterStateUpdateCompletionTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterStateUpdateCompletionTime(settings MetricSettings) metricElasticsearchNodeClusterStateUpdateCompletionTime {
	m := metricElasticsearchNodeClusterStateUpdateCompletionTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterStateUpdateComputationTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.state_update.computation_time metric with initial data.
func (m *metricElasticsearchNodeClusterStateUpdateComputationTime) init() {
	m.data.SetName("elasticsearch.node.cluster.state_update.computation_time")
	m.data.SetDescription("The cumulative amount of time spent computing cluster state updates since the node started.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeClusterStateUpdateComputationTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("status", clusterStateUpdateTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterStateUpdateComputationTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterStateUpdateComputationTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterStateUpdateComputationTime(settings MetricSettings) metricElasticsearchNodeClusterStateUpdateComputationTime {
	m := metricElasticsearchNodeClusterStateUpdateComputationTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterStateUpdateContextConstructionTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.state_update.context_construction_time metric with initial data.
func (m *metricElasticsearchNodeClusterStateUpdateContextConstructionTime) init() {
	m.data.SetName("elasticsearch.node.cluster.state_update.context_construction_time")
	m.data.SetDescription("The cumulative amount of time spent constructing a publication context since the node started.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeClusterStateUpdateContextConstructionTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("status", clusterStateUpdateTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterStateUpdateContextConstructionTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterStateUpdateContextConstructionTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterStateUpdateContextConstructionTime(settings MetricSettings) metricElasticsearchNodeClusterStateUpdateContextConstructionTime {
	m := metricElasticsearchNodeClusterStateUpdateContextConstructionTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterStateUpdateCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.state_update.count metric with initial data.
func (m *metricElasticsearchNodeClusterStateUpdateCount) init() {
	m.data.SetName("elasticsearch.node.cluster.state_update.count")
	m.data.SetDescription("The number of cluster state update attempts that changed the cluster state since the node started.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeClusterStateUpdateCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("status", clusterStateUpdateTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterStateUpdateCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterStateUpdateCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterStateUpdateCount(settings MetricSettings) metricElasticsearchNodeClusterStateUpdateCount {
	m := metricElasticsearchNodeClusterStateUpdateCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterStateUpdateMasterApplyTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.state_update.master_apply_time metric with initial data.
func (m *metricElasticsearchNodeClusterStateUpdateMasterApplyTime) init() {
	m.data.SetName("elasticsearch.node.cluster.state_update.master_apply_time")
	m.data.SetDescription("The cumulative amount of time spent applying cluster state updates on the elected master since the node started.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeClusterStateUpdateMasterApplyTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("status", clusterStateUpdateTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterStateUpdateMasterApplyTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterStateUpdateMasterApplyTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterStateUpdateMasterApplyTime(settings MetricSettings) metricElasticsearchNodeClusterStateUpdateMasterApplyTime {
	m := metricElasticsearchNodeClusterStateUpdateMasterApplyTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterStateUpdateNotificationTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.state_update.notification_time metric with initial data.
func (m *metricElasticsearchNodeClusterStateUpdateNotificationTime) init() {
	m.data.SetName("elasticsearch.node.cluster.state_update.notification_time")
	m.data.SetDescription("The cumulative amount of time spent notifying listeners of a cluster state update since the node started.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeClusterStateUpdateNotificationTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("status", clusterStateUpdateTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterStateUpdateNotificationTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterStateUpdateNotificationTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterStateUpdateNotificationTime(settings MetricSettings) metricElasticsearchNodeClusterStateUpdateNotificationTime {
	m := metricElasticsearchNodeClusterStateUpdateNotificationTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeDiskIoRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.disk.io.read metric with initial data.
func (m *metricElasticsearchNodeDiskIoRead) init() {
	m.data.SetName("elasticsearch.node.disk.io.read")
	m.data.SetDescription("The total number of kilobytes read across all file stores for this node.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeDiskIoRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeDiskIoRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeDiskIoRead) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeDiskIoRead(settings MetricSettings) metricElasticsearchNodeDiskIoRead {
	m := metricElasticsearchNodeDiskIoRead{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeDiskIoWrite struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.disk.io.write metric with initial data.
func (m *metricElasticsearchNodeDiskIoWrite) init() {
	m.data.SetName("elasticsearch.node.disk.io.write")
	m.data.SetDescription("The total number of kilobytes written across all file stores for this node.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeDiskIoWrite) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeDiskIoWrite) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeDiskIoWrite) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeDiskIoWrite(settings MetricSettings) metricElasticsearchNodeDiskIoWrite {
	m := metricElasticsearchNodeDiskIoWrite{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeDocuments struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.documents metric with initial data.
func (m *metricElasticsearchNodeDocuments) init() {
	m.data.SetName("elasticsearch.node.documents")
	m.data.SetDescription("The number of documents on the node.")
	m.data.SetUnit("{documents}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeDocuments) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, documentStateAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("state", documentStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeDocuments) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeDocuments) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeDocuments(settings MetricSettings) metricElasticsearchNodeDocuments {
	m := metricElasticsearchNodeDocuments{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeFsDiskAvailable struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.fs.disk.available metric with initial data.
func (m *metricElasticsearchNodeFsDiskAvailable) init() {
	m.data.SetName("elasticsearch.node.fs.disk.available")
	m.data.SetDescription("The amount of disk space available across all file stores for this node.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeFsDiskAvailable) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeFsDiskAvailable) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeFsDiskAvailable) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeFsDiskAvailable(settings MetricSettings) metricElasticsearchNodeFsDiskAvailable {
	m := metricElasticsearchNodeFsDiskAvailable{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeHTTPConnections struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.http.connections metric with initial data.
func (m *metricElasticsearchNodeHTTPConnections) init() {
	m.data.SetName("elasticsearch.node.http.connections")
	m.data.SetDescription("The number of HTTP connections to the node.")
	m.data.SetUnit("{connections}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeHTTPConnections) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeHTTPConnections) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeHTTPConnections) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeHTTPConnections(settings MetricSettings) metricElasticsearchNodeHTTPConnections {
	m := metricElasticsearchNodeHTTPConnections{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeIngestCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.ingest.count metric with initial data.
func (m *metricElasticsearchNodeIngestCount) init() {
	m.data.SetName("elasticsearch.node.ingest.count")
	m.data.SetDescription("Total number of documents ingested during the lifetime of this node.")
	m.data.SetUnit("{documents}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeIngestCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeIngestCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeIngestCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeIngestCount(settings MetricSettings) metricElasticsearchNodeIngestCount {
	m := metricElasticsearchNodeIngestCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeIngestCurrent struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.ingest.current metric with initial data.
func (m *metricElasticsearchNodeIngestCurrent) init() {
	m.data.SetName("elasticsearch.node.ingest.current")
	m.data.SetDescription("Total number of documents currently being ingested.")
	m.data.SetUnit("{documents}")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchNodeIngestCurrent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeIngestCurrent) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeIngestCurrent) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeIngestCurrent(settings MetricSettings) metricElasticsearchNodeIngestCurrent {
	m := metricElasticsearchNodeIngestCurrent{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeIngestFailed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.ingest.failed metric with initial data.
func (m *metricElasticsearchNodeIngestFailed) init() {
	m.data.SetName("elasticsearch.node.ingest.failed")
	m.data.SetDescription("Total number of failed ingest operations during the lifetime of this node.")
	m.data.SetUnit("{operation}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeIngestFailed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeIngestFailed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeIngestFailed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeIngestFailed(settings MetricSettings) metricElasticsearchNodeIngestFailed {
	m := metricElasticsearchNodeIngestFailed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeIngestPipelineCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.ingest.pipeline.count metric with initial data.
func (m *metricElasticsearchNodeIngestPipelineCount) init() {
	m.data.SetName("elasticsearch.node.ingest.pipeline.count")
	m.data.SetDescription("Total number of documents ingested during the lifetime of this node.")
	m.data.SetUnit("{documents}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeIngestPipelineCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestPipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("name", ingestPipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeIngestPipelineCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeIngestPipelineCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeIngestPipelineCount(settings MetricSettings) metricElasticsearchNodeIngestPipelineCount {
	m := metricElasticsearchNodeIngestPipelineCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeIngestPipelineCurrent struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.ingest.pipeline.current metric with initial data.
func (m *metricElasticsearchNodeIngestPipelineCurrent) init() {
	m.data.SetName("elasticsearch.node.ingest.pipeline.current")
	m.data.SetDescription("Total number of documents currently being ingested by a pipeline.")
	m.data.SetUnit("{documents}")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeIngestPipelineCurrent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestPipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("name", ingestPipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeIngestPipelineCurrent) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeIngestPipelineCurrent) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeIngestPipelineCurrent(settings MetricSettings) metricElasticsearchNodeIngestPipelineCurrent {
	m := metricElasticsearchNodeIngestPipelineCurrent{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeIngestPipelineFailed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.ingest.pipeline.failed metric with initial data.
func (m *metricElasticsearchNodeIngestPipelineFailed) init() {
	m.data.SetName("elasticsearch.node.ingest.pipeline.failed")
	m.data.SetDescription("Total number of failed operations for the ingest pipeline.")
	m.data.SetUnit("{operation}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeIngestPipelineFailed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, ingestPipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("name", ingestPipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeIngestPipelineFailed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeIngestPipelineFailed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeIngestPipelineFailed(settings MetricSettings) metricElasticsearchNodeIngestPipelineFailed {
	m := metricElasticsearchNodeIngestPipelineFailed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeOpenFiles struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.open_files metric with initial data.
func (m *metricElasticsearchNodeOpenFiles) init() {
	m.data.SetName("elasticsearch.node.open_files")
	m.data.SetDescription("The number of open file descriptors held by the node.")
	m.data.SetUnit("{files}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeOpenFiles) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeOpenFiles) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeOpenFiles) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeOpenFiles(settings MetricSettings) metricElasticsearchNodeOpenFiles {
	m := metricElasticsearchNodeOpenFiles{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeOperationsCompleted struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.operations.completed metric with initial data.
func (m *metricElasticsearchNodeOperationsCompleted) init() {
	m.data.SetName("elasticsearch.node.operations.completed")
	m.data.SetDescription("The number of operations completed.")
	m.data.SetUnit("{operations}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeOperationsCompleted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, operationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("operation", operationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeOperationsCompleted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeOperationsCompleted) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeOperationsCompleted(settings MetricSettings) metricElasticsearchNodeOperationsCompleted {
	m := metricElasticsearchNodeOperationsCompleted{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeOperationsTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.operations.time metric with initial data.
func (m *metricElasticsearchNodeOperationsTime) init() {
	m.data.SetName("elasticsearch.node.operations.time")
	m.data.SetDescription("Time spent on operations.")
	m.data.SetUnit("ms")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeOperationsTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, operationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("operation", operationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeOperationsTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeOperationsTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeOperationsTime(settings MetricSettings) metricElasticsearchNodeOperationsTime {
	m := metricElasticsearchNodeOperationsTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeScriptCacheEvictions struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.script.cache_evictions metric with initial data.
func (m *metricElasticsearchNodeScriptCacheEvictions) init() {
	m.data.SetName("elasticsearch.node.script.cache_evictions")
	m.data.SetDescription("Total number of times the script cache has evicted old data.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeScriptCacheEvictions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeScriptCacheEvictions) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeScriptCacheEvictions) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeScriptCacheEvictions(settings MetricSettings) metricElasticsearchNodeScriptCacheEvictions {
	m := metricElasticsearchNodeScriptCacheEvictions{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeScriptCompilationLimitTriggered struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.script.compilation_limit_triggered metric with initial data.
func (m *metricElasticsearchNodeScriptCompilationLimitTriggered) init() {
	m.data.SetName("elasticsearch.node.script.compilation_limit_triggered")
	m.data.SetDescription("Total number of times the script compilation circuit breaker has limited inline script compilations.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeScriptCompilationLimitTriggered) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeScriptCompilationLimitTriggered) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeScriptCompilationLimitTriggered) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeScriptCompilationLimitTriggered(settings MetricSettings) metricElasticsearchNodeScriptCompilationLimitTriggered {
	m := metricElasticsearchNodeScriptCompilationLimitTriggered{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeScriptCompilations struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.script.compilations metric with initial data.
func (m *metricElasticsearchNodeScriptCompilations) init() {
	m.data.SetName("elasticsearch.node.script.compilations")
	m.data.SetDescription("Total number of inline script compilations performed by the node.")
	m.data.SetUnit("{compilations}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeScriptCompilations) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeScriptCompilations) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeScriptCompilations) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeScriptCompilations(settings MetricSettings) metricElasticsearchNodeScriptCompilations {
	m := metricElasticsearchNodeScriptCompilations{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeShardsDataSetSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.shards.data_set.size metric with initial data.
func (m *metricElasticsearchNodeShardsDataSetSize) init() {
	m.data.SetName("elasticsearch.node.shards.data_set.size")
	m.data.SetDescription("Total data set size of all shards assigned to the node. This includes the size of shards not stored fully on the node, such as the cache for partially mounted indices.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeShardsDataSetSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeShardsDataSetSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeShardsDataSetSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeShardsDataSetSize(settings MetricSettings) metricElasticsearchNodeShardsDataSetSize {
	m := metricElasticsearchNodeShardsDataSetSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeShardsReservedSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.shards.reserved.size metric with initial data.
func (m *metricElasticsearchNodeShardsReservedSize) init() {
	m.data.SetName("elasticsearch.node.shards.reserved.size")
	m.data.SetDescription("A prediction of how much larger the shard stores on this node will eventually grow due to ongoing peer recoveries, restoring snapshots, and similar activities. A value of -1 indicates that this is not available.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeShardsReservedSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeShardsReservedSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeShardsReservedSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeShardsReservedSize(settings MetricSettings) metricElasticsearchNodeShardsReservedSize {
	m := metricElasticsearchNodeShardsReservedSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeShardsSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.shards.size metric with initial data.
func (m *metricElasticsearchNodeShardsSize) init() {
	m.data.SetName("elasticsearch.node.shards.size")
	m.data.SetDescription("The size of the shards assigned to this node.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeShardsSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeShardsSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeShardsSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeShardsSize(settings MetricSettings) metricElasticsearchNodeShardsSize {
	m := metricElasticsearchNodeShardsSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeThreadPoolTasksFinished struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.thread_pool.tasks.finished metric with initial data.
func (m *metricElasticsearchNodeThreadPoolTasksFinished) init() {
	m.data.SetName("elasticsearch.node.thread_pool.tasks.finished")
	m.data.SetDescription("The number of tasks finished by the thread pool.")
	m.data.SetUnit("{tasks}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeThreadPoolTasksFinished) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, threadPoolNameAttributeValue string, taskStateAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("thread_pool_name", threadPoolNameAttributeValue)
	dp.Attributes().InsertString("state", taskStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeThreadPoolTasksFinished) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeThreadPoolTasksFinished) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeThreadPoolTasksFinished(settings MetricSettings) metricElasticsearchNodeThreadPoolTasksFinished {
	m := metricElasticsearchNodeThreadPoolTasksFinished{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeThreadPoolTasksQueued struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.thread_pool.tasks.queued metric with initial data.
func (m *metricElasticsearchNodeThreadPoolTasksQueued) init() {
	m.data.SetName("elasticsearch.node.thread_pool.tasks.queued")
	m.data.SetDescription("The number of queued tasks in the thread pool.")
	m.data.SetUnit("{tasks}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeThreadPoolTasksQueued) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, threadPoolNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("thread_pool_name", threadPoolNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeThreadPoolTasksQueued) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeThreadPoolTasksQueued) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeThreadPoolTasksQueued(settings MetricSettings) metricElasticsearchNodeThreadPoolTasksQueued {
	m := metricElasticsearchNodeThreadPoolTasksQueued{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeThreadPoolThreads struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.thread_pool.threads metric with initial data.
func (m *metricElasticsearchNodeThreadPoolThreads) init() {
	m.data.SetName("elasticsearch.node.thread_pool.threads")
	m.data.SetDescription("The number of threads in the thread pool.")
	m.data.SetUnit("{threads}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchNodeThreadPoolThreads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, threadPoolNameAttributeValue string, threadStateAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("thread_pool_name", threadPoolNameAttributeValue)
	dp.Attributes().InsertString("state", threadStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeThreadPoolThreads) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeThreadPoolThreads) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeThreadPoolThreads(settings MetricSettings) metricElasticsearchNodeThreadPoolThreads {
	m := metricElasticsearchNodeThreadPoolThreads{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeTranslogOperations struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.translog.operations metric with initial data.
func (m *metricElasticsearchNodeTranslogOperations) init() {
	m.data.SetName("elasticsearch.node.translog.operations")
	m.data.SetDescription("Number of transaction log operations.")
	m.data.SetUnit("{operations}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeTranslogOperations) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeTranslogOperations) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeTranslogOperations) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeTranslogOperations(settings MetricSettings) metricElasticsearchNodeTranslogOperations {
	m := metricElasticsearchNodeTranslogOperations{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeTranslogSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.translog.size metric with initial data.
func (m *metricElasticsearchNodeTranslogSize) init() {
	m.data.SetName("elasticsearch.node.translog.size")
	m.data.SetDescription("Size of the transaction log.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeTranslogSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeTranslogSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeTranslogSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeTranslogSize(settings MetricSettings) metricElasticsearchNodeTranslogSize {
	m := metricElasticsearchNodeTranslogSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeTranslogUncommittedSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.translog.uncommitted.size metric with initial data.
func (m *metricElasticsearchNodeTranslogUncommittedSize) init() {
	m.data.SetName("elasticsearch.node.translog.uncommitted.size")
	m.data.SetDescription("Size of uncommitted transaction log operations.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricElasticsearchNodeTranslogUncommittedSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeTranslogUncommittedSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeTranslogUncommittedSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeTranslogUncommittedSize(settings MetricSettings) metricElasticsearchNodeTranslogUncommittedSize {
	m := metricElasticsearchNodeTranslogUncommittedSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchOsCPULoadAvg15m struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.os.cpu.load_avg.15m metric with initial data.
func (m *metricElasticsearchOsCPULoadAvg15m) init() {
	m.data.SetName("elasticsearch.os.cpu.load_avg.15m")
	m.data.SetDescription("Fifteen-minute load average on the system (field is not present if fifteen-minute load average is not available).")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchOsCPULoadAvg15m) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchOsCPULoadAvg15m) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchOsCPULoadAvg15m) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchOsCPULoadAvg15m(settings MetricSettings) metricElasticsearchOsCPULoadAvg15m {
	m := metricElasticsearchOsCPULoadAvg15m{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchOsCPULoadAvg1m struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.os.cpu.load_avg.1m metric with initial data.
func (m *metricElasticsearchOsCPULoadAvg1m) init() {
	m.data.SetName("elasticsearch.os.cpu.load_avg.1m")
	m.data.SetDescription("One-minute load average on the system (field is not present if one-minute load average is not available).")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchOsCPULoadAvg1m) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchOsCPULoadAvg1m) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchOsCPULoadAvg1m) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchOsCPULoadAvg1m(settings MetricSettings) metricElasticsearchOsCPULoadAvg1m {
	m := metricElasticsearchOsCPULoadAvg1m{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchOsCPULoadAvg5m struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.os.cpu.load_avg.5m metric with initial data.
func (m *metricElasticsearchOsCPULoadAvg5m) init() {
	m.data.SetName("elasticsearch.os.cpu.load_avg.5m")
	m.data.SetDescription("Five-minute load average on the system (field is not present if five-minute load average is not available).")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchOsCPULoadAvg5m) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchOsCPULoadAvg5m) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchOsCPULoadAvg5m) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchOsCPULoadAvg5m(settings MetricSettings) metricElasticsearchOsCPULoadAvg5m {
	m := metricElasticsearchOsCPULoadAvg5m{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchOsCPUUsage struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.os.cpu.usage metric with initial data.
func (m *metricElasticsearchOsCPUUsage) init() {
	m.data.SetName("elasticsearch.os.cpu.usage")
	m.data.SetDescription("Recent CPU usage for the whole system, or -1 if not supported.")
	m.data.SetUnit("%")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricElasticsearchOsCPUUsage) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchOsCPUUsage) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchOsCPUUsage) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchOsCPUUsage(settings MetricSettings) metricElasticsearchOsCPUUsage {
	m := metricElasticsearchOsCPUUsage{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchOsMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.os.memory metric with initial data.
func (m *metricElasticsearchOsMemory) init() {
	m.data.SetName("elasticsearch.os.memory")
	m.data.SetDescription("Amount of physical memory.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricElasticsearchOsMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, memoryStateAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("state", memoryStateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchOsMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchOsMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchOsMemory(settings MetricSettings) metricElasticsearchOsMemory {
	m := metricElasticsearchOsMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricJvmClassesLoaded struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.classes.loaded metric with initial data.
func (m *metricJvmClassesLoaded) init() {
	m.data.SetName("jvm.classes.loaded")
	m.data.SetDescription("The number of loaded classes")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricJvmClassesLoaded) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmClassesLoaded) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmClassesLoaded) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmClassesLoaded(settings MetricSettings) metricJvmClassesLoaded {
	m := metricJvmClassesLoaded{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricJvmGcCollectionsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.gc.collections.count metric with initial data.
func (m *metricJvmGcCollectionsCount) init() {
	m.data.SetName("jvm.gc.collections.count")
	m.data.SetDescription("The total number of garbage collections that have occurred")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricJvmGcCollectionsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, collectorNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("name", collectorNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmGcCollectionsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmGcCollectionsCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmGcCollectionsCount(settings MetricSettings) metricJvmGcCollectionsCount {
	m := metricJvmGcCollectionsCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricJvmGcCollectionsElapsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.gc.collections.elapsed metric with initial data.
func (m *metricJvmGcCollectionsElapsed) init() {
	m.data.SetName("jvm.gc.collections.elapsed")
	m.data.SetDescription("The approximate accumulated collection elapsed time")
	m.data.SetUnit("ms")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricJvmGcCollectionsElapsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, collectorNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("name", collectorNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmGcCollectionsElapsed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmGcCollectionsElapsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmGcCollectionsElapsed(settings MetricSettings) metricJvmGcCollectionsElapsed {
	m := metricJvmGcCollectionsElapsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryHeapCommitted struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.heap.committed metric with initial data.
func (m *metricJvmMemoryHeapCommitted) init() {
	m.data.SetName("jvm.memory.heap.committed")
	m.data.SetDescription("The amount of memory that is guaranteed to be available for the heap")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricJvmMemoryHeapCommitted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryHeapCommitted) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryHeapCommitted) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryHeapCommitted(settings MetricSettings) metricJvmMemoryHeapCommitted {
	m := metricJvmMemoryHeapCommitted{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryHeapMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.heap.max metric with initial data.
func (m *metricJvmMemoryHeapMax) init() {
	m.data.SetName("jvm.memory.heap.max")
	m.data.SetDescription("The maximum amount of memory can be used for the heap")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricJvmMemoryHeapMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryHeapMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryHeapMax) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryHeapMax(settings MetricSettings) metricJvmMemoryHeapMax {
	m := metricJvmMemoryHeapMax{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryHeapUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.heap.used metric with initial data.
func (m *metricJvmMemoryHeapUsed) init() {
	m.data.SetName("jvm.memory.heap.used")
	m.data.SetDescription("The current heap memory usage")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricJvmMemoryHeapUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryHeapUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryHeapUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryHeapUsed(settings MetricSettings) metricJvmMemoryHeapUsed {
	m := metricJvmMemoryHeapUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryNonheapCommitted struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.nonheap.committed metric with initial data.
func (m *metricJvmMemoryNonheapCommitted) init() {
	m.data.SetName("jvm.memory.nonheap.committed")
	m.data.SetDescription("The amount of memory that is guaranteed to be available for non-heap purposes")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricJvmMemoryNonheapCommitted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryNonheapCommitted) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryNonheapCommitted) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryNonheapCommitted(settings MetricSettings) metricJvmMemoryNonheapCommitted {
	m := metricJvmMemoryNonheapCommitted{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryNonheapUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.nonheap.used metric with initial data.
func (m *metricJvmMemoryNonheapUsed) init() {
	m.data.SetName("jvm.memory.nonheap.used")
	m.data.SetDescription("The current non-heap memory usage")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricJvmMemoryNonheapUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryNonheapUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryNonheapUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryNonheapUsed(settings MetricSettings) metricJvmMemoryNonheapUsed {
	m := metricJvmMemoryNonheapUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryPoolMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.pool.max metric with initial data.
func (m *metricJvmMemoryPoolMax) init() {
	m.data.SetName("jvm.memory.pool.max")
	m.data.SetDescription("The maximum amount of memory can be used for the memory pool")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricJvmMemoryPoolMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, memoryPoolNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("name", memoryPoolNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryPoolMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryPoolMax) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryPoolMax(settings MetricSettings) metricJvmMemoryPoolMax {
	m := metricJvmMemoryPoolMax{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryPoolUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.pool.used metric with initial data.
func (m *metricJvmMemoryPoolUsed) init() {
	m.data.SetName("jvm.memory.pool.used")
	m.data.SetDescription("The current memory pool memory usage")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricJvmMemoryPoolUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, memoryPoolNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().InsertString("name", memoryPoolNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryPoolUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryPoolUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryPoolUsed(settings MetricSettings) metricJvmMemoryPoolUsed {
	m := metricJvmMemoryPoolUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricJvmThreadsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.threads.count metric with initial data.
func (m *metricJvmThreadsCount) init() {
	m.data.SetName("jvm.threads.count")
	m.data.SetDescription("The current number of threads")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricJvmThreadsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmThreadsCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmThreadsCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmThreadsCount(settings MetricSettings) metricJvmThreadsCount {
	m := metricJvmThreadsCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user settings.
type MetricsBuilder struct {
	startTime                                                        pcommon.Timestamp   // start time that will be applied to all recorded data points.
	metricsCapacity                                                  int                 // maximum observed number of metrics per resource.
	resourceCapacity                                                 int                 // maximum observed number of resource attributes.
	metricsBuffer                                                    pmetric.Metrics     // accumulates metrics data before emitting.
	buildInfo                                                        component.BuildInfo // contains version information
	metricElasticsearchBreakerMemoryEstimated                        metricElasticsearchBreakerMemoryEstimated
	metricElasticsearchBreakerMemoryLimit                            metricElasticsearchBreakerMemoryLimit
	metricElasticsearchBreakerTripped                                metricElasticsearchBreakerTripped
	metricElasticsearchClusterDataNodes                              metricElasticsearchClusterDataNodes
	metricElasticsearchClusterHealth                                 metricElasticsearchClusterHealth
	metricElasticsearchClusterNodes                                  metricElasticsearchClusterNodes
	metricElasticsearchClusterShards                                 metricElasticsearchClusterShards
	metricElasticsearchIndexingPressureMemoryCoordinating            metricElasticsearchIndexingPressureMemoryCoordinating
	metricElasticsearchIndexingPressureMemoryLimit                   metricElasticsearchIndexingPressureMemoryLimit
	metricElasticsearchIndexingPressureMemoryPrimary                 metricElasticsearchIndexingPressureMemoryPrimary
	metricElasticsearchIndexingPressureMemoryReplica                 metricElasticsearchIndexingPressureMemoryReplica
	metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections  metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections
	metricElasticsearchIndexingPressureMemoryTotalReplicaRejections  metricElasticsearchIndexingPressureMemoryTotalReplicaRejections
	metricElasticsearchNodeCacheEvictions                            metricElasticsearchNodeCacheEvictions
	metricElasticsearchNodeCacheMemoryUsage                          metricElasticsearchNodeCacheMemoryUsage
	metricElasticsearchNodeClusterConnections                        metricElasticsearchNodeClusterConnections
	metricElasticsearchNodeClusterIo                                 metricElasticsearchNodeClusterIo
	metricElasticsearchNodeClusterPublishedStatesCompatible          metricElasticsearchNodeClusterPublishedStatesCompatible
	metricElasticsearchNodeClusterPublishedStatesFull                metricElasticsearchNodeClusterPublishedStatesFull
	metricElasticsearchNodeClusterPublishedStatesIncompatible        metricElasticsearchNodeClusterPublishedStatesIncompatible
	metricElasticsearchNodeClusterStateQueueCommitted                metricElasticsearchNodeClusterStateQueueCommitted
	metricElasticsearchNodeClusterStateQueuePending                  metricElasticsearchNodeClusterStateQueuePending
	metricElasticsearchNodeClusterStateUpdateCommitTime              metricElasticsearchNodeClusterStateUpdateCommitTime
	metricElasticsearchNodeClusterStateUpdateCompletionTime          metricElasticsearchNodeClusterStateUpdateCompletionTime
	metricElasticsearchNodeClusterStateUpdateComputationTime         metricElasticsearchNodeClusterStateUpdateComputationTime
	metricElasticsearchNodeClusterStateUpdateContextConstructionTime metricElasticsearchNodeClusterStateUpdateContextConstructionTime
	metricElasticsearchNodeClusterStateUpdateCount                   metricElasticsearchNodeClusterStateUpdateCount
	metricElasticsearchNodeClusterStateUpdateMasterApplyTime         metricElasticsearchNodeClusterStateUpdateMasterApplyTime
	metricElasticsearchNodeClusterStateUpdateNotificationTime        metricElasticsearchNodeClusterStateUpdateNotificationTime
	metricElasticsearchNodeDiskIoRead                                metricElasticsearchNodeDiskIoRead
	metricElasticsearchNodeDiskIoWrite                               metricElasticsearchNodeDiskIoWrite
	metricElasticsearchNodeDocuments                                 metricElasticsearchNodeDocuments
	metricElasticsearchNodeFsDiskAvailable                           metricElasticsearchNodeFsDiskAvailable
	metricElasticsearchNodeHTTPConnections                           metricElasticsearchNodeHTTPConnections
	metricElasticsearchNodeIngestCount                               metricElasticsearchNodeIngestCount
	metricElasticsearchNodeIngestCurrent                             metricElasticsearchNodeIngestCurrent
	metricElasticsearchNodeIngestFailed                              metricElasticsearchNodeIngestFailed
	metricElasticsearchNodeIngestPipelineCount                       metricElasticsearchNodeIngestPipelineCount
	metricElasticsearchNodeIngestPipelineCurrent                     metricElasticsearchNodeIngestPipelineCurrent
	metricElasticsearchNodeIngestPipelineFailed                      metricElasticsearchNodeIngestPipelineFailed
	metricElasticsearchNodeOpenFiles                                 metricElasticsearchNodeOpenFiles
	metricElasticsearchNodeOperationsCompleted                       metricElasticsearchNodeOperationsCompleted
	metricElasticsearchNodeOperationsTime                            metricElasticsearchNodeOperationsTime
	metricElasticsearchNodeScriptCacheEvictions                      metricElasticsearchNodeScriptCacheEvictions
	metricElasticsearchNodeScriptCompilationLimitTriggered           metricElasticsearchNodeScriptCompilationLimitTriggered
	metricElasticsearchNodeScriptCompilations                        metricElasticsearchNodeScriptCompilations
	metricElasticsearchNodeShardsDataSetSize                         metricElasticsearchNodeShardsDataSetSize
	metricElasticsearchNodeShardsReservedSize                        metricElasticsearchNodeShardsReservedSize
	metricElasticsearchNodeShardsSize                                metricElasticsearchNodeShardsSize
	metricElasticsearchNodeThreadPoolTasksFinished                   metricElasticsearchNodeThreadPoolTasksFinished
	metricElasticsearchNodeThreadPoolTasksQueued                     metricElasticsearchNodeThreadPoolTasksQueued
	metricElasticsearchNodeThreadPoolThreads                         metricElasticsearchNodeThreadPoolThreads
	metricElasticsearchNodeTranslogOperations                        metricElasticsearchNodeTranslogOperations
	metricElasticsearchNodeTranslogSize                              metricElasticsearchNodeTranslogSize
	metricElasticsearchNodeTranslogUncommittedSize                   metricElasticsearchNodeTranslogUncommittedSize
	metricElasticsearchOsCPULoadAvg15m                               metricElasticsearchOsCPULoadAvg15m
	metricElasticsearchOsCPULoadAvg1m                                metricElasticsearchOsCPULoadAvg1m
	metricElasticsearchOsCPULoadAvg5m                                metricElasticsearchOsCPULoadAvg5m
	metricElasticsearchOsCPUUsage                                    metricElasticsearchOsCPUUsage
	metricElasticsearchOsMemory                                      metricElasticsearchOsMemory
	metricJvmClassesLoaded                                           metricJvmClassesLoaded
	metricJvmGcCollectionsCount                                      metricJvmGcCollectionsCount
	metricJvmGcCollectionsElapsed                                    metricJvmGcCollectionsElapsed
	metricJvmMemoryHeapCommitted                                     metricJvmMemoryHeapCommitted
	metricJvmMemoryHeapMax                                           metricJvmMemoryHeapMax
	metricJvmMemoryHeapUsed                                          metricJvmMemoryHeapUsed
	metricJvmMemoryNonheapCommitted                                  metricJvmMemoryNonheapCommitted
	metricJvmMemoryNonheapUsed                                       metricJvmMemoryNonheapUsed
	metricJvmMemoryPoolMax                                           metricJvmMemoryPoolMax
	metricJvmMemoryPoolUsed                                          metricJvmMemoryPoolUsed
	metricJvmThreadsCount                                            metricJvmThreadsCount
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func NewMetricsBuilder(settings MetricsSettings, buildInfo component.BuildInfo, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		startTime:     pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer: pmetric.NewMetrics(),
		buildInfo:     buildInfo,
		metricElasticsearchBreakerMemoryEstimated:                        newMetricElasticsearchBreakerMemoryEstimated(settings.ElasticsearchBreakerMemoryEstimated),
		metricElasticsearchBreakerMemoryLimit:                            newMetricElasticsearchBreakerMemoryLimit(settings.ElasticsearchBreakerMemoryLimit),
		metricElasticsearchBreakerTripped:                                newMetricElasticsearchBreakerTripped(settings.ElasticsearchBreakerTripped),
		metricElasticsearchClusterDataNodes:                              newMetricElasticsearchClusterDataNodes(settings.ElasticsearchClusterDataNodes),
		metricElasticsearchClusterHealth:                                 newMetricElasticsearchClusterHealth(settings.ElasticsearchClusterHealth),
		metricElasticsearchClusterNodes:                                  newMetricElasticsearchClusterNodes(settings.ElasticsearchClusterNodes),
		metricElasticsearchClusterShards:                                 newMetricElasticsearchClusterShards(settings.ElasticsearchClusterShards),
		metricElasticsearchIndexingPressureMemoryCoordinating:            newMetricElasticsearchIndexingPressureMemoryCoordinating(settings.ElasticsearchIndexingPressureMemoryCoordinating),
		metricElasticsearchIndexingPressureMemoryLimit:                   newMetricElasticsearchIndexingPressureMemoryLimit(settings.ElasticsearchIndexingPressureMemoryLimit),
		metricElasticsearchIndexingPressureMemoryPrimary:                 newMetricElasticsearchIndexingPressureMemoryPrimary(settings.ElasticsearchIndexingPressureMemoryPrimary),
		metricElasticsearchIndexingPressureMemoryReplica:                 newMetricElasticsearchIndexingPressureMemoryReplica(settings.ElasticsearchIndexingPressureMemoryReplica),
		metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections:  newMetricElasticsearchIndexingPressureMemoryTotalPrimaryRejections(settings.ElasticsearchIndexingPressureMemoryTotalPrimaryRejections),
		metricElasticsearchIndexingPressureMemoryTotalReplicaRejections:  newMetricElasticsearchIndexingPressureMemoryTotalReplicaRejections(settings.ElasticsearchIndexingPressureMemoryTotalReplicaRejections),
		metricElasticsearchNodeCacheEvictions:                            newMetricElasticsearchNodeCacheEvictions(settings.ElasticsearchNodeCacheEvictions),
		metricElasticsearchNodeCacheMemoryUsage:                          newMetricElasticsearchNodeCacheMemoryUsage(settings.ElasticsearchNodeCacheMemoryUsage),
		metricElasticsearchNodeClusterConnections:                        newMetricElasticsearchNodeClusterConnections(settings.ElasticsearchNodeClusterConnections),
		metricElasticsearchNodeClusterIo:                                 newMetricElasticsearchNodeClusterIo(settings.ElasticsearchNodeClusterIo),
		metricElasticsearchNodeClusterPublishedStatesCompatible:          newMetricElasticsearchNodeClusterPublishedStatesCompatible(settings.ElasticsearchNodeClusterPublishedStatesCompatible),
		metricElasticsearchNodeClusterPublishedStatesFull:                newMetricElasticsearchNodeClusterPublishedStatesFull(settings.ElasticsearchNodeClusterPublishedStatesFull),
		metricElasticsearchNodeClusterPublishedStatesIncompatible:        newMetricElasticsearchNodeClusterPublishedStatesIncompatible(settings.ElasticsearchNodeClusterPublishedStatesIncompatible),
		metricElasticsearchNodeClusterStateQueueCommitted:                newMetricElasticsearchNodeClusterStateQueueCommitted(settings.ElasticsearchNodeClusterStateQueueCommitted),
		metricElasticsearchNodeClusterStateQueuePending:                  newMetricElasticsearchNodeClusterStateQueuePending(settings.ElasticsearchNodeClusterStateQueuePending),
		metricElasticsearchNodeClusterStateUpdateCommitTime:              newMetricElasticsearchNodeClusterStateUpdateCommitTime(settings.ElasticsearchNodeClusterStateUpdateCommitTime),
		metricElasticsearchNodeClusterStateUpdateCompletionTime:          newMetricElasticsearchNodeClusterStateUpdateCompletionTime(settings.ElasticsearchNodeClusterStateUpdateCompletionTime),
		metricElasticsearchNodeClusterStateUpdateComputationTime:         newMetricElasticsearchNodeClusterStateUpdateComputationTime(settings.ElasticsearchNodeClusterStateUpdateComputationTime),
		metricElasticsearchNodeClusterStateUpdateContextConstructionTime: newMetricElasticsearchNodeClusterStateUpdateContextConstructionTime(settings.ElasticsearchNodeClusterStateUpdateContextConstructionTime),
		metricElasticsearchNodeClusterStateUpdateCount:                   newMetricElasticsearchNodeClusterStateUpdateCount(settings.ElasticsearchNodeClusterStateUpdateCount),
		metricElasticsearchNodeClusterStateUpdateMasterApplyTime:         newMetricElasticsearchNodeClusterStateUpdateMasterApplyTime(settings.ElasticsearchNodeClusterStateUpdateMasterApplyTime),
		metricElasticsearchNodeClusterStateUpdateNotificationTime:        newMetricElasticsearchNodeClusterStateUpdateNotificationTime(settings.ElasticsearchNodeClusterStateUpdateNotificationTime),
		metricElasticsearchNodeDiskIoRead:                                newMetricElasticsearchNodeDiskIoRead(settings.ElasticsearchNodeDiskIoRead),
		metricElasticsearchNodeDiskIoWrite:                               newMetricElasticsearchNodeDiskIoWrite(settings.ElasticsearchNodeDiskIoWrite),
		metricElasticsearchNodeDocuments:                                 newMetricElasticsearchNodeDocuments(settings.ElasticsearchNodeDocuments),
		metricElasticsearchNodeFsDiskAvailable:                           newMetricElasticsearchNodeFsDiskAvailable(settings.ElasticsearchNodeFsDiskAvailable),
		metricElasticsearchNodeHTTPConnections:                           newMetricElasticsearchNodeHTTPConnections(settings.ElasticsearchNodeHTTPConnections),
		metricElasticsearchNodeIngestCount:                               newMetricElasticsearchNodeIngestCount(settings.ElasticsearchNodeIngestCount),
		metricElasticsearchNodeIngestCurrent:                             newMetricElasticsearchNodeIngestCurrent(settings.ElasticsearchNodeIngestCurrent),
		metricElasticsearchNodeIngestFailed:                              newMetricElasticsearchNodeIngestFailed(settings.ElasticsearchNodeIngestFailed),
		metricElasticsearchNodeIngestPipelineCount:                       newMetricElasticsearchNodeIngestPipelineCount(settings.ElasticsearchNodeIngestPipelineCount),
		metricElasticsearchNodeIngestPipelineCurrent:                     newMetricElasticsearchNodeIngestPipelineCurrent(settings.ElasticsearchNodeIngestPipelineCurrent),
		metricElasticsearchNodeIngestPipelineFailed:                      newMetricElasticsearchNodeIngestPipelineFailed(settings.ElasticsearchNodeIngestPipelineFailed),
		metricElasticsearchNodeOpenFiles:                                 newMetricElasticsearchNodeOpenFiles(settings.ElasticsearchNodeOpenFiles),
		metricElasticsearchNodeOperationsCompleted:                       newMetricElasticsearchNodeOperationsCompleted(settings.ElasticsearchNodeOperationsCompleted),
		metricElasticsearchNodeOperationsTime:                            newMetricElasticsearchNodeOperationsTime(settings.ElasticsearchNodeOperationsTime),
		metricElasticsearchNodeScriptCacheEvictions:                      newMetricElasticsearchNodeScriptCacheEvictions(settings.ElasticsearchNodeScriptCacheEvictions),
		metricElasticsearchNodeScriptCompilationLimitTriggered:           newMetricElasticsearchNodeScriptCompilationLimitTriggered(settings.ElasticsearchNodeScriptCompilationLimitTriggered),
		metricElasticsearchNodeScriptCompilations:                        newMetricElasticsearchNodeScriptCompilations(settings.ElasticsearchNodeScriptCompilations),
		metricElasticsearchNodeShardsDataSetSize:                         newMetricElasticsearchNodeShardsDataSetSize(settings.ElasticsearchNodeShardsDataSetSize),
		metricElasticsearchNodeShardsReservedSize:                        newMetricElasticsearchNodeShardsReservedSize(settings.ElasticsearchNodeShardsReservedSize),
		metricElasticsearchNodeShardsSize:                                newMetricElasticsearchNodeShardsSize(settings.ElasticsearchNodeShardsSize),
		metricElasticsearchNodeThreadPoolTasksFinished:                   newMetricElasticsearchNodeThreadPoolTasksFinished(settings.ElasticsearchNodeThreadPoolTasksFinished),
		metricElasticsearchNodeThreadPoolTasksQueued:                     newMetricElasticsearchNodeThreadPoolTasksQueued(settings.ElasticsearchNodeThreadPoolTasksQueued),
		metricElasticsearchNodeThreadPoolThreads:                         newMetricElasticsearchNodeThreadPoolThreads(settings.ElasticsearchNodeThreadPoolThreads),
		metricElasticsearchNodeTranslogOperations:                        newMetricElasticsearchNodeTranslogOperations(settings.ElasticsearchNodeTranslogOperations),
		metricElasticsearchNodeTranslogSize:                              newMetricElasticsearchNodeTranslogSize(settings.ElasticsearchNodeTranslogSize),
		metricElasticsearchNodeTranslogUncommittedSize:                   newMetricElasticsearchNodeTranslogUncommittedSize(settings.ElasticsearchNodeTranslogUncommittedSize),
		metricElasticsearchOsCPULoadAvg15m:                               newMetricElasticsearchOsCPULoadAvg15m(settings.ElasticsearchOsCPULoadAvg15m),
		metricElasticsearchOsCPULoadAvg1m:                                newMetricElasticsearchOsCPULoadAvg1m(settings.ElasticsearchOsCPULoadAvg1m),
		metricElasticsearchOsCPULoadAvg5m:                                newMetricElasticsearchOsCPULoadAvg5m(settings.ElasticsearchOsCPULoadAvg5m),
		metricElasticsearchOsCPUUsage:                                    newMetricElasticsearchOsCPUUsage(settings.ElasticsearchOsCPUUsage),
		metricElasticsearchOsMemory:                                      newMetricElasticsearchOsMemory(settings.ElasticsearchOsMemory),
		metricJvmClassesLoaded:                                           newMetricJvmClassesLoaded(settings.JvmClassesLoaded),
		metricJvmGcCollectionsCount:                                      newMetricJvmGcCollectionsCount(settings.JvmGcCollectionsCount),
		metricJvmGcCollectionsElapsed:                                    newMetricJvmGcCollectionsElapsed(settings.JvmGcCollectionsElapsed),
		metricJvmMemoryHeapCommitted:                                     newMetricJvmMemoryHeapCommitted(settings.JvmMemoryHeapCommitted),
		metricJvmMemoryHeapMax:                                           newMetricJvmMemoryHeapMax(settings.JvmMemoryHeapMax),
		metricJvmMemoryHeapUsed:                                          newMetricJvmMemoryHeapUsed(settings.JvmMemoryHeapUsed),
		metricJvmMemoryNonheapCommitted:                                  newMetricJvmMemoryNonheapCommitted(settings.JvmMemoryNonheapCommitted),
		metricJvmMemoryNonheapUsed:                                       newMetricJvmMemoryNonheapUsed(settings.JvmMemoryNonheapUsed),
		metricJvmMemoryPoolMax:                                           newMetricJvmMemoryPoolMax(settings.JvmMemoryPoolMax),
		metricJvmMemoryPoolUsed:                                          newMetricJvmMemoryPoolUsed(settings.JvmMemoryPoolUsed),
		metricJvmThreadsCount:                                            newMetricJvmThreadsCount(settings.JvmThreadsCount),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
	if mb.resourceCapacity < rm.Resource().Attributes().Len() {
		mb.resourceCapacity = rm.Resource().Attributes().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption func(pmetric.ResourceMetrics)

// WithElasticsearchClusterName sets provided value as "elasticsearch.cluster.name" attribute for current resource.
func WithElasticsearchClusterName(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().UpsertString("elasticsearch.cluster.name", val)
	}
}

// WithElasticsearchNodeName sets provided value as "elasticsearch.node.name" attribute for current resource.
func WithElasticsearchNodeName(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().UpsertString("elasticsearch.node.name", val)
	}
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).DataType() {
			case pmetric.MetricDataTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricDataTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	}
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(rmo ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	rm.Resource().Attributes().EnsureCapacity(mb.resourceCapacity)
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName("otelcol/elasticsearchreceiver")
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricElasticsearchBreakerMemoryEstimated.emit(ils.Metrics())
	mb.metricElasticsearchBreakerMemoryLimit.emit(ils.Metrics())
	mb.metricElasticsearchBreakerTripped.emit(ils.Metrics())
	mb.metricElasticsearchClusterDataNodes.emit(ils.Metrics())
	mb.metricElasticsearchClusterHealth.emit(ils.Metrics())
	mb.metricElasticsearchClusterNodes.emit(ils.Metrics())
	mb.metricElasticsearchClusterShards.emit(ils.Metrics())
	mb.metricElasticsearchIndexingPressureMemoryCoordinating.emit(ils.Metrics())
	mb.metricElasticsearchIndexingPressureMemoryLimit.emit(ils.Metrics())
	mb.metricElasticsearchIndexingPressureMemoryPrimary.emit(ils.Metrics())
	mb.metricElasticsearchIndexingPressureMemoryReplica.emit(ils.Metrics())
	mb.metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections.emit(ils.Metrics())
	mb.metricElasticsearchIndexingPressureMemoryTotalReplicaRejections.emit(ils.Metrics())
	mb.metricElasticsearchNodeCacheEvictions.emit(ils.Metrics())
	mb.metricElasticsearchNodeCacheMemoryUsage.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterConnections.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterIo.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterPublishedStatesCompatible.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterPublishedStatesFull.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterPublishedStatesIncompatible.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterStateQueueCommitted.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterStateQueuePending.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterStateUpdateCommitTime.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterStateUpdateCompletionTime.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterStateUpdateComputationTime.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterStateUpdateContextConstructionTime.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterStateUpdateCount.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterStateUpdateMasterApplyTime.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterStateUpdateNotificationTime.emit(ils.Metrics())
	mb.metricElasticsearchNodeDiskIoRead.emit(ils.Metrics())
	mb.metricElasticsearchNodeDiskIoWrite.emit(ils.Metrics())
	mb.metricElasticsearchNodeDocuments.emit(ils.Metrics())
	mb.metricElasticsearchNodeFsDiskAvailable.emit(ils.Metrics())
	mb.metricElasticsearchNodeHTTPConnections.emit(ils.Metrics())
	mb.metricElasticsearchNodeIngestCount.emit(ils.Metrics())
	mb.metricElasticsearchNodeIngestCurrent.emit(ils.Metrics())
	mb.metricElasticsearchNodeIngestFailed.emit(ils.Metrics())
	mb.metricElasticsearchNodeIngestPipelineCount.emit(ils.Metrics())
	mb.metricElasticsearchNodeIngestPipelineCurrent.emit(ils.Metrics())
	mb.metricElasticsearchNodeIngestPipelineFailed.emit(ils.Metrics())
	mb.metricElasticsearchNodeOpenFiles.emit(ils.Metrics())
	mb.metricElasticsearchNodeOperationsCompleted.emit(ils.Metrics())
	mb.metricElasticsearchNodeOperationsTime.emit(ils.Metrics())
	mb.metricElasticsearchNodeScriptCacheEvictions.emit(ils.Metrics())
	mb.metricElasticsearchNodeScriptCompilationLimitTriggered.emit(ils.Metrics())
	mb.metricElasticsearchNodeScriptCompilations.emit(ils.Metrics())
	mb.metricElasticsearchNodeShardsDataSetSize.emit(ils.Metrics())
	mb.metricElasticsearchNodeShardsReservedSize.emit(ils.Metrics())
	mb.metricElasticsearchNodeShardsSize.emit(ils.Metrics())
	mb.metricElasticsearchNodeThreadPoolTasksFinished.emit(ils.Metrics())
	mb.metricElasticsearchNodeThreadPoolTasksQueued.emit(ils.Metrics())
	mb.metricElasticsearchNodeThreadPoolThreads.emit(ils.Metrics())
	mb.metricElasticsearchNodeTranslogOperations.emit(ils.Metrics())
	mb.metricElasticsearchNodeTranslogSize.emit(ils.Metrics())
	mb.metricElasticsearchNodeTranslogUncommittedSize.emit(ils.Metrics())
	mb.metricElasticsearchOsCPULoadAvg15m.emit(ils.Metrics())
	mb.metricElasticsearchOsCPULoadAvg1m.emit(ils.Metrics())
	mb.metricElasticsearchOsCPULoadAvg5m.emit(ils.Metrics())
	mb.metricElasticsearchOsCPUUsage.emit(ils.Metrics())
	mb.metricElasticsearchOsMemory.emit(ils.Metrics())
	mb.metricJvmClassesLoaded.emit(ils.Metrics())
	mb.metricJvmGcCollectionsCount.emit(ils.Metrics())
	mb.metricJvmGcCollectionsElapsed.emit(ils.Metrics())
	mb.metricJvmMemoryHeapCommitted.emit(ils.Metrics())
	mb.metricJvmMemoryHeapMax.emit(ils.Metrics())
	mb.metricJvmMemoryHeapUsed.emit(ils.Metrics())
	mb.metricJvmMemoryNonheapCommitted.emit(ils.Metrics())
	mb.metricJvmMemoryNonheapUsed.emit(ils.Metrics())
	mb.metricJvmMemoryPoolMax.emit(ils.Metrics())
	mb.metricJvmMemoryPoolUsed.emit(ils.Metrics())
	mb.metricJvmThreadsCount.emit(ils.Metrics())
	for _, op := range rmo {
		op(rm)
	}
	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user settings, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(rmo ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(rmo...)
	metrics := pmetric.NewMetrics()
	mb.metricsBuffer.MoveTo(metrics)
	return metrics
}

// RecordElasticsearchBreakerMemoryEstimatedDataPoint adds a data point to elasticsearch.breaker.memory.estimated metric.
func (mb *MetricsBuilder) RecordElasticsearchBreakerMemoryEstimatedDataPoint(ts pcommon.Timestamp, val int64, circuitBreakerNameAttributeValue string) {
	mb.metricElasticsearchBreakerMemoryEstimated.recordDataPoint(mb.startTime, ts, val, circuitBreakerNameAttributeValue)
}

// RecordElasticsearchBreakerMemoryLimitDataPoint adds a data point to elasticsearch.breaker.memory.limit metric.
func (mb *MetricsBuilder) RecordElasticsearchBreakerMemoryLimitDataPoint(ts pcommon.Timestamp, val int64, circuitBreakerNameAttributeValue string) {
	mb.metricElasticsearchBreakerMemoryLimit.recordDataPoint(mb.startTime, ts, val, circuitBreakerNameAttributeValue)
}

// RecordElasticsearchBreakerTrippedDataPoint adds a data point to elasticsearch.breaker.tripped metric.
func (mb *MetricsBuilder) RecordElasticsearchBreakerTrippedDataPoint(ts pcommon.Timestamp, val int64, circuitBreakerNameAttributeValue string) {
	mb.metricElasticsearchBreakerTripped.recordDataPoint(mb.startTime, ts, val, circuitBreakerNameAttributeValue)
}

// RecordElasticsearchClusterDataNodesDataPoint adds a data point to elasticsearch.cluster.data_nodes metric.
func (mb *MetricsBuilder) RecordElasticsearchClusterDataNodesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchClusterDataNodes.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchClusterHealthDataPoint adds a data point to elasticsearch.cluster.health metric.
func (mb *MetricsBuilder) RecordElasticsearchClusterHealthDataPoint(ts pcommon.Timestamp, val int64, healthStatusAttributeValue AttributeHealthStatus) {
	mb.metricElasticsearchClusterHealth.recordDataPoint(mb.startTime, ts, val, healthStatusAttributeValue.String())
}

// RecordElasticsearchClusterNodesDataPoint adds a data point to elasticsearch.cluster.nodes metric.
func (mb *MetricsBuilder) RecordElasticsearchClusterNodesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchClusterNodes.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchClusterShardsDataPoint adds a data point to elasticsearch.cluster.shards metric.
func (mb *MetricsBuilder) RecordElasticsearchClusterShardsDataPoint(ts pcommon.Timestamp, val int64, shardStateAttributeValue AttributeShardState) {
	mb.metricElasticsearchClusterShards.recordDataPoint(mb.startTime, ts, val, shardStateAttributeValue.String())
}

// RecordElasticsearchIndexingPressureMemoryCoordinatingDataPoint adds a data point to elasticsearch.indexing_pressure.memory.coordinating metric.
func (mb *MetricsBuilder) RecordElasticsearchIndexingPressureMemoryCoordinatingDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchIndexingPressureMemoryCoordinating.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchIndexingPressureMemoryLimitDataPoint adds a data point to elasticsearch.indexing_pressure.memory.limit metric.
func (mb *MetricsBuilder) RecordElasticsearchIndexingPressureMemoryLimitDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchIndexingPressureMemoryLimit.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchIndexingPressureMemoryPrimaryDataPoint adds a data point to elasticsearch.indexing_pressure.memory.primary metric.
func (mb *MetricsBuilder) RecordElasticsearchIndexingPressureMemoryPrimaryDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchIndexingPressureMemoryPrimary.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchIndexingPressureMemoryReplicaDataPoint adds a data point to elasticsearch.indexing_pressure.memory.replica metric.
func (mb *MetricsBuilder) RecordElasticsearchIndexingPressureMemoryReplicaDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchIndexingPressureMemoryReplica.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchIndexingPressureMemoryTotalPrimaryRejectionsDataPoint adds a data point to elasticsearch.indexing_pressure.memory.total.primary_rejections metric.
func (mb *MetricsBuilder) RecordElasticsearchIndexingPressureMemoryTotalPrimaryRejectionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchIndexingPressureMemoryTotalPrimaryRejections.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchIndexingPressureMemoryTotalReplicaRejectionsDataPoint adds a data point to elasticsearch.indexing_pressure.memory.total.replica_rejections metric.
func (mb *MetricsBuilder) RecordElasticsearchIndexingPressureMemoryTotalReplicaRejectionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchIndexingPressureMemoryTotalReplicaRejections.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeCacheEvictionsDataPoint adds a data point to elasticsearch.node.cache.evictions metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeCacheEvictionsDataPoint(ts pcommon.Timestamp, val int64, cacheNameAttributeValue AttributeCacheName) {
	mb.metricElasticsearchNodeCacheEvictions.recordDataPoint(mb.startTime, ts, val, cacheNameAttributeValue.String())
}

// RecordElasticsearchNodeCacheMemoryUsageDataPoint adds a data point to elasticsearch.node.cache.memory.usage metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeCacheMemoryUsageDataPoint(ts pcommon.Timestamp, val int64, cacheNameAttributeValue AttributeCacheName) {
	mb.metricElasticsearchNodeCacheMemoryUsage.recordDataPoint(mb.startTime, ts, val, cacheNameAttributeValue.String())
}

// RecordElasticsearchNodeClusterConnectionsDataPoint adds a data point to elasticsearch.node.cluster.connections metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterConnectionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeClusterConnections.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeClusterIoDataPoint adds a data point to elasticsearch.node.cluster.io metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterIoDataPoint(ts pcommon.Timestamp, val int64, directionAttributeValue AttributeDirection) {
	mb.metricElasticsearchNodeClusterIo.recordDataPoint(mb.startTime, ts, val, directionAttributeValue.String())
}

// RecordElasticsearchNodeClusterPublishedStatesCompatibleDataPoint adds a data point to elasticsearch.node.cluster.published_states.compatible metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterPublishedStatesCompatibleDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeClusterPublishedStatesCompatible.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeClusterPublishedStatesFullDataPoint adds a data point to elasticsearch.node.cluster.published_states.full metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterPublishedStatesFullDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeClusterPublishedStatesFull.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeClusterPublishedStatesIncompatibleDataPoint adds a data point to elasticsearch.node.cluster.published_states.incompatible metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterPublishedStatesIncompatibleDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeClusterPublishedStatesIncompatible.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeClusterStateQueueCommittedDataPoint adds a data point to elasticsearch.node.cluster.state_queue.committed metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterStateQueueCommittedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeClusterStateQueueCommitted.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeClusterStateQueuePendingDataPoint adds a data point to elasticsearch.node.cluster.state_queue.pending metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterStateQueuePendingDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeClusterStateQueuePending.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeClusterStateUpdateCommitTimeDataPoint adds a data point to elasticsearch.node.cluster.state_update.commit_time metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterStateUpdateCommitTimeDataPoint(ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue AttributeClusterStateUpdateType) {
	mb.metricElasticsearchNodeClusterStateUpdateCommitTime.recordDataPoint(mb.startTime, ts, val, clusterStateUpdateTypeAttributeValue.String())
}

// RecordElasticsearchNodeClusterStateUpdateCompletionTimeDataPoint adds a data point to elasticsearch.node.cluster.state_update.completion_time metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterStateUpdateCompletionTimeDataPoint(ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue AttributeClusterStateUpdateType) {
	mb.metricElasticsearchNodeClusterStateUpdateCompletionTime.recordDataPoint(mb.startTime, ts, val, clusterStateUpdateTypeAttributeValue.String())
}

// RecordElasticsearchNodeClusterStateUpdateComputationTimeDataPoint adds a data point to elasticsearch.node.cluster.state_update.computation_time metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterStateUpdateComputationTimeDataPoint(ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue AttributeClusterStateUpdateType) {
	mb.metricElasticsearchNodeClusterStateUpdateComputationTime.recordDataPoint(mb.startTime, ts, val, clusterStateUpdateTypeAttributeValue.String())
}

// RecordElasticsearchNodeClusterStateUpdateContextConstructionTimeDataPoint adds a data point to elasticsearch.node.cluster.state_update.context_construction_time metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterStateUpdateContextConstructionTimeDataPoint(ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue AttributeClusterStateUpdateType) {
	mb.metricElasticsearchNodeClusterStateUpdateContextConstructionTime.recordDataPoint(mb.startTime, ts, val, clusterStateUpdateTypeAttributeValue.String())
}

// RecordElasticsearchNodeClusterStateUpdateCountDataPoint adds a data point to elasticsearch.node.cluster.state_update.count metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterStateUpdateCountDataPoint(ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue AttributeClusterStateUpdateType) {
	mb.metricElasticsearchNodeClusterStateUpdateCount.recordDataPoint(mb.startTime, ts, val, clusterStateUpdateTypeAttributeValue.String())
}

// RecordElasticsearchNodeClusterStateUpdateMasterApplyTimeDataPoint adds a data point to elasticsearch.node.cluster.state_update.master_apply_time metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterStateUpdateMasterApplyTimeDataPoint(ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue AttributeClusterStateUpdateType) {
	mb.metricElasticsearchNodeClusterStateUpdateMasterApplyTime.recordDataPoint(mb.startTime, ts, val, clusterStateUpdateTypeAttributeValue.String())
}

// RecordElasticsearchNodeClusterStateUpdateNotificationTimeDataPoint adds a data point to elasticsearch.node.cluster.state_update.notification_time metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterStateUpdateNotificationTimeDataPoint(ts pcommon.Timestamp, val int64, clusterStateUpdateTypeAttributeValue AttributeClusterStateUpdateType) {
	mb.metricElasticsearchNodeClusterStateUpdateNotificationTime.recordDataPoint(mb.startTime, ts, val, clusterStateUpdateTypeAttributeValue.String())
}

// RecordElasticsearchNodeDiskIoReadDataPoint adds a data point to elasticsearch.node.disk.io.read metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeDiskIoReadDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeDiskIoRead.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeDiskIoWriteDataPoint adds a data point to elasticsearch.node.disk.io.write metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeDiskIoWriteDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeDiskIoWrite.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeDocumentsDataPoint adds a data point to elasticsearch.node.documents metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeDocumentsDataPoint(ts pcommon.Timestamp, val int64, documentStateAttributeValue AttributeDocumentState) {
	mb.metricElasticsearchNodeDocuments.recordDataPoint(mb.startTime, ts, val, documentStateAttributeValue.String())
}

// RecordElasticsearchNodeFsDiskAvailableDataPoint adds a data point to elasticsearch.node.fs.disk.available metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeFsDiskAvailableDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeFsDiskAvailable.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeHTTPConnectionsDataPoint adds a data point to elasticsearch.node.http.connections metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeHTTPConnectionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeHTTPConnections.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeIngestCountDataPoint adds a data point to elasticsearch.node.ingest.count metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeIngestCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeIngestCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeIngestCurrentDataPoint adds a data point to elasticsearch.node.ingest.current metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeIngestCurrentDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeIngestCurrent.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeIngestFailedDataPoint adds a data point to elasticsearch.node.ingest.failed metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeIngestFailedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeIngestFailed.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeIngestPipelineCountDataPoint adds a data point to elasticsearch.node.ingest.pipeline.count metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeIngestPipelineCountDataPoint(ts pcommon.Timestamp, val int64, ingestPipelineNameAttributeValue string) {
	mb.metricElasticsearchNodeIngestPipelineCount.recordDataPoint(mb.startTime, ts, val, ingestPipelineNameAttributeValue)
}

// RecordElasticsearchNodeIngestPipelineCurrentDataPoint adds a data point to elasticsearch.node.ingest.pipeline.current metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeIngestPipelineCurrentDataPoint(ts pcommon.Timestamp, val int64, ingestPipelineNameAttributeValue string) {
	mb.metricElasticsearchNodeIngestPipelineCurrent.recordDataPoint(mb.startTime, ts, val, ingestPipelineNameAttributeValue)
}

// RecordElasticsearchNodeIngestPipelineFailedDataPoint adds a data point to elasticsearch.node.ingest.pipeline.failed metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeIngestPipelineFailedDataPoint(ts pcommon.Timestamp, val int64, ingestPipelineNameAttributeValue string) {
	mb.metricElasticsearchNodeIngestPipelineFailed.recordDataPoint(mb.startTime, ts, val, ingestPipelineNameAttributeValue)
}

// RecordElasticsearchNodeOpenFilesDataPoint adds a data point to elasticsearch.node.open_files metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeOpenFilesDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeOpenFiles.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeOperationsCompletedDataPoint adds a data point to elasticsearch.node.operations.completed metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeOperationsCompletedDataPoint(ts pcommon.Timestamp, val int64, operationAttributeValue AttributeOperation) {
	mb.metricElasticsearchNodeOperationsCompleted.recordDataPoint(mb.startTime, ts, val, operationAttributeValue.String())
}

// RecordElasticsearchNodeOperationsTimeDataPoint adds a data point to elasticsearch.node.operations.time metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeOperationsTimeDataPoint(ts pcommon.Timestamp, val int64, operationAttributeValue AttributeOperation) {
	mb.metricElasticsearchNodeOperationsTime.recordDataPoint(mb.startTime, ts, val, operationAttributeValue.String())
}

// RecordElasticsearchNodeScriptCacheEvictionsDataPoint adds a data point to elasticsearch.node.script.cache_evictions metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeScriptCacheEvictionsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeScriptCacheEvictions.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeScriptCompilationLimitTriggeredDataPoint adds a data point to elasticsearch.node.script.compilation_limit_triggered metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeScriptCompilationLimitTriggeredDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeScriptCompilationLimitTriggered.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeScriptCompilationsDataPoint adds a data point to elasticsearch.node.script.compilations metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeScriptCompilationsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeScriptCompilations.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeShardsDataSetSizeDataPoint adds a data point to elasticsearch.node.shards.data_set.size metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeShardsDataSetSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeShardsDataSetSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeShardsReservedSizeDataPoint adds a data point to elasticsearch.node.shards.reserved.size metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeShardsReservedSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeShardsReservedSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeShardsSizeDataPoint adds a data point to elasticsearch.node.shards.size metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeShardsSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeShardsSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeThreadPoolTasksFinishedDataPoint adds a data point to elasticsearch.node.thread_pool.tasks.finished metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeThreadPoolTasksFinishedDataPoint(ts pcommon.Timestamp, val int64, threadPoolNameAttributeValue string, taskStateAttributeValue AttributeTaskState) {
	mb.metricElasticsearchNodeThreadPoolTasksFinished.recordDataPoint(mb.startTime, ts, val, threadPoolNameAttributeValue, taskStateAttributeValue.String())
}

// RecordElasticsearchNodeThreadPoolTasksQueuedDataPoint adds a data point to elasticsearch.node.thread_pool.tasks.queued metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeThreadPoolTasksQueuedDataPoint(ts pcommon.Timestamp, val int64, threadPoolNameAttributeValue string) {
	mb.metricElasticsearchNodeThreadPoolTasksQueued.recordDataPoint(mb.startTime, ts, val, threadPoolNameAttributeValue)
}

// RecordElasticsearchNodeThreadPoolThreadsDataPoint adds a data point to elasticsearch.node.thread_pool.threads metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeThreadPoolThreadsDataPoint(ts pcommon.Timestamp, val int64, threadPoolNameAttributeValue string, threadStateAttributeValue AttributeThreadState) {
	mb.metricElasticsearchNodeThreadPoolThreads.recordDataPoint(mb.startTime, ts, val, threadPoolNameAttributeValue, threadStateAttributeValue.String())
}

// RecordElasticsearchNodeTranslogOperationsDataPoint adds a data point to elasticsearch.node.translog.operations metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeTranslogOperationsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeTranslogOperations.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeTranslogSizeDataPoint adds a data point to elasticsearch.node.translog.size metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeTranslogSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeTranslogSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeTranslogUncommittedSizeDataPoint adds a data point to elasticsearch.node.translog.uncommitted.size metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeTranslogUncommittedSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchNodeTranslogUncommittedSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchOsCPULoadAvg15mDataPoint adds a data point to elasticsearch.os.cpu.load_avg.15m metric.
func (mb *MetricsBuilder) RecordElasticsearchOsCPULoadAvg15mDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricElasticsearchOsCPULoadAvg15m.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchOsCPULoadAvg1mDataPoint adds a data point to elasticsearch.os.cpu.load_avg.1m metric.
func (mb *MetricsBuilder) RecordElasticsearchOsCPULoadAvg1mDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricElasticsearchOsCPULoadAvg1m.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchOsCPULoadAvg5mDataPoint adds a data point to elasticsearch.os.cpu.load_avg.5m metric.
func (mb *MetricsBuilder) RecordElasticsearchOsCPULoadAvg5mDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricElasticsearchOsCPULoadAvg5m.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchOsCPUUsageDataPoint adds a data point to elasticsearch.os.cpu.usage metric.
func (mb *MetricsBuilder) RecordElasticsearchOsCPUUsageDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricElasticsearchOsCPUUsage.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchOsMemoryDataPoint adds a data point to elasticsearch.os.memory metric.
func (mb *MetricsBuilder) RecordElasticsearchOsMemoryDataPoint(ts pcommon.Timestamp, val int64, memoryStateAttributeValue AttributeMemoryState) {
	mb.metricElasticsearchOsMemory.recordDataPoint(mb.startTime, ts, val, memoryStateAttributeValue.String())
}

// RecordJvmClassesLoadedDataPoint adds a data point to jvm.classes.loaded metric.
func (mb *MetricsBuilder) RecordJvmClassesLoadedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricJvmClassesLoaded.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmGcCollectionsCountDataPoint adds a data point to jvm.gc.collections.count metric.
func (mb *MetricsBuilder) RecordJvmGcCollectionsCountDataPoint(ts pcommon.Timestamp, val int64, collectorNameAttributeValue string) {
	mb.metricJvmGcCollectionsCount.recordDataPoint(mb.startTime, ts, val, collectorNameAttributeValue)
}

// RecordJvmGcCollectionsElapsedDataPoint adds a data point to jvm.gc.collections.elapsed metric.
func (mb *MetricsBuilder) RecordJvmGcCollectionsElapsedDataPoint(ts pcommon.Timestamp, val int64, collectorNameAttributeValue string) {
	mb.metricJvmGcCollectionsElapsed.recordDataPoint(mb.startTime, ts, val, collectorNameAttributeValue)
}

// RecordJvmMemoryHeapCommittedDataPoint adds a data point to jvm.memory.heap.committed metric.
func (mb *MetricsBuilder) RecordJvmMemoryHeapCommittedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricJvmMemoryHeapCommitted.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmMemoryHeapMaxDataPoint adds a data point to jvm.memory.heap.max metric.
func (mb *MetricsBuilder) RecordJvmMemoryHeapMaxDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricJvmMemoryHeapMax.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmMemoryHeapUsedDataPoint adds a data point to jvm.memory.heap.used metric.
func (mb *MetricsBuilder) RecordJvmMemoryHeapUsedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricJvmMemoryHeapUsed.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmMemoryNonheapCommittedDataPoint adds a data point to jvm.memory.nonheap.committed metric.
func (mb *MetricsBuilder) RecordJvmMemoryNonheapCommittedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricJvmMemoryNonheapCommitted.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmMemoryNonheapUsedDataPoint adds a data point to jvm.memory.nonheap.used metric.
func (mb *MetricsBuilder) RecordJvmMemoryNonheapUsedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricJvmMemoryNonheapUsed.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmMemoryPoolMaxDataPoint adds a data point to jvm.memory.pool.max metric.
func (mb *MetricsBuilder) RecordJvmMemoryPoolMaxDataPoint(ts pcommon.Timestamp, val int64, memoryPoolNameAttributeValue string) {
	mb.metricJvmMemoryPoolMax.recordDataPoint(mb.startTime, ts, val, memoryPoolNameAttributeValue)
}

// RecordJvmMemoryPoolUsedDataPoint adds a data point to jvm.memory.pool.used metric.
func (mb *MetricsBuilder) RecordJvmMemoryPoolUsedDataPoint(ts pcommon.Timestamp, val int64, memoryPoolNameAttributeValue string) {
	mb.metricJvmMemoryPoolUsed.recordDataPoint(mb.startTime, ts, val, memoryPoolNameAttributeValue)
}

// RecordJvmThreadsCountDataPoint adds a data point to jvm.threads.count metric.
func (mb *MetricsBuilder) RecordJvmThreadsCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricJvmThreadsCount.recordDataPoint(mb.startTime, ts, val)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}
